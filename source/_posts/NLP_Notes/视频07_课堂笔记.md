---
title: 视频07_课堂笔记
date: 2023-11-16 20:47:39
tag: NLP_Notes
---

# day07_课堂笔记

## 1 注意力机制由来

- seq2seq架构介绍(encoder-decoder)

  - encoder:编码器, 生成固定上下文张量c
  - decoder:解码器, 生成预测序列
    - 自回归预测: 只能使用上一时间步的预测结果作为下一时间步的输入

  ![1748483329298](1748483329298.png)

- seq2seq架构存在问题

  - c不变->信息不变/信息瓶颈
  - 使用GRU模型, 处理超长序列时也会产生梯度消失或梯度爆炸
  - 基于以上两个问题引用了注意力机制

## 2 注意力机制介绍

- 概念
  - 一种增强神经网络模型性能的技术/工具
  - 预测时每个时间步都要计算一个中间语义张量C(动态C) C1,C2,C3...
    - C1 = 0.5欢迎 + 0.3来 + 0.2北京
    - C2 = 0.3欢迎 + 0.6来 + 0.1北京
- 核心思想
  - 通过计算==动态中间语义张量c==来增强模型表达能力
- 作用
  - 增强神经网络模型性能
    - 增强可解释性 -> 权值
    - 缓解信息瓶颈问题 -> 动态C
    - 解决长序列问题 -> 使用自注意力机制替换RNN/LSTM/GRU 解决梯度消失或爆炸问题

## 3 注意力机制分类

### 3.1 soft attention(软注意力)

> 概念: 关注所有的输入词, 给每个词添加0-1之间的概率权重, 所有词的概率权重相加为1

- 不加attention的seq2seq框架

  - 编码器计算==固定长度的中间语义张量c==
  - 解码器一步一步解码,每一个时间步使用 固定c和上一时间步预测值融合的结果->X 再和 上一时间步的隐藏状态值 得到当前时间步的预测结果

- 加attention的seq2seq框架

  - 编码器计算每个时间步的隐藏状态值 h1 h2 h3 ...
  - 计算解码器每个时间步隐藏状态值前权重系数(经过softmax处理)  w1h1 w2h2 w3h3...
    - h1 h2 h3 拼接结果就是初始的中间语义张量c
  - 每个时间步的w1h1进行加权求和计算出动态中间语义张量c

- 注意力权重(概率)计算 (a11 a12 a13的结果值)

  ![1748493123217](1748493123217.png)

- 注意力的三步计算流程

  - query和key进行相似度计算, 得到 attention score(权重分值)
    - 点积
    - 缩放点积
    - 加性
  - attention score经过softmax, 得到 attention prob(权重概率)
  - attention prob和value进行加权求和, 得到 动态中间语义张量c

  ![1748502589934](1748502589934.png)

- query、key、value含义

  - query: 解码器时间步的输入x/隐藏状态值
  - key: 编码器时间步的词语嵌入向量/隐藏状态值
  - value: 编码器时间步的词语嵌入向量/隐藏状态值
  - nlp中一般情况下, key=value
  - query!=key=value -> 一般注意力机制/软注意力机制
  - query=key=value -> 自注意力机制

### 3.2 hard attention(硬注意力)

> 概念: 关注部分的输入词, 非0即1, 关注1对应的词

- 选择概率最大k个token,将对应权重设为1,其他的都是0
- 随机采样, 选择k个token
- 在强化学习中使用

### 3.3 self attention(自注意力)

> 概念: 舍弃传统的RNN/LSTM/GRU模型, 使用相似度计算的某种方式(缩放点积)算词之间的语义->并行计算, 信息不丢失

- transformer编码器或解码器中 -> query=key=value
- 并行计算两两token之间的相似度
  - 编码器端计算==上下文==两两token的相似度
  - 解码器端计算==上文==两两token的相似度

## 4 注意力机制规则

- 加性注意力

  - 先将q和k根据特征维度进行拼接后进行线性计算 -> softmax -> 和V加权求和(三维矩阵乘法)

    ![1748512221326](1748512221326.png)

  - 先将q和k根据特征维度进行拼接后进行线性计算[继续进行tanh激活计算,sum求和] -> softmax -> 和V加权求和(三维矩阵乘法)

    ![1748512227535](1748512227535.png)

- 点积注意力

  - q和k的转置进行点积 -> softmax -> 和V加权求和(三维矩阵乘法)

- 缩放点积注意力

  - q和k的转置进行点积, 点积结果除以sqrt(dk) -> softmax -> 和V加权求和(三维矩阵乘法)

  - 为什么除以sqrt(dk)?

    - 防止点击结果值过大, 导致梯度饱和(全为1, 对称性问题)

    ![1748512378106](1748512378106.png)

## 5 深度神经网络的注意力机制



## 6 seq2seq架构中注意力机制

- 编码器端的注意力机制
  - 自注意力机制 query=key=value
    - 并行计算两两token之间的语义关系(捕获语义) -> 计算上下文的token
- 解码器端的注意力机制
  - 自注意力机制 query=key=value
    - 并行计算两两token之间的语义关系(捕获语义) -> 计算上文的token
  - 一般注意力机制 query!=key=value
    - 让解码器选择性地关注编码器输出的相关信息





































