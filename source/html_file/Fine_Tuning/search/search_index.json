{"config":{"lang":["en"],"separator":"[\\s\\u200b\\u3000\\-\u3001\u3002\uff0c\uff0e\uff1f\uff01\uff1b]+","pipeline":["stemmer"]},"docs":[{"location":"index.html","title":"\u6b22\u8fce\u5b66\u4e60\u5927\u6a21\u578b\u5fae\u8c03\u6280\u672f","text":""},{"location":"01-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BE%AE%E8%B0%83%E4%B8%BB%E8%A6%81%E6%96%B9%E5%BC%8F/01-%E5%A4%A7%E6%A8%A1%E5%9E%8BPrompt-Tuning%E6%96%B9%E6%B3%95.html","title":"Prompt-Tuning\u65b9\u6cd5","text":""},{"location":"01-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BE%AE%E8%B0%83%E4%B8%BB%E8%A6%81%E6%96%B9%E5%BC%8F/01-%E5%A4%A7%E6%A8%A1%E5%9E%8BPrompt-Tuning%E6%96%B9%E6%B3%95.html#_1","title":"\u5b66\u4e60\u76ee\u6807","text":"<ul> <li>\u4e86\u89e3LLM\u8fdb\u9636\u5386\u7a0b\u56db\u79cd\u8303\u5f0f</li> <li>\u638c\u63e1Fine-Tuning\u6a21\u578b\u5fae\u8c03\u7684\u57fa\u672c\u539f\u7406</li> <li>\u638c\u63e1Prompt_Tuning\u6a21\u578b\u5fae\u8c03\u7684\u57fa\u672c\u539f\u7406</li> </ul>"},{"location":"01-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BE%AE%E8%B0%83%E4%B8%BB%E8%A6%81%E6%96%B9%E5%BC%8F/01-%E5%A4%A7%E6%A8%A1%E5%9E%8BPrompt-Tuning%E6%96%B9%E6%B3%95.html#1-nlp","title":"1 NLP\u4efb\u52a1\u56db\u79cd\u8303\u5f0f","text":"<p>\u76ee\u524d\u5b66\u672f\u754c\u4e00\u822c\u5c06NLP\u4efb\u52a1\u7684\u53d1\u5c55\u5206\u4e3a\u56db\u4e2a\u9636\u6bb5\uff0c\u5373NLP\u56db\u8303\u5f0f\uff1a</p> <ul> <li>\u7b2c\u4e00\u8303\u5f0f\uff1a\u57fa\u4e8e\u300c\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u6a21\u578b\u300d\u7684\u8303\u5f0f\uff0c\u5982TF-IDF\u7279\u5f81+\u6734\u7d20\u8d1d\u53f6\u65af\u7b49\u673a\u5668\u7b97\u6cd5\uff1b</li> <li>\u7b2c\u4e8c\u8303\u5f0f\uff1a\u57fa\u4e8e\u300c\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u300d\u7684\u8303\u5f0f\uff0c\u5982word2vec\u7279\u5f81+LSTM\u7b49\u6df1\u5ea6\u5b66\u4e60\u7b97\u6cd5\uff0c\u76f8\u6bd4\u4e8e\u7b2c\u4e00\u8303\u5f0f\uff0c\u6a21\u578b\u51c6\u786e\u6709\u6240\u63d0\u9ad8\uff0c\u7279\u5f81\u5de5\u7a0b\u7684\u5de5\u4f5c\u4e5f\u6709\u6240\u51cf\u5c11\uff1b</li> <li>\u7b2c\u4e09\u8303\u5f0f\uff1a\u57fa\u4e8e\u300c\u9884\u8bad\u7ec3\u6a21\u578b+fine-tuning\u300d\u7684\u8303\u5f0f\uff0c\u5982Bert+fine-tuning\u7684NLP\u4efb\u52a1\uff0c\u76f8\u6bd4\u4e8e\u7b2c\u4e8c\u8303\u5f0f\uff0c\u6a21\u578b\u51c6\u786e\u5ea6\u663e\u8457\u63d0\u9ad8\uff0c\u6a21\u578b\u4e5f\u968f\u4e4b\u53d8\u5f97\u66f4\u5927\uff0c\u4f46\u5c0f\u6570\u636e\u96c6\u5c31\u53ef\u8bad\u7ec3\u51fa\u597d\u6a21\u578b\uff1b</li> <li>\u7b2c\u56db\u8303\u5f0f\uff1a\u57fa\u4e8e\u300c\u9884\u8bad\u7ec3\u6a21\u578b+Prompt+\u9884\u6d4b\u300d\u7684\u8303\u5f0f\uff0c\u5982Bert+Prompt\u7684\u8303\u5f0f\u76f8\u6bd4\u4e8e\u7b2c\u4e09\u8303\u5f0f\uff0c\u6a21\u578b\u8bad\u7ec3\u6240\u9700\u7684\u8bad\u7ec3\u6570\u636e\u663e\u8457\u51cf\u5c11\u3002</li> </ul> <p>\u5728\u6574\u4e2aNLP\u9886\u57df\uff0c\u6574\u4e2a\u53d1\u5c55\u5386\u7a0b\u662f\u671d\u7740\u7cbe\u5ea6\u66f4\u9ad8\u3001\u5c11\u76d1\u7763\uff0c\u751a\u81f3\u65e0\u76d1\u7763\u7684\u65b9\u5411\u53d1\u5c55\u7684\u3002\u800c Prompt-Tuning \u662f\u76ee\u524d\u5b66\u672f\u754c\u5411\u8fd9\u4e2a\u65b9\u5411\u8fdb\u519b\u6700\u65b0\u4e5f\u662f\u6700\u706b\u7684\u7814\u7a76\u6210\u679c\u3002</p>"},{"location":"01-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BE%AE%E8%B0%83%E4%B8%BB%E8%A6%81%E6%96%B9%E5%BC%8F/01-%E5%A4%A7%E6%A8%A1%E5%9E%8BPrompt-Tuning%E6%96%B9%E6%B3%95.html#2-fine-tuning","title":"2 Fine-Tuning(\u5fae\u8c03)","text":"<p>Fine-Tuning\u5c5e\u4e8e\u4e00\u79cd\u8fc1\u79fb\u5b66\u4e60\u65b9\u5f0f\uff0c\u5728\u81ea\u7136\u8bed\u8a00\u5904\u7406\uff08NLP\uff09\u4e2d\uff0cFine-Tuning\u662f\u7528\u4e8e\u5c06\u9884\u8bad\u7ec3\u7684\u8bed\u8a00\u6a21\u578b\u9002\u5e94\u4e8e\u7279\u5b9a\u4efb\u52a1\u6216\u9886\u57df\u3002Fine-Tuning\u7684\u57fa\u672c\u601d\u60f3\u662f\u91c7\u7528\u5df2\u7ecf\u5728\u5927\u91cf\u6587\u672c\u4e0a\u8fdb\u884c\u8bad\u7ec3\u7684\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\uff0c\u7136\u540e\u5728\u5c0f\u89c4\u6a21\u7684\u7279\u5b9a\u4efb\u52a1\u6587\u672c\u4e0a\u7ee7\u7eed\u8bad\u7ec3\u5b83\u3002</p> <p>\u7ecf\u5178\u7684Fine-Tuning\u65b9\u6cd5\u5305\u62ec\u5c06\u9884\u8bad\u7ec3\u6a21\u578b\u4e0e\u5c11\u91cf\u7279\u5b9a\u4efb\u52a1\u6570\u636e\u4e00\u8d77\u7ee7\u7eed\u8bad\u7ec3\u3002\u5728\u8fd9\u4e2a\u8fc7\u7a0b\u4e2d\uff0c\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u6743\u91cd\u88ab\u66f4\u65b0\uff0c\u4ee5\u66f4\u597d\u5730\u9002\u5e94\u4efb\u52a1\u3002\u6240\u9700\u7684Fine-Tuning\u91cf\u53d6\u51b3\u4e8e\u9884\u8bad\u7ec3\u8bed\u6599\u5e93\u548c\u4efb\u52a1\u7279\u5b9a\u8bed\u6599\u5e93\u4e4b\u95f4\u7684\u76f8\u4f3c\u6027\u3002\u5982\u679c\u4e24\u8005\u76f8\u4f3c\uff0c\u53ef\u80fd\u53ea\u9700\u8981\u5c11\u91cf\u7684Fine-Tuning\uff0c\u5982\u679c\u4e24\u8005\u4e0d\u76f8\u4f3c\uff0c\u5219\u53ef\u80fd\u9700\u8981\u66f4\u591a\u7684Fine-Tuning\u3002</p> <p>\u4f46\u662f\uff0c\u5728\u5927\u591a\u6570\u4e0b\u6e38\u4efb\u52a1\u5fae\u8c03\u65f6\uff0c\u4e0b\u6e38\u4efb\u52a1\u7684\u76ee\u6807\u548c\u9884\u8bad\u7ec3\u7684\u76ee\u6807\u5dee\u8ddd\u8fc7\u5927\u5bfc\u81f4\u63d0\u5347\u6548\u679c\u4e0d\u660e\u663e\uff0c\u5fae\u8c03\u8fc7\u7a0b\u4e2d\u9700\u8981\u4f9d\u8d56\u5927\u91cf\u7684\u76d1\u7763\u8bed\u6599\u7b49\u7b49\u3002\u81f3\u6b64\uff0c\u4ee5GPT3\u3001PET\u7b49\u4e3a\u9996\u7684\u6a21\u578b\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u7684\u65b0\u7684\u5fae\u8c03\u8303\u5f0f\u2014\u2014Prompt-Tuning\u3002\u8be5\u65b9\u6cd5\u7684\u76ee\u7684\u662f\u901a\u8fc7\u6dfb\u52a0\u6a21\u677f\u7684\u65b9\u6cd5\u5c06\u4efb\u52a1\u76ee\u6807\u8f6c\u5316\u4e3a\u4e0e\u9884\u8bad\u7ec3\u76ee\u6807\u76f8\u4f3c\u7684\u5f62\u5f0f\uff08\u5982MLM\uff09\uff0c\u907f\u514d\u5f15\u5165\u989d\u5916\u7684\u53c2\u6570\u7684\u540c\u65f6\uff0c\u6700\u5927\u5316\u5229\u7528\u6a21\u578b\u7684\u9884\u8bad\u7ec3\u77e5\u8bc6\u3002</p> <p>Prompt-Tuning\u4e3b\u8981\u89e3\u51b3\u4f20\u7edfFine-Tuning\u65b9\u5f0f\u7684\u4e24\u4e2a\u75db\u70b9\uff1a</p> <ul> <li>\u53c2\u6570\u89c4\u6a21\u8fc7\u5927\u3001\u8ba1\u7b97\u4e0e\u5b58\u50a8\u6210\u672c\u9ad8\uff1a\u5728\u5168\u91cf\u5fae\u8c03\u4e2d\uff0c\u6a21\u578b\u7684\u6240\u6709\u53c2\u6570\u90fd\u4f1a\u66f4\u65b0\u3002\u5bf9\u4e8e\u5927\u6a21\u578b\uff0c\u8fd9\u610f\u5473\u7740\u663e\u5b58\u5360\u7528\u5927\u3001\u8bad\u7ec3\u901f\u5ea6\u6162\uff0c\u800c\u4e14\u6bcf\u4e2a\u4e0b\u6e38\u4efb\u52a1\u90fd\u8981\u4fdd\u5b58\u4e00\u4efd\u5b8c\u6574\u7684\u6a21\u578b\u6743\u91cd\uff0c\u5b58\u50a8\u5f00\u9500\u5de8\u5927\u3002</li> <li>\u8fc1\u79fb\u4e0e\u591a\u4efb\u52a1\u573a\u666f\u4e0b\u7684\u6548\u7387\u4f4e\uff1aFine-Tuning \u6bcf\u4e2a\u4efb\u52a1\u90fd\u8981\u8bad\u7ec3\u4e00\u4efd\u65b0\u6a21\u578b\uff0c\u4e0d\u80fd\u5f88\u597d\u5730\u5728\u591a\u4e2a\u4efb\u52a1\u4e4b\u95f4\u5171\u4eab\u5927\u6a21\u578b\u53c2\u6570\u3002\u5bfc\u81f4\u4e00\u4e2a\u4efb\u52a1\u4e00\u4efd\u5927\u6a21\u578b\uff0c\u96be\u4ee5\u9ad8\u6548\u9002\u914d\u591a\u4efb\u52a1\u3001\u591a\u573a\u666f\u3002\u3002 </li> </ul>"},{"location":"01-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BE%AE%E8%B0%83%E4%B8%BB%E8%A6%81%E6%96%B9%E5%BC%8F/01-%E5%A4%A7%E6%A8%A1%E5%9E%8BPrompt-Tuning%E6%96%B9%E6%B3%95.html#3-prompt-tuning","title":"3 Prompt-Tuning(\u63d0\u793a\u5fae\u8c03)","text":""},{"location":"01-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BE%AE%E8%B0%83%E4%B8%BB%E8%A6%81%E6%96%B9%E5%BC%8F/01-%E5%A4%A7%E6%A8%A1%E5%9E%8BPrompt-Tuning%E6%96%B9%E6%B3%95.html#31-prompt","title":"3.1 \u4ec0\u4e48\u662fPrompt?","text":"<p>prompt\u987e\u540d\u601d\u4e49\u5c31\u662f\u201c\u63d0\u793a\u201d\u7684\u610f\u601d\uff0c\u5e94\u8be5\u6709\u4eba\u73a9\u8fc7\u4f60\u753b\u6211\u731c\u8fd9\u4e2a\u6e38\u620f\u5427\uff0c\u5bf9\u65b9\u6839\u636e\u4e00\u4e2a\u8bcd\u8bed\u753b\u4e00\u5e45\u753b\uff0c\u6211\u4eec\u6765\u731c\u4ed6\u753b\u7684\u662f\u4ec0\u4e48\uff0c\u56e0\u4e3a\u6709\u592a\u591a\u7075\u9b42\u753b\u624b\u4e86\uff0c\u753b\u98ce\u6e05\u5947\uff0c\u6216\u8005\u4f60\u4eec\u6ca1\u6709\u5fc3\u6709\u7075\u7280\uff0c\u6839\u672c\u5c31\u4e0d\u597d\u731c\u554a\uff01\u8fd9\u65f6\u5019\u5c4f\u5e55\u4e0a\u4f1a\u51fa\u73b0\u4e00\u4e9b\u63d0\u793a\u8bcd\u6bd4\u59823\u4e2a\u5b57\uff0c\u6c34\u679c\uff0c\u90a3\u5c82\u4e0d\u662f\u597d\u731c\u4e00\u70b9\u4e86\u561b\uff0c\u6bd5\u7adf3\u4e2a\u5b57\u7684\u6c34\u679c\u4e5f\u4e0d\u591a\u5440\u3002\u770b\u5230\u4e86\u5427\uff0c\u8fd9\u5c31\u662fprompt\u7684\u9b45\u529b.</p> <p>\u7b80\u5355\u6765\u8bf4\uff0cPrompt\u5c31\u662f\u4f60\u7ed9\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u6240\u6709\u8f93\u5165\uff0c\u76ee\u7684\u662f\u5f15\u5bfc\u5b83\u751f\u6210\u4f60\u60f3\u8981\u7684\u8f93\u51fa\u3002\u5b83\u5c31\u50cf\u4f60\u548cAI\u52a9\u624b\u4e4b\u95f4\u7684\u201c\u5bf9\u8bdd\u542f\u52a8\u5668\u201d\u6216\u201c\u6307\u4ee4\u201d\u3002</p> <p>\u4f60\u53ef\u4ee5\u628a\u5b83\u60f3\u8c61\u6210\uff1a</p> <ul> <li>\u7ed9\u4e00\u4e2a\u975e\u5e38\u806a\u660e\u7684\u5b66\u751f\u51fa\u8003\u9898\uff1a\u4f60\u7684Prompt\u5c31\u662f\u90a3\u4e2a\u9898\u76ee\uff0cLLM\u5c31\u662f\u5b66\u751f\u3002\u9898\u76ee\u51fa\u5f97\u597d\uff0c\u5b66\u751f\u624d\u80fd\u7406\u89e3\u4f60\u7684\u610f\u56fe\uff0c\u7ed9\u51fa\u51c6\u786e\u7684\u7b54\u6848\u3002</li> <li>\u548c\u4e00\u4f4d\u77e5\u8bc6\u6e0a\u535a\u4f46\u9700\u8981\u5f15\u5bfc\u7684\u4e13\u5bb6\u4ea4\u6d41\uff1a\u4f60\u7684Prompt\u5c31\u662f\u4f60\u63d0\u95ee\u7684\u65b9\u5f0f\u3001\u5177\u4f53\u95ee\u9898\u548c\u8868\u8fbe\u65b9\u5f0f\uff0cLLM\u5c31\u662f\u4e13\u5bb6\u3002\u6b63\u786e\u5f15\u5bfc\u4e13\u5bb6\uff0c\u624d\u80fd\u83b7\u5f97\u66f4\u597d\u7684\u7b54\u6848\u3002</li> </ul>"},{"location":"01-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BE%AE%E8%B0%83%E4%B8%BB%E8%A6%81%E6%96%B9%E5%BC%8F/01-%E5%A4%A7%E6%A8%A1%E5%9E%8BPrompt-Tuning%E6%96%B9%E6%B3%95.html#32-prompt-tuing","title":"3.2 Prompt-Tuing\u5b9a\u4e49","text":"<p>\u57fa\u4e8eFine-Tuning\u7684\u65b9\u6cd5\u662f\u8ba9\u9884\u8bad\u7ec3\u6a21\u578b\u53bb\u8fc1\u5c31\u4e0b\u6e38\u4efb\u52a1\uff0c\u800c\u57fa\u4e8ePrompt-Tuning\u7684\u65b9\u6cd5\u53ef\u4ee5\u8ba9\u4e0b\u6e38\u4efb\u52a1\u53bb\u8fc1\u5c31\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u5176\u76ee\u7684\u662f\u5c06Fine-tuning\u7684\u4e0b\u6e38\u4efb\u52a1\u76ee\u6807\u8f6c\u6362\u4e3aPre-training\u7684\u4efb\u52a1\u3002</p> <p>\u90a3\u4e48\u5177\u4f53\u5982\u4f55\u5de5\u4f5c\u5462\uff1f\u6211\u4eec\u4ee5\u4e00\u4e2a\u4e8c\u5206\u7c7b\u7684\u60c5\u611f\u5206\u6790\u4e3a\u4f8b\u5b50\uff0c\u8fdb\u884c\u7b80\u5355\u7406\u89e3\uff1a</p> <ul> <li>eg:  \u5b9a\u4e00\u4e2a\u53e5\u5b50<code>[CLS] I like the Disney films very much. [SEP]</code> </li> <li>\u4f20\u7edf\u7684Fine-tuning\u65b9\u6cd5: \u5c06\u5176\u901a\u8fc7BERT\u7684Transformer\u83b7\u5f97 <code>[CLS]</code>\u8868\u5f81\u4e4b\u540e\u518d\u5582\u5165\u65b0\u589e\u52a0\u7684MLP\u5206\u7c7b\u5668\u8fdb\u884c\u4e8c\u5206\u7c7b\uff0c\u9884\u6d4b\u8be5\u53e5\u5b50\u662f\u79ef\u6781\u7684\uff08positive\uff09\u8fd8\u662f\u6d88\u6781\u7684\uff08negative\uff09\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u5b9a\u91cf\u7684\u8bad\u7ec3\u6570\u636e\u6765\u8bad\u7ec3\u3002</li> <li>Prompt-Tuning\u6267\u884c\u6b65\u9aa4\uff1a<ul> <li>1.\u6784\u5efa\u6a21\u677f\uff08Template Construction\uff09:  \u901a\u8fc7\u4eba\u5de5\u5b9a\u4e49\u3001\u81ea\u52a8\u641c\u7d22\u3001\u6587\u672c\u751f\u6210\u7b49\u65b9\u6cd5\uff0c\u751f\u6210\u4e0e\u7ed9\u5b9a\u53e5\u5b50\u76f8\u5173\u7684\u4e00\u4e2a\u542b\u6709<code>[MASK]</code>\u6807\u8bb0\u7684\u6a21\u677f\u3002\u4f8b\u5982<code>It was [MASK].</code>\uff0c\u5e76\u62fc\u63a5\u5230\u539f\u59cb\u7684\u6587\u672c\u4e2d\uff0c\u83b7\u5f97Prompt-Tuning\u7684\u8f93\u5165\uff1a<code>[CLS] I like the Disney films very much. [SEP] It was [MASK]. [SEP]</code>\u3002\u5c06\u5176\u5582\u5165BERT\u6a21\u578b\u4e2d\uff0c\u5e76\u590d\u7528\u9884\u8bad\u7ec3\u597d\u7684MLM\u5206\u7c7b\u5668\uff08\u5728huggingface\u4e2d\u4e3aBertForMaskedLM\uff09\uff0c\u5373\u53ef\u76f4\u63a5\u5f97\u5230<code>[MASK]</code>\u9884\u6d4b\u7684\u5404\u4e2atoken\u7684\u6982\u7387\u5206\u5e03\u3002</li> <li>2.\u6807\u7b7e\u8bcd\u6620\u5c04\uff08Label Word Verbalizer\uff09 \uff1a\u56e0\u4e3a<code>[MASK]</code>\u90e8\u5206\u6211\u4eec\u53ea\u5bf9\u90e8\u5206\u8bcd\u611f\u5174\u8da3\uff0c\u56e0\u6b64\u9700\u8981\u5efa\u7acb\u4e00\u4e2a\u6620\u5c04\u5173\u7cfb\u3002\u4f8b\u5982\u5982\u679c<code>[MASK]</code>\u9884\u6d4b\u7684\u8bcd\u662f\u201cgreat\u201d\uff0c\u5219\u8ba4\u4e3a\u662fpositive\u7c7b\uff0c\u5982\u679c\u662f\u201cterrible\u201d\uff0c\u5219\u8ba4\u4e3a\u662fnegative\u7c7b\u3002</li> <li>3.\u8bad\u7ec3\uff1a\u6839\u636eVerbalizer\uff0c\u5219\u53ef\u4ee5\u83b7\u5f97\u6307\u5b9alabel word\u7684\u9884\u6d4b\u6982\u7387\u5206\u5e03\uff0c\u5e76\u91c7\u7528\u4ea4\u53c9\u4fe1\u606f\u71b5\u8fdb\u884c\u8bad\u7ec3\u3002\u8fd9\u79cd\u65b9\u6cd5\u6bd4\u4f20\u7edf\u5fae\u8c03\u7684\u65b9\u5f0f\u9700\u8981\u7684\u6570\u636e\u91cf\u5c11\uff0c\u6548\u679c\u4e5f\u4e0d\u9519\u3002</li> </ul> </li> </ul> <p>\u6ce8\u610f\u601d\u8003\uff1a\u4e0d\u540c\u7684\u53e5\u5b50\u5e94\u8be5\u6709\u4e0d\u540c\u7684template\u548clabel word\uff0c\u6ca1\u9519\uff0c\u56e0\u4e3a\u6bcf\u4e2a\u53e5\u5b50\u53ef\u80fd\u671f\u671b\u9884\u6d4b\u51fa\u6765\u7684label word\u90fd\u4e0d\u540c\uff0c\u56e0\u6b64\u5982\u4f55\u6700\u5927\u5316\u7684\u5bfb\u627e\u5f53\u524d\u4efb\u52a1\u66f4\u52a0\u5408\u9002\u7684template\u548clabel word\u662fPrompt-tuning\u975e\u5e38\u91cd\u8981\u7684\u6311\u6218\u3002</p> <p>\u5176\u5b9e\u6211\u4eec\u53ef\u4ee5\u7406\u89e3\uff0c\u5f15\u5165\u7684\u6a21\u677f\u548c\u6807\u7b7e\u8bcd\u672c\u8d28\u4e0a\u5c5e\u4e8e\u4e00\u79cd\u6570\u636e\u589e\u5f3a\uff0c\u901a\u8fc7\u6dfb\u52a0\u63d0\u793a\u7684\u65b9\u5f0f\u5f15\u5165\u5148\u9a8c\u77e5\u8bc6\u3002</p>"},{"location":"01-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BE%AE%E8%B0%83%E4%B8%BB%E8%A6%81%E6%96%B9%E5%BC%8F/01-%E5%A4%A7%E6%A8%A1%E5%9E%8BPrompt-Tuning%E6%96%B9%E6%B3%95.html#4-prompt-tuning","title":"4 Prompt-Tuning\u6280\u672f\u53d1\u5c55\u5386\u7a0b","text":"<p>Prompt-Tuning\u81eaGPT-3\u88ab\u63d0\u51fa\u4ee5\u6765\uff0c\u4ece\u4f20\u7edf\u7684\u79bb\u6563\u3001\u8fde\u7eed\u7684Prompt\u6784\u5efa\u3001\u8d70\u5411\u9762\u5411\u8d85\u5927\u89c4\u6a21\u6a21\u578b\u7684In-Context Learning\u3001Instruction-tuning\u548cChain_of_Thought\uff0c\u5176\u53d1\u5c55\u5386\u7a0b\u53ef\u4ee5\u6982\u62ec\u4e3a\u4ee5\u4e0b\u51e0\u4e2a\u5173\u952e\u70b9\uff1a</p> <ul> <li>\u4ece In-Context Learning \u5230 Prompt Tuning\uff1a Prompt Tuning \u7684\u7075\u611f\u6765\u81ea\u4e8e In-Context Learning\uff0c\u4f46\u5b83\u901a\u8fc7\u8bad\u7ec3 Prompt Tokens\uff0c\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u6027\u80fd\u548c\u53c2\u6570\u6548\u7387\u3002</li> <li>\u4ece Hard Prompt \u5230 Soft Prompt\uff1aPrompt Tuning \u4f7f\u7528\u53ef\u5b66\u4e60\u7684\u5411\u91cf\u4f5c\u4e3a Prompt\uff0c\u800c\u4e0d\u662f\u81ea\u7136\u8bed\u8a00\u6587\u672c\uff0c\u4ece\u800c\u907f\u514d\u4e86\u4eba\u5de5\u8bbe\u8ba1 Prompt \u7684\u56f0\u96be\u3002</li> <li>\u4ece\u5355\u5c42 Prompt \u5230\u591a\u5c42 Prompt\uff1aPrefix-Tuning \u5728\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u6bcf\u4e00\u5c42\u90fd\u6dfb\u52a0 Prompt Tokens\uff0c\u4ece\u800c\u66f4\u597d\u5730\u63a7\u5236\u6a21\u578b\u7684\u884c\u4e3a\u3002</li> <li>\u4ece\u624b\u5de5\u8bbe\u8ba1\u5230\u81ea\u52a8\u5316\u641c\u7d22\uff1a\u7814\u7a76\u4eba\u5458\u5f00\u59cb\u63a2\u7d22\u5982\u4f55\u81ea\u52a8\u5316 Prompt Engineering\uff0c\u4ee5\u8fdb\u4e00\u6b65\u63d0\u9ad8 Prompt Tuning \u7684\u6548\u7387\u548c\u6548\u679c\u3002</li> </ul>"},{"location":"01-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BE%AE%E8%B0%83%E4%B8%BB%E8%A6%81%E6%96%B9%E5%BC%8F/01-%E5%A4%A7%E6%A8%A1%E5%9E%8BPrompt-Tuning%E6%96%B9%E6%B3%95.html#5-prompt-tuning","title":"5 \u9762\u5411\u8d85\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u7684Prompt-Tuning","text":"<p>\u8fd1\u4e24\u5e74\u6765\uff0c\u968f\u4e4bPrompt-Tuning\u6280\u672f\u7684\u53d1\u5c55\uff0c\u6709\u8bf8\u591a\u5de5\u4f5c\u53d1\u73b0\uff0c\u5bf9\u4e8e\u8d85\u8fc710\u4ebf\u53c2\u6570\u91cf\u7684\u6a21\u578b\u6765\u8bf4\uff0cPrompt-Tuning\u6240\u5e26\u6765\u7684\u589e\u76ca\u8fdc\u8fdc\u9ad8\u4e8e\u6807\u51c6\u7684Fine-tuning\uff0c\u5c0f\u6837\u672c\u751a\u81f3\u662f\u96f6\u6837\u672c\u7684\u6027\u80fd\u4e5f\u80fd\u591f\u6781\u5927\u5730\u88ab\u6fc0\u53d1\u51fa\u6765\uff0c\u5f97\u76ca\u4e8e\u8fd9\u4e9b\u6a21\u578b\u7684 \u53c2\u6570\u91cf\u8db3\u591f\u5927\uff0c\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u4f7f\u7528\u4e86**\u8db3\u591f\u591a\u7684\u8bed\u6599**\uff0c\u540c\u65f6\u8bbe\u8ba1\u7684**\u9884\u8bad\u7ec3\u4efb\u52a1\u8db3\u591f\u6709\u6548**\u3002\u6700\u4e3a\u7ecf\u5178\u7684\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u5219\u662f2020\u5e74\u63d0\u51fa\u7684GPT-3\uff0c\u5176\u62e5\u6709\u5927\u7ea61750\u4ebf\u7684\u53c2\u6570\uff0c\u4e14\u53d1\u73b0\u53ea\u9700\u8981\u8bbe\u8ba1\u5408\u9002\u7684\u6a21\u677f\u6216\u6307\u4ee4\u5373\u53ef\u4ee5**\u5b9e\u73b0\u514d\u53c2\u6570\u8bad\u7ec3\u7684\u96f6\u6837\u672c\u5b66\u4e60** \u3002</p> <p>\u672c\u6587\u9ed8\u8ba4\u4ee5GPT-3\u4e3a\u4f8b\uff0c\u4ecb\u7ecd\u51e0\u4e2a\u9762\u5411\u8d85\u5927\u89c4\u6a21\u7684Prompt-Tuning\u65b9\u6cd5\uff0c\u5206\u522b\u4e3a\uff1a</p> <ul> <li>\u4e0a\u4e0b\u6587\u5b66\u4e60 In-Context Learning\uff08ICL\uff09\uff1a\u76f4\u63a5\u6311\u9009\u5c11\u91cf\u7684\u8bad\u7ec3\u6837\u672c\u4f5c\u4e3a\u8be5\u4efb\u52a1\u7684\u63d0\u793a\uff1b</li> <li>\u6307\u4ee4\u5b66\u4e60 Instruction-Tuning\uff1a\u6784\u5efa\u4efb\u52a1\u6307\u4ee4\u96c6\uff0c\u4fc3\u4f7f\u6a21\u578b\u6839\u636e\u4efb\u52a1\u6307\u4ee4\u505a\u51fa\u53cd\u9988\uff1b</li> <li>\u601d\u7ef4\u94fe Chain-of-Thought\uff08CoT\uff09\uff1a\u7ed9\u4e88\u6216\u6fc0\u53d1\u6a21\u578b\u5177\u6709\u63a8\u7406\u548c\u89e3\u91ca\u7684\u4fe1\u606f\uff0c\u901a\u8fc7\u7ebf\u6027\u94fe\u5f0f\u7684\u6a21\u5f0f\u6307\u5bfc\u6a21\u578b\u751f\u6210\u5408\u7406\u7684\u7ed3\u679c\u3002</li> </ul>"},{"location":"01-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BE%AE%E8%B0%83%E4%B8%BB%E8%A6%81%E6%96%B9%E5%BC%8F/01-%E5%A4%A7%E6%A8%A1%E5%9E%8BPrompt-Tuning%E6%96%B9%E6%B3%95.html#51-in-context-learning","title":"5.1 In-Context Learning\uff08\u4e0a\u4e0b\u6587\u5b66\u4e60\uff09","text":"<p>ICL\u7684\u6838\u5fc3\u601d\u60f3\u662f**\u901a\u8fc7\u4e0a\u4e0b\u6587\u793a\u4f8b\uff08demonstrations\uff09\u8ba9\u6a21\u578b\u7406\u89e3\u4efb\u52a1\uff0c\u800c\u65e0\u9700\u663e\u5f0f\u8bad\u7ec3**\u3002\u8fd9\u4e00\u601d\u8def\u5728\u65e9\u671f few-shot \u5b66\u4e60\u4e2d\u5df2\u6709\u4f53\u73b0\uff0c\u4f46\u8fd9\u4e9b\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u8bad\u7ec3\u9636\u6bb5\u66f4\u65b0\u6a21\u578b\u53c2\u6570\uff1bGPT-3\uff082020\uff09\u9996\u6b21\u5c06\u5176\u5b8c\u5168\u7f6e\u4e8e\u63a8\u7406\u9636\u6bb5\uff0c\u5e76\u660e\u786e\u547d\u540d\u4e3a\"In-Context Learning\"\u3002</p> <p>\u5e38\u7528\u7684In-context learning\u65b9\u6cd5\u5305\u62ec\uff1a</p> <ul> <li>zero-shot learning<ul> <li>\u5b9a\u4e49: \u7ed9\u51fa\u4efb\u52a1\u7684\u63cf\u8ff0, \u7136\u540e\u63d0\u4f9b\u6d4b\u8bd5\u6570\u636e\u5bf9\u5176\u8fdb\u884c\u9884\u6d4b, \u76f4\u63a5\u8ba9\u9884\u8bad\u7ec3\u597d\u7684\u6a21\u578b\u53bb\u8fdb\u884c\u4efb\u52a1\u6d4b\u8bd5. </li> <li>\u793a\u4f8b: \u5411\u6a21\u578b\u8f93\u5165\u201c\u8fd9\u4e2a\u4efb\u52a1\u8981\u6c42\u5c06\u4e2d\u6587\u7ffb\u8bd1\u4e3a\u82f1\u6587. \u9500\u552e-&gt;\u201d, \u7136\u540e\u8981\u6c42\u6a21\u578b\u9884\u6d4b\u4e0b\u4e00\u4e2a\u8f93\u51fa\u5e94\u8be5\u662f\u4ec0\u4e48, \u6b63\u786e\u7b54\u6848\u5e94\u4e3a\u201csell\u201d.</li> </ul> </li> <li>one-shot learning<ul> <li>\u5b9a\u4e49: \u5728\u9884\u8bad\u7ec3\u548c\u771f\u6b63\u7ffb\u8bd1\u7684\u6837\u672c\u4e4b\u95f4, \u63d2\u5165\u4e00\u4e2a\u6837\u672c\u505a\u6307\u5bfc. \u76f8\u5f53\u4e8e\u5728\u9884\u8bad\u7ec3\u597d\u7684\u7ed3\u679c\u548c\u6240\u8981\u6267\u884c\u7684\u4efb\u52a1\u4e4b\u95f4, \u7ed9\u4e00\u4e2a\u4f8b\u5b50, \u544a\u8bc9\u6a21\u578b\u82f1\u8bed\u7ffb\u8bd1\u4e3a\u6cd5\u8bed, \u5e94\u8be5\u8fd9\u4e48\u7ffb\u8bd1. </li> <li>\u793a\u4f8b: \u5411\u6a21\u578b\u8f93\u5165\u201c\u8fd9\u4e2a\u4efb\u52a1\u8981\u6c42\u5c06\u4e2d\u6587\u7ffb\u8bd1\u4e3a\u82f1\u6587. \u4f60\u597d-&gt;hello, \u9500\u552e-&gt;\u201d, \u7136\u540e\u8981\u6c42\u6a21\u578b\u9884\u6d4b\u4e0b\u4e00\u4e2a\u8f93\u51fa\u5e94\u8be5\u662f\u4ec0\u4e48, \u6b63\u786e\u7b54\u6848\u5e94\u4e3a\u201csell\u201d. </li> </ul> </li> <li>few-shot learning<ul> <li>\u5b9a\u4e49: \u5728\u9884\u8bad\u7ec3\u548c\u771f\u6b63\u7ffb\u8bd1\u7684\u6837\u672c\u4e4b\u95f4, \u63d2\u5165\u591a\u4e2a\u6837\u672c\uff08\u4e00\u822c10-100\u6761\uff09\u505a\u6307\u5bfc. \u76f8\u5f53\u4e8e\u5728\u9884\u8bad\u7ec3\u597d\u7684\u7ed3\u679c\u548c\u6240\u8981\u6267\u884c\u7684\u4efb\u52a1\u4e4b\u95f4, \u7ed9\u591a\u4e2a\u4f8b\u5b50, \u544a\u8bc9\u6a21\u578b\u5e94\u8be5\u5982\u4f55\u5de5\u4f5c. </li> <li>\u793a\u4f8b: \u5411\u6a21\u578b\u8f93\u5165\u201c\u8fd9\u4e2a\u4efb\u52a1\u8981\u6c42\u5c06\u4e2d\u6587\u7ffb\u8bd1\u4e3a\u82f1\u6587. \u4f60\u597d-&gt;hello, \u518d\u89c1-&gt;goodbye, \u8d2d\u4e70-&gt;purchase, \u9500\u552e-&gt;\u201d, \u7136\u540e\u8981\u6c42\u6a21\u578b\u9884\u6d4b\u4e0b\u4e00\u4e2a\u8f93\u51fa\u5e94\u8be5\u662f\u4ec0\u4e48, \u6b63\u786e\u7b54\u6848\u5e94\u4e3a\u201csell\u201d. </li> </ul> </li> </ul> <p>\u76ee\u524dIn-context Learning\u4f9d\u7136\u4e0e\u666e\u901a\u7684fine-tuning\u6709\u4e00\u5b9a\u5dee\u8ddd\uff0c\u4e14\u9884\u6d4b\u7684\u7ed3\u679c\u65b9\u5dee\u5f88\u5927\uff0c\u540c\u65f6\u4e5f\u9700\u8981\u82b1\u8d39\u65f6\u95f4\u8003\u8651template\u7684\u6784\u5efa\u3002</p>"},{"location":"01-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BE%AE%E8%B0%83%E4%B8%BB%E8%A6%81%E6%96%B9%E5%BC%8F/01-%E5%A4%A7%E6%A8%A1%E5%9E%8BPrompt-Tuning%E6%96%B9%E6%B3%95.html#52-instruction-tuning","title":"5.2 Instruction-Tuning\uff08\u6307\u4ee4\u5fae\u8c03\uff09","text":"<p>\u9762\u5411\u8d85\u5927\u89c4\u6a21\u6a21\u578b\u7b2c\u4e8c\u4e2aPrompt\u6280\u672f\u662f\u6307\u4ee4\u5b66\u4e60\u3002\u5176\u5b9ePrompt-Tuning\u672c\u8d28\u4e0a\u662f\u5bf9\u4e0b\u6e38\u4efb\u52a1\u7684\u6307\u4ee4\uff0c\u7b80\u5355\u7684\u6765\u8bf4\uff1a\u5c31\u662f\u544a\u8bc9\u6a21\u578b\u9700\u8981\u505a\u4ec0\u4e48\u4efb\u52a1\uff0c\u8f93\u51fa\u4ec0\u4e48\u5185\u5bb9\u3002\u5728\u5bf9\u5927\u89c4\u6a21\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\u65f6\uff0c\u53ef\u4ee5\u4f7f\u7528\u5404\u79cd\u7c7b\u578b\u7684\u4efb\u52a1\u5b9a\u4e49\u6307\u4ee4\u8fdb\u884c\u8bad\u7ec3\uff0c\u6765\u63d0\u9ad8\u6a21\u578b\u5bf9\u4e0d\u540c\u4efb\u52a1\u7684\u6cdb\u5316\u80fd\u529b\u3002</p> <p>\u4ec0\u4e48\u662fInstruction-Tuning\uff1f\u8ba9\u6211\u4eec\u5148\u629b\u5f00\u8111\u5b50\u91cc\u7684\u4e00\u5207\u6982\u5ff5\uff0c\u628a\u81ea\u5df1\u5f53\u6210\u4e00\u4e2a\u6a21\u578b\u3002\u6211\u7ed9\u4f60\u4e24\u4e2a\u4efb\u52a1\uff1a</p> <ul> <li>1.\u5e26\u5973\u670b\u53cb\u53bb\u4e86\u4e00\u5bb6\u9910\u5385\uff0c\u5979\u5403\u7684\u5f88\u5f00\u5fc3\uff0c\u8fd9\u5bb6\u9910\u5385\u592a__\u4e86\uff01</li> <li> <p>2.\u5224\u65ad\u8fd9\u53e5\u8bdd\u7684\u60c5\u611f\uff0c\u56de\u7b54\u662f\u79ef\u6781\u8fd8\u662f\u6d88\u6781\uff1a\u5e26\u5973\u670b\u53cb\u53bb\u4e86\u4e00\u5bb6\u9910\u5385\uff0c\u5979\u5403\u7684\u5f88\u5f00\u5fc3\u3002</p> </li> <li> <p>\u8fd9\u4e24\u4e2a\u4efb\u52a1\u4e00\u4e2a\u662f\u8865\u5168\u4efb\u52a1\uff0c\u4e00\u4e2a\u662f\u5224\u522b\u4efb\u52a1\u3002Prompt\u5c31\u662f\u7b2c\u4e00\u79cd\u6a21\u5f0f\uff0cInstruction\u5c31\u662f\u7b2c\u4e8c\u79cd\u3002</p> </li> </ul> <p>Instruction-Tuning\u548cPrompt-Tuning\u7684\u6838\u5fc3\u4e00\u6837\uff0c\u5c31\u662f\u53bb \u53d1\u6398\u8bed\u8a00\u6a21\u578b\u672c\u8eab\u5177\u5907\u7684\u77e5\u8bc6 \u3002\u800c\u4ed6\u4eec\u7684\u4e0d\u540c\u70b9\u5c31\u5728\u4e8e:</p> <ul> <li>Prompt\u662f\u53bb\u6fc0\u53d1\u8bed\u8a00\u6a21\u578b\u7684 \u8865\u5168\u80fd\u529b \uff0c\u6bd4\u5982\u7ed9\u51fa\u4e0a\u534a\u53e5\u751f\u6210\u4e0b\u534a\u53e5\u3001\u6216\u8005\u505a\u5b8c\u5f62\u586b\u7a7a\u3002</li> <li>Instruction-Tuning\u5219\u662f\u6fc0\u53d1\u8bed\u8a00\u6a21\u578b\u7684 \u7406\u89e3\u80fd\u529b \uff0c\u901a\u8fc7\u7ed9\u51fa\u66f4\u660e\u663e\u7684\u6307\u4ee4/\u6307\u793a\uff0c\u8ba9\u6a21\u578b\u53bb\u7406\u89e3\u5e76\u505a\u51fa\u6b63\u786e\u7684action. </li> <li>Promp-Tuningt\u5728 \u6ca1\u6709\u7cbe\u8c03 \u7684\u6a21\u578b\u4e0a\u4e5f\u80fd\u6709\u4e00\u5b9a\u6548\u679c\uff0c\u4f46\u662fInstruct-Tuning\u5219 \u5fc5\u987b\u5bf9\u6a21\u578b\u7cbe\u8c03 \uff0c\u8ba9\u6a21\u578b\u77e5\u9053\u8fd9\u79cd\u6307\u4ee4\u6a21\u5f0f\u3002</li> </ul> <p>\u5982\u4f55\u5b9e\u73b0Instruction-Tuning\uff1f</p> <p>\u901a\u5e38\u662f\u5728\u4e00\u4e2a\u9884\u8bad\u7ec3\u5927\u6a21\u578b\u7684\u57fa\u7840\u4e0a\uff0c\u6536\u96c6\u5927\u89c4\u6a21\u6307\u4ee4-\u54cd\u5e94\u5bf9\uff0c\u5e76\u5c06\u8fd9\u4e9b\u6307\u4ee4\u8f6c\u5316\u4e3a\u81ea\u7136\u8bed\u8a00\u63d0\u793a\uff08prompt\uff09\u8f93\u5165\u6a21\u578b\uff0c\u901a\u8fc7\u76d1\u7763\u5fae\u8c03\uff08Supervised Fine-Tuning, SFT\uff09\u8ba9\u6a21\u578b\u5b66\u4f1a\u6309\u7167\u6307\u4ee4\u751f\u6210\u7b26\u5408\u9884\u671f\u7684\u8f93\u51fa\uff1b\u8bad\u7ec3\u65f6\u5c06\u6307\u4ee4\u4e0e\u4e0a\u4e0b\u6587\u62fc\u63a5\u6210\u8f93\u5165\u5e8f\u5217\uff0c\u76ee\u6807\u662f\u6700\u5927\u5316\u6a21\u578b\u5bf9\u6b63\u786e\u54cd\u5e94\u7684\u751f\u6210\u6982\u7387\uff0c\u4ece\u800c\u8ba9\u6a21\u578b\u5728\u63a8\u7406\u65f6\u5bf9\u672a\u89c1\u8fc7\u7684\u65b0\u6307\u4ee4\u4e5f\u80fd\u8fdb\u884c\u7b26\u5408\u610f\u56fe\u7684\u751f\u6210\u3002</p> <p>\u4e3a\u6bcf\u4e2a\u4efb\u52a1\u8bbe\u8ba1\u591a\u4e2a\u6307\u4ee4\u6a21\u7248\uff0c\u6d4b\u8bd5\u65f6\u770b\u5e73\u5747\u548c\u6700\u597d\u7684\u8868\u73b0\u3002</p>"},{"location":"01-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BE%AE%E8%B0%83%E4%B8%BB%E8%A6%81%E6%96%B9%E5%BC%8F/01-%E5%A4%A7%E6%A8%A1%E5%9E%8BPrompt-Tuning%E6%96%B9%E6%B3%95.html#53-chain-of-thought","title":"5.3 Chain-of-Thought\uff08\u601d\u7ef4\u94fe\uff09","text":"<p>\u601d\u7ef4\u94fe (Chain-of-thought\uff0cCoT) \u7684\u6982\u5ff5\u662f\u5728 Google \u7684\u8bba\u6587 \"Chain-of-Thought Prompting Elicits Reasoning in Large Language Models\" \u4e2d\u88ab\u9996\u6b21\u63d0\u51fa\u3002\u601d\u7ef4\u94fe\uff08CoT\uff09\u662f\u4e00\u79cd\u6539\u8fdb\u7684\u63d0\u793a\u7b56\u7565\uff0c\u7528\u4e8e\u63d0\u9ad8 LLM \u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\uff0c\u5982\u7b97\u672f\u63a8\u7406\u3001\u5e38\u8bc6\u63a8\u7406\u548c\u7b26\u53f7\u63a8\u7406\u3002</p> <p>CoT \u6ca1\u6709\u50cf ICL \u90a3\u6837\u7b80\u5355\u5730\u7528\u8f93\u5165\u8f93\u51fa\u5bf9\u6784\u5efa\u63d0\u793a\uff0c\u800c\u662f\u7ed3\u5408\u4e86\u4e2d\u95f4\u63a8\u7406\u6b65\u9aa4\uff0c\u8fd9\u4e9b\u6b65\u9aa4\u53ef\u4ee5\u4f5c\u4e3a\u5927\u6a21\u578b\u7684\u8f93\u5165\u3002\u7b80\u5355\u6765\u8bf4\uff0c\u601d\u7ef4\u94fe\u662f\u4e00\u79cd\u79bb\u6563\u5f0f\u63d0\u793a\u5b66\u4e60\uff0c\u66f4\u5177\u4f53\u5730\uff0c\u76f8\u6bd4\u4e8e\u4e4b\u524d\u7684\u4e0a\u4e0b\u6587\u5b66\u4e60\uff08\u5373\u4e0d\u8fdb\u884c\u8bad\u7ec3\uff0c\u5c06\u4f8b\u5b50\u6dfb\u52a0\u5230\u5f53\u524d\u6837\u672c\u8f93\u5165\u7684\u524d\u9762\uff0c\u8ba9\u6a21\u578b\u4e00\u6b21\u8f93\u5165\u8fd9\u4e9b\u6587\u672c\u8fdb\u884c\u8f93\u51fa\u5b8c\u6210\u4efb\u52a1\uff09\uff0c\u601d\u7ef4\u94fe\u591a\u4e86\u4e2d\u95f4\u7684\u63a8\u5bfc\u8fc7\u7a0b\u63d0\u793a\u3002</p> <p>\u4ee5\u4e00\u4e2a\u6570\u5b66\u9898\u4e3a\u4f8b\uff1a</p> <ul> <li>\u53ef\u4ee5\u770b\u5230\u6a21\u578b\u65e0\u6cd5\u505a\u51fa\u6b63\u786e\u7684\u56de\u7b54\u3002\u4f46\u5982\u679c\u8bf4\uff0c\u6211\u4eec\u7ed9\u6a21\u578b\u4e00\u4e9b\u5173\u4e8e\u89e3\u9898\u7684\u601d\u8def\uff0c\u5c31\u50cf\u6211\u4eec\u6570\u5b66\u8003\u8bd5\uff0c\u90fd\u4f1a\u628a\u89e3\u9898\u8fc7\u7a0b\u5199\u51fa\u6765\u518d\u6700\u7ec8\u5f97\u51fa\u7b54\u6848\uff0c\u4e0d\u7136\u65e0\u6cd5\u5f97\u5206\u3002CoT \u505a\u7684\u5c31\u662f\u8fd9\u4ef6\u4e8b\uff0c\u793a\u4f8b\u5982\u4e0b\uff1a</li> </ul> <ul> <li> <p>\u53ef\u4ee5\u770b\u5230\uff0c\u7c7b\u4f3c\u7684\u7b97\u672f\u9898\uff0c\u601d\u7ef4\u94fe\u63d0\u793a\u4f1a\u5728\u7ed9\u51fa\u7b54\u6848\u4e4b\u524d\uff0c\u8fd8\u4f1a\u81ea\u52a8\u7ed9\u51fa\u63a8\u7406\u6b65\u9aa4\uff1a</p> <p>\u201c\u7f57\u6770\u5148\u67095\u4e2a\u7403\uff0c2\u76d23\u4e2a\u7f51\u7403\u7b49\u4e8e6\u4e2a\uff0c5 + 6 = 11\u201d \u201c\u98df\u5802\u539f\u6765\u670923\u4e2a\u82f9\u679c\uff0c\u7528\u4e8620\u4e2a\uff0c23-20=3\uff1b\u53c8\u4e70\u4e866\u4e2a\u82f9\u679c\uff0c3+6=9</p> </li> </ul> <p>\u4e0a\u8ff0\u4f8b\u5b50\u8bc1\u660e\u4e86\u601d\u7ef4\u94fe\u63d0\u793a\u7ed9\u51fa\u4e86\u6b63\u786e\u7b54\u6848\uff0c\u800c\u76f4\u63a5\u7ed9\u51fa\u7b54\u6848\u7684\u4f20\u7edf\u63d0\u793a\u5b66\u4e60\uff0c\u7ed3\u679c\u662f\u9519\u7684\uff0c\u8fde\u5f88\u57fa\u672c\u7684\u6570\u5b66\u8ba1\u7b97\u90fd\u505a\u4e0d\u597d\u3002\u7b80\u5355\u6765\u8bf4\uff0c\u8bed\u8a00\u6a21\u578b\u5f88\u96be\u5c06\u6240\u6709\u7684\u8bed\u4e49\u76f4\u63a5\u8f6c\u5316\u4e3a\u4e00\u4e2a\u65b9\u7a0b\uff0c\u56e0\u4e3a\u8fd9\u662f\u4e00\u4e2a\u66f4\u52a0\u590d\u6742\u7684\u601d\u8003\u8fc7\u7a0b\uff0c\u4f46\u53ef\u4ee5\u901a\u8fc7\u4e2d\u95f4\u6b65\u9aa4\uff0c\u6765\u66f4\u597d\u5730\u63a8\u7406\u95ee\u9898\u7684\u6bcf\u4e2a\u90e8\u5206\u3002</p> <p>CoT\u5206\u7c7b\uff1a</p> <ul> <li>Few-shot CoT \uff1a\u662f ICL \u7684\u4e00\u79cd\u7279\u6b8a\u60c5\u51b5\uff0c\u5b83\u901a\u8fc7\u878d\u5408 CoT \u63a8\u7406\u6b65\u9aa4\uff0c\u5c06\u6bcf\u4e2a\u6f14\u793a\u3008input\uff0coutput\u3009\u6269\u5145\u4e3a\u3008input\uff0cCoT\uff0coutput\u3009\u3002</li> <li>Zero-shot CoT\uff1a\u4e0e Few-shot CoT \u4e0d\u540c \u5728 prompt \u4e2d\u4e0d\u5305\u62ec\u4eba\u5de5\u6807\u6ce8\u7684\u4efb\u52a1\u6f14\u793a\u3002\u76f8\u53cd\uff0c\u5b83\u76f4\u63a5\u751f\u6210\u63a8\u7406\u6b65\u9aa4\uff0c\u7136\u540e\u4f7f\u7528\u751f\u6210\u7684 CoT \u6765\u5bfc\u51fa\u7b54\u6848\u3002\uff08\u5176\u4e2d LLM \u9996\u5148\u7531 \u201cLet's think step by step\u201d \u63d0\u793a\u751f\u6210\u63a8\u7406\u6b65\u9aa4\uff0c\u7136\u540e\u7531 \u201cTherefore, the answer is\u201d \u63d0\u793a\u5f97\u51fa\u6700\u7ec8\u7b54\u6848\u3002\u4ed6\u4eec\u53d1\u73b0\uff0c\u5f53\u6a21\u578b\u89c4\u6a21\u8d85\u8fc7\u4e00\u5b9a\u89c4\u6a21\u65f6\uff0c\u8fd9\u79cd\u7b56\u7565\u4f1a\u5927\u5927\u63d0\u9ad8\u6027\u80fd\uff0c\u4f46\u5bf9\u5c0f\u89c4\u6a21\u6a21\u578b\u65e0\u6548\uff0c\u663e\u793a\u51fa\u663e\u8457\u7684\u6d8c\u73b0\u80fd\u529b\u6a21\u5f0f\uff09\u3002  </li> </ul> <p>\u4e00\u4e2a\u6709\u6548\u7684\u601d\u7ef4\u94fe\u5e94\u8be5\u5177\u6709\u4ee5\u4e0b\u7279\u70b9\uff1a</p> <ul> <li> <p>\u903b\u8f91\u6027\uff1a\u601d\u7ef4\u94fe\u4e2d\u7684\u6bcf\u4e2a\u601d\u8003\u6b65\u9aa4\u90fd\u5e94\u8be5\u662f\u6709\u903b\u8f91\u5173\u7cfb\u7684\uff0c\u5b83\u4eec\u5e94\u8be5\u76f8\u4e92\u8fde\u63a5\uff0c\u4ece\u800c\u5f62\u6210\u4e00\u4e2a\u5b8c\u6574\u7684\u601d\u8003\u8fc7\u7a0b\u3002</p> </li> <li> <p>\u5168\u9762\u6027\uff1a\u601d\u7ef4\u94fe\u5e94\u8be5\u5c3d\u53ef\u80fd\u5730\u5168\u9762\u548c\u7ec6\u81f4\u5730\u8003\u8651\u95ee\u9898\uff0c\u4ee5\u786e\u4fdd\u4e0d\u4f1a\u5ffd\u7565\u4efb\u4f55\u53ef\u80fd\u7684\u56e0\u7d20\u548c\u5f71\u54cd\u3002</p> </li> <li> <p>\u53ef\u884c\u6027\uff1a\u601d\u7ef4\u94fe\u4e2d\u7684\u6bcf\u4e2a\u601d\u8003\u6b65\u9aa4\u90fd\u5e94\u8be5\u662f\u53ef\u884c\u7684\uff0c\u4e5f\u5c31\u662f\u8bf4\uff0c\u5b83\u4eec\u5e94\u8be5\u53ef\u4ee5\u88ab\u5b9e\u9645\u64cd\u4f5c\u548c\u5b9e\u65bd\u3002</p> </li> <li> <p>\u53ef\u9a8c\u8bc1\u6027\uff1a\u601d\u7ef4\u94fe\u4e2d\u7684\u6bcf\u4e2a\u601d\u8003\u6b65\u9aa4\u90fd\u5e94\u8be5\u662f\u53ef\u4ee5\u9a8c\u8bc1\u7684\uff0c\u4e5f\u5c31\u662f\u8bf4\uff0c\u5b83\u4eec\u5e94\u8be5\u53ef\u4ee5\u901a\u8fc7\u5b9e\u9645\u7684\u6570\u636e\u548c\u4e8b\u5b9e\u6765\u9a8c\u8bc1\u5176\u6b63\u786e\u6027\u548c\u6709\u6548\u6027\u3002</p> </li> </ul>"},{"location":"01-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BE%AE%E8%B0%83%E4%B8%BB%E8%A6%81%E6%96%B9%E5%BC%8F/01-%E5%A4%A7%E6%A8%A1%E5%9E%8BPrompt-Tuning%E6%96%B9%E6%B3%95.html#6-prompt-tuning","title":"6 \u9762\u5411\u5c0f\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u7684Prompt-Tuning","text":"<p>\u5bf9\u4e8e\u8d85\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u4e0a\u7684Prompt-Tuning\u867d\u7136\u6709\u6548\uff0c\u4f46\u662f\u6bd5\u7adf\u9700\u8981\u5efa\u7acb\u5728\u8d85\u5927\u89c4\u6a21\u7684\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u4e0a\uff0c\u8fd9\u4e9b\u6a21\u578b\u7684\u53c2\u6570\u6570\u91cf\u901a\u5e38\u8d85\u8fc7100\u4ebf\uff0c\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u5f88\u96be\u5e94\u7528\u3002\u56e0\u6b64\u4f17\u591a\u7814\u7a76\u8005\u5f00\u59cb\u63a2\u7d22GPT-3\u8fd9\u5957\u601d\u8def\u5728\u5c0f\u89c4\u6a21\u7684\u8bed\u8a00\u6a21\u578b\uff08\u5982Bert\uff09\u4e0a\u8fd8\u662f\u5426\u9002\u7528\uff1f\u4e8b\u5b9e\u4e0a\uff0c\u8fd9\u5957\u65b9\u6cd5\u5728\u5c0f\u89c4\u6a21\u7684\u8bed\u8a00\u6a21\u578b\u4e0a\u662f\u53ef\u884c\u7684\uff0c\u4f46\u662f\u9700\u8981\u6ce8\u610f\uff1a</p> <ul> <li>\u5927\u6a21\u578b\u5728 few-shot/zero-shot \u5b66\u4e60\u4e0a\u8868\u73b0\u5f3a\u5927\uff0c\u800c\u5c0f\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u8fdc\u4e0d\u5982 GPT-3\u3002</li> <li>\u5c0f\u6a21\u578b\u5728\u9762\u5bf9\u5c11\u91cf\u8bad\u7ec3\u6570\u636e\u65f6\u66f4\u5bb9\u6613\u8fc7\u62df\u5408\uff0cprompt \u7684\u5fae\u5c0f\u6539\u52a8\u53ef\u80fd\u5bfc\u81f4\u6027\u80fd\u5267\u70c8\u6ce2\u52a8\u3002</li> </ul> <p>\u56e0\u6b64\uff0c\u9762\u5411\u5c0f\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u7684Prompt-Tuning\u7814\u7a76\u540c\u6837\u91cd\u8981\u3002</p>"},{"location":"01-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BE%AE%E8%B0%83%E4%B8%BB%E8%A6%81%E6%96%B9%E5%BC%8F/01-%E5%A4%A7%E6%A8%A1%E5%9E%8BPrompt-Tuning%E6%96%B9%E6%B3%95.html#61-prompt-tuningpet","title":"6.1 Prompt-Tuning\u7684\u9f3b\u7956\u2014PET\u6a21\u578b","text":"<p>PET\u6a21\u578b\uff08Pattern-Exploiting Training\uff09\u51fa\u81ea\u300aExploiting Cloze Questions for Few Shot Text Classification and Natural Language Inference\u300b\uff08EACL2021\uff09\uff0c\u5b83\u662f\u4e00\u79cd \u7ed3\u5408\u4e86\u63d0\u793a\u5b66\u4e60\u548c\u5c11\u91cf\u6807\u6ce8\u6837\u672c\u76d1\u7763\u5b66\u4e60 \uff08Few-shot Supervised Learning\uff09\u7684\u8bad\u7ec3\u65b9\u6cd5\uff0c\u4e3b\u8981\u7528\u4e8e\u5c06\u4e0b\u6e38\u4efb\u52a1\uff08\u5982\u6587\u672c\u5206\u7c7b\uff09\u8f6c\u5316\u4e3a\u7c7b\u4f3c\u9884\u8bad\u7ec3\u4efb\u52a1\uff08\u5982\u63a9\u7801\u8bed\u8a00\u5efa\u6a21\uff09\u7684\u5f62\u5f0f\u3002\u5b83\u901a\u8fc7\u8bbe\u8ba1\u81ea\u7136\u8bed\u8a00\u6a21\u5f0f\uff08pattern\uff09\u548c\u6807\u7b7e\u8bcd\uff08verbalizer\uff09\uff0c\u5c06\u8f93\u5165\u53e5\u5b50\u8f6c\u6362\u4e3a\u5e26\u6709[MASK]\u4f4d\u7f6e\u7684\u6587\u672c\uff0c\u4f8b\u5982\u201c\u8fd9\u4e2a\u7535\u5f71\u5f88[MASK]\u3002\u201d\uff0c\u7136\u540e\u7528\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\uff08\u5982BERT\uff09\u9884\u6d4b[MASK]\u4f4d\u7f6e\u7684\u8bcd\uff0c\u518d\u901a\u8fc7verbalizer\u8f6c\u6362\u6765\u5b8c\u6210\u5206\u7c7b\u4efb\u52a1\u3002</p> <p>PET\u6a21\u578b\u63d0\u51fa\u4e24\u4e2a\u5f88\u91cd\u8981\u7684\u7ec4\u4ef6\uff1a</p> <ul> <li>Pattern\uff08Template\uff09 \uff1a\u8bb0\u4f5cT, \u5373\u4e0a\u6587\u63d0\u5230\u7684Template\uff0c\u5176\u4e3a\u989d\u5916\u6dfb\u52a0\u7684\u5e26\u6709<code>[mask]</code>\u6807\u8bb0\u7684\u77ed\u6587\u672c\uff0c\u901a\u5e38\u4e00\u4e2a\u6837\u672c\u53ea\u6709\u4e00\u4e2aPattern\uff08\u56e0\u4e3a\u6211\u4eec\u5e0c\u671b\u53ea\u67091\u4e2a\u8ba9\u6a21\u578b\u9884\u6d4b\u7684<code>[mask]</code>\u6807\u8bb0\uff09\u3002\u7531\u4e8e\u4e0d\u540c\u7684\u4efb\u52a1\u3001\u4e0d\u540c\u7684\u6837\u672c\u53ef\u80fd\u4f1a\u6709\u5176\u66f4\u52a0\u5408\u9002\u7684pattern\uff0c\u56e0\u6b64\u5982\u4f55\u6784\u5efa\u5408\u9002\u7684pattern\u662fPrompt-Tuning\u7684\u7814\u7a76\u70b9\u4e4b\u4e00\uff1b</li> <li>Verbalizer \uff1a\u8bb0\u4f5cV, \u5373\u6807\u7b7e\u8bcd\u7684\u6620\u5c04\uff0c\u5bf9\u4e8e\u5177\u4f53\u7684\u5206\u7c7b\u4efb\u52a1\uff0c\u9700\u8981\u9009\u62e9\u6307\u5b9a\u7684\u6807\u7b7e\u8bcd\uff08label word\uff09\u3002\u4f8b\u5982\u60c5\u611f\u5206\u6790\u4e2d\uff0c\u6211\u4eec\u671f\u671bVerbalizer\u53ef\u80fd\u662f \uff08positive\u548cnegative\u662f\u7c7b\u6807\u7b7e\uff09\u3002\u540c\u6837\uff0c\u4e0d\u540c\u7684\u4efb\u52a1\u6709\u5176\u76f8\u5e94\u7684label word\uff0c\u4f46\u9700\u8981\u6ce8\u610f\u7684\u662f\uff0cVerbalizer\u7684\u6784\u5efa\u9700\u8981\u53d6\u51b3\u4e8e\u5bf9\u5e94\u7684Pattern\u3002\u56e0\u6b64 \u5982\u4f55\u6784\u5efaVerbalizer\u662f\u53e6\u4e00\u4e2a\u7814\u7a76\u6311\u6218 \u3002</li> </ul> <p>\u4e0a\u8ff0\u4e24\u4e2a\u7ec4\u4ef6\u88ab\u79f0\u4e3aPattern-Verbalizer-Pair\uff08PVP\uff09\uff0c\u5728\u540e\u7eed\u7684\u5927\u591a\u6570\u7814\u7a76\u4e2d\u5747\u91c7\u7528\u8fd9\u79cdPVP\u7ec4\u4ef6\u3002\u57fa\u4e8ePVP\u7684\u8bad\u7ec3\u76ee\u6807\u53ef\u4ee5\u5b9a\u4e49\u4e3a\u4e0b\u9762\u5f62\u5f0f\uff1a\u7ed9\u5b9a\u4e00\u4e2a\u53e5\u5b50\u548c\u5b83\u5bf9\u5e94\u7684\u6807\u7b7e\uff0c\u901a\u8fc7\u9884\u5148\u5b9a\u4e49\u7684 PVP \u7ec4\u4ef6\uff0c\u9996\u5148\u5c06\u53e5\u5b50\u6309\u7167\u6a21\u677f\u8fdb\u884c\u52a0\u5de5\uff0c\u628a\u539f\u672c\u7684\u4efb\u52a1\u8f6c\u5316\u4e3a\u586b\u7a7a\uff08Masked Language Modeling\uff09\u7684\u5f62\u5f0f\uff0c\u7136\u540e\u8ba9\u6a21\u578b\u9884\u6d4b\u6a21\u677f\u4e2d\u5360\u4f4d\u7b26\u7684\u4f4d\u7f6e\u5e94\u8be5\u51fa\u73b0\u7684\u8bcd\u3002\u901a\u8fc7\u5c06\u6a21\u578b\u9884\u6d4b\u7684\u8bcd\u4e0e\u6807\u7b7e\u6620\u5c04\u5bf9\u5e94\u8d77\u6765\uff0c\u8ba1\u7b97\u635f\u5931\u5e76\u4f18\u5316\u6a21\u578b\u53c2\u6570\u3002\u540e\u7eed\u6a21\u578b\u5728\u770b\u5230\u7c7b\u4f3c\u53e5\u5b50\u65f6\uff0c\u80fd\u591f\u901a\u8fc7\u6a21\u677f\u6b63\u786e\u5730\u9884\u6d4b\u5bf9\u5e94\u7684\u6807\u7b7e\u3002</p> <p></p> <p>\u76ee\u524d\u57fa\u4e8ePVP\u6846\u67b6\uff0c\u5f53\u524d\u6700\u9700\u8981\u5173\u6ce8\u7684\u95ee\u9898\u662f\u5982\u4f55\u9009\u62e9\u6216\u6784\u5efa\u5408\u9002\u7684Pattern\u548cVerbalizer \u3002\u4e00\u79cd\u7b80\u5355\u7684\u65b9\u6cd5\u662f\u6839\u636e\u7279\u5b9a\u4efb\u52a1\u7684\u6027\u8d28\u548c\u5148\u9a8c\u77e5\u8bc6\u4eba\u5de5\u8bbe\u8ba1\u6a21\u677f\u3002\u4f8b\u5982\u4e0a\u6587\u4f8b\u5b50\u4e2d\u901a\u5e38\u4f1a\u9009\u62e9<code>It was [mask].</code> \u4f5c\u4e3a\u60c5\u611f\u5206\u6790\u7c7b\u7684\u6a21\u677f\u3002\u4eba\u5de5\u6784\u5efa\u65b9\u6cd5\u867d\u7136\u76f4\u89c2\u7b80\u5355\uff0c\u4f46\u662f\u81f4\u547d\u95ee\u9898\u4e5f\u5f88\u7a81\u51fa\u3002\u6709\u76f8\u5173\u5de5\u4f5c\u5728\u5b9e\u9a8c\u4e2d\u53d1\u73b0\uff0c\u5728\u540c\u6837\u7684\u6570\u636e\u96c6\u548c\u8bad\u7ec3\u6761\u4ef6\u4e0b\uff0c \u9009\u62e9\u4e0d\u540c\u7684Pattern\u548cVerbalizer\u4f1a\u4ea7\u751f\u5dee\u5f02\u5f88\u5927\u7684\u7ed3\u679c \uff0c\u5982\u4e0b\u56fe\u6240\u793a\uff08\u4e00\u822c\u60c5\u51b5\u4e0b\uff0cTemplate\u7b49\u540c\u4e8ePattern\uff0cVerbalizer\u7b49\u540c\u4e8eLabel word\uff09\uff1a</p> <p>\u4ece\u4e0a\u56fe\u7ed3\u679c\u53ef\u53d1\u73b0\uff0c\u5728\u76f8\u540cPattern\u65f6\uff0c\u9009\u62e9\u4e0d\u540c\u7684label word\u5bf9\u7ed3\u679c\u5f71\u54cd\u5f88\u5927\uff0c\u540c\u7406\uff0c\u4e0d\u540c\u7684Pattern\u5bf9\u7ed3\u679c\u5f71\u54cd\u4e5f\u5f88\u660e\u663e\uff0c\u5728\u771f\u6b63\u5e94\u7528\u4e2d\uff0c\u8c03\u53c2\u8005\u9700\u8981\u5c1d\u8bd5\u591a\u4e2a\u4e0d\u540c\u7684\u6a21\u677f\u548c\u6807\u7b7e\u8bcd\u4ee5\u7a77\u4e3e\u51fa\u6700\u597d\u7684\u7ed3\u679c\uff0c\u5e76\u4e0d\u80fd\u5145\u5206\u53d1\u6325Prompt\u7b80\u5355\u5feb\u6377\u7684\u4f18\u52bf\u3002\u56e0\u6b64\u6211\u4eec\u603b\u7ed3\u4eba\u5de5\u8bbe\u8ba1\u65b9\u6cd5\u7684\u7f3a\u9677\uff1a</p> <ul> <li>\u91c7\u7528\u4eba\u5de5\u6784\u5efa\u7684\u65b9\u6cd5\u6210\u672c\u9ad8\uff0c\u9700\u8981\u4e0e\u9886\u57df\u4efb\u52a1\u76f8\u5173\u7684\u5148\u9a8c\u77e5\u8bc6\uff1b</li> <li>\u4eba\u5de5\u8bbe\u8ba1\u7684Pattern\u548cVerbalizer\u4e0d\u80fd\u4fdd\u8bc1\u83b7\u5f97\u6700\u4f18\u89e3\uff0c\u8bad\u7ec3\u4e0d\u7a33\u5b9a\uff0c\u4e0d\u540c\u7684PVP\u5bf9\u7ed3\u679c\u4ea7\u751f\u7684\u5dee\u5f02\u660e\u663e\uff0c\u65b9\u5dee\u5927\uff1b</li> <li>\u5728\u9884\u8bad\u7ec3\u9636\u6bb5MLM\u4efb\u52a1\u5e76\u975e\u5b8c\u5168\u6309\u7167PVP\u7684\u6a21\u5f0f\u8fdb\u884c\u8bad\u7ec3\u7684\uff08\u6bd4\u5982MLM\u8bad\u7ec3\u901a\u5e38\u90fd\u662f\u957f\u6587\u672c\uff0cmask\u7684\u6570\u91cf\u4e5f\u5e76\u975e\u53ea\u67091\u4e2a\uff0c\u9884\u6d4b\u7684\u6982\u7387\u5206\u5e03\u4e5f\u5e76\u975e\u662f\u6709\u9650\u7684\uff09\uff0c\u56e0\u6b64\u4eba\u5de5\u6784\u5efa\u7684Pattern\u548cVerbalizer\u4f7f\u5f97Prompt-Tuning\u4e0eMLM\u5728\u8bed\u4e49\u548c\u5206\u5e03\u4e0a\u4f9d\u7136\u5b58\u5728\u5dee\u5f02\u3002</li> </ul> <p>\u3010\u62d3\u5c55\uff1aPET\u6a21\u578b\u7684\u5b9e\u73b0\u6b65\u9aa4\u3011</p> <p>1\uff09\u51c6\u5907\u6570\u636e</p> <ul> <li>\u6709\u4e00\u4efd\u6807\u6ce8\u6570\u636e\u96c6 L\uff08few-shot\uff09\uff0c\u548c\u4e00\u4efd\u672a\u6807\u6ce8\u6570\u636e\u96c6 U\u3002</li> </ul> <p>2\uff09\u8bbe\u8ba1 pattern\uff08\u6a21\u677f\uff09\u548c verbalizer\uff08\u6620\u5c04\u8bcd\uff09</p> <ul> <li>Pattern\uff1a\u628a\u539f\u59cb\u8f93\u5165\u53d8\u4e3a cloze/\u586b\u7a7a\u5f62\u5f0f\uff08\u5305\u542b\u4e00\u4e2a [MASK]\uff09\uff0c\u4f8b\u5982\u60c5\u611f\u5206\u7c7b\uff1a<ul> <li>\u8f93\u5165\u53e5\u5b50 <code>X = \"\u8fd9\u90e8\u7535\u5f71\u592a\u68d2\u4e86\"</code> \u2192 pattern: <code>\"[X]\u3002\u8fd9\u6761\u8bc4\u8bba\u662f [MASK] \u3002\"</code></li> </ul> </li> <li>Verbalizer\uff1a\u628a\u7c7b\u522b\u6620\u5c04\u5230\u4e00\u4e2a\u6216\u591a\u4e2a\u8bcd\uff08label words\uff09\uff0c\u4f8b\u5982\uff1a<ul> <li>positive \u2192 <code>\"\u597d\"</code></li> <li>negative \u2192 <code>\"\u5dee\"</code></li> </ul> </li> </ul> <p>3\uff09\u4e3a\u591a\u4e2a pattern \u5206\u522b\u5fae\u8c03\uff08\u8bad\u7ec3 PET \u5b50\u6a21\u578b\uff09</p> <ul> <li>\u9009\u62e9 k\u4e2a\u4e0d\u540c\u7684 pattern\uff08\u4e0d\u540c\u98ce\u683c\u7684\u63d0\u793a\uff09\uff0c\u5bf9\u6bcf\u4e2a pattern\uff1a<ul> <li>\u628a L\u4e2d\u7684\u6837\u672c\u8f6c\u6362\u4e3a cloze \u683c\u5f0f\uff1b</li> <li>\u7528 Masked LM \u7684\u76ee\u6807\u8bad\u7ec3\uff08\u53ea\u5173\u6ce8 [MASK] \u4f4d\u7f6e\u9884\u6d4b\u662f\u5426\u4e3a verbalizer \u6307\u5b9a\u7684\u8bcd\uff09\uff0c\u5f97\u5230\u6a21\u578b Mp\u3002</li> </ul> </li> <li>\u635f\u5931\u8ba1\u7b97\u901a\u5e38\u662f\u9488\u5bf9 [MASK] \u4f4d\u7f6e\u4e0a\u9884\u6d4b\u5230 label words \u7684\u4ea4\u53c9\u71b5\u3002</li> </ul> <p>4\uff09\u7528\u5b50\u6a21\u578b\u96c6\u5408\uff08ensemble\uff09\u5bf9\u672a\u6807\u6ce8\u6570\u636e\u505a\u8f6f\u6807\u6ce8</p> <ul> <li>\u5bf9\u6bcf\u4e2a x\u2208U\uff0c\u7528\u6bcf\u4e2a\u5b50\u6a21\u578b\u9884\u6d4b [MASK] \u7684\u8bcd\u8868\u5206\u5e03\uff0c\u901a\u8fc7 verbalizer \u628a\u8bcd\u8868\u6982\u7387\u805a\u5408\u6210\u7c7b\u522b\u6982\u7387\uff1b</li> <li>\u5bf9\u6240\u6709 k\u4e2a\u6a21\u578b\u5e73\u5747/\u52a0\u6743\uff0c\u5f97\u5230 \u8f6f\u6807\u7b7e\uff08soft labels\uff09 P(y\u2223x)\u3002</li> </ul> <p>5\uff09\u84b8\u998f / \u8bad\u7ec3\u6700\u7ec8\u5206\u7c7b\u5668\uff08distillation\uff09</p> <ul> <li>\u7528 L\uff08\u771f\u5b9e\u6807\u7b7e\uff09\u548c U\uff08\u8f6f\u6807\u7b7e\uff09\u5408\u5e76\u8bad\u7ec3\u4e00\u4e2a\u6700\u7ec8\u7684\u5224\u522b\u5f0f\u5206\u7c7b\u5668C\uff08\u901a\u5e38\u662f\u540c\u4e00\u9884\u8bad\u7ec3 LM \u52a0 classifier head\uff0c\u4f7f\u7528\u4ea4\u53c9\u71b5\u5bf9\u6807\u7b7e\u8fdb\u884c\u77e5\u8bc6\u84b8\u998f\uff09\u3002</li> <li>\u8fd9\u6837\u5f97\u5230\u7684\u6700\u7ec8\u6a21\u578b\u6bd4\u5355\u4e2a PET \u5b50\u6a21\u578b\u66f4\u7a33\u5b9a\u3001\u6027\u80fd\u66f4\u597d\u3002</li> </ul> <p></p>"},{"location":"01-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BE%AE%E8%B0%83%E4%B8%BB%E8%A6%81%E6%96%B9%E5%BC%8F/01-%E5%A4%A7%E6%A8%A1%E5%9E%8BPrompt-Tuning%E6%96%B9%E6%B3%95.html#62-prompt-oriented-fine-tuning","title":"6.2 Prompt-Oriented Fine-Tuning","text":"<p>\u5728PET\u6a21\u578b\u4e2d\uff0c\u5bf9MLM\u8fdb\u884c\u5fae\u8c03\u65f6\uff0c\u4f7f\u7528\u7684\u65b9\u5f0f\u4e3aPrompt-Oriented Fine-Tuning\u3002 </p> <p>Prompt-Oriented Fine-Tuning\u7684 \u672c\u8d28\u662f\u5c06\u76ee\u6807\u4efb\u52a1\u8f6c\u6362\u4e3a\u9002\u5e94\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u9884\u8bad\u7ec3\u4efb\u52a1\uff0c\u4ee5\u9002\u5e94\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u5b66\u4e60\u4f53\u7cfb \u3002</p> <p>\u6839\u636e\u63d0\u793a\u7684\u7c7b\u578b\u4e0d\u540c\uff0cPOFT\u65b9\u6cd5\u4e3b\u8981\u5206\u6210\u4e09\u79cd\u7c7b\u578b\uff1a</p> <ul> <li>\u79bb\u6563\u63d0\u793a\uff1a\u4e5f\u53eb\u786c\u6a21\u7248\uff0c\u5176\u63d0\u793a\u662f\u7531\u771f\u5b9e\u7684\u81ea\u7136\u8bed\u8a00\u5355\u8bcd\u6216\u7b26\u53f7\u7ec4\u6210\uff0c\u76f4\u63a5\u62fc\u63a5\u5230\u8f93\u5165\u4e2d\u3002</li> <li>\u8fde\u7eed\u63d0\u793a\uff1a\u63d0\u793a\u4e0d\u662f\u5b9e\u9645\u7684\u5355\u8bcd\uff0c\u800c\u662f**\u53ef\u8bad\u7ec3\u7684\u5411\u91cf**\uff0c\u63d2\u5165\u5230\u8f93\u5165 embedding \u5e8f\u5217\u4e2d\u3002</li> <li>\u6df7\u5408\u63d0\u793a\uff1a\u540c\u65f6\u4f7f\u7528**\u4eba\u5de5\u53ef\u8bfb\u7684\u79bb\u6563 token**\u548c**\u53ef\u8bad\u7ec3\u7684\u8fde\u7eed\u5411\u91cf**\u3002</li> </ul> <p>\u6309\u7167\u8bad\u7ec3\u65f6\u53c2\u6570\u66f4\u65b0\u7684\u8303\u56f4\u4e0d\u540c\uff0cPOFT\u65b9\u6cd5\u4e3b\u8981\u5206\u6210\u4e09\u79cd\u7c7b\u578b\uff1a</p> <ul> <li>\u5168\u91cf\u5fae\u8c03\uff08Full Fine-Tuning\uff09\uff1a\u6a21\u578b\u6240\u6709\u53c2\u6570\u90fd\u53c2\u4e0e\u66f4\u65b0\uff0c\u5305\u62ec\u9884\u8bad\u7ec3\u6a21\u578b\u53c2\u6570\u548c\u4e0b\u6e38\u4efb\u52a1\u5c42\u53c2\u6570\u3002\u5982PET\u6a21\u578b\u3002</li> <li>\u90e8\u5206\u53c2\u6570\u5fae\u8c03\uff08Partial Fine-Tuning\uff09\uff1a\u53ea\u66f4\u65b0\u9884\u8bad\u7ec3\u6a21\u578b\u4e2d\u7684\u4e00\u90e8\u5206\u53c2\u6570\uff0c\u6bd4\u5982\u9ad8\u5c42 transformer block\u3001\u67d0\u4e9b attention \u5c42\u6216\u7279\u5b9a\u6a21\u5757\uff0c\u5176\u4f59\u53c2\u6570\u51bb\u7ed3\u3002\u5982Adapter Tuning\u3002</li> <li>\u4ec5\u63d0\u793a\u53c2\u6570\u5fae\u8c03\uff08Prompt-Only Tuning\uff09\uff1a\u51bb\u7ed3\u539f\u59cb\u9884\u8bad\u7ec3\u6a21\u578b\u53c2\u6570\uff0c\u53ea\u8bad\u7ec3 prompt \u53c2\u6570\u3002\u5982P-tuning\u3001Prompt Tuning\u7b49\u3002</li> </ul> <p>\u5177\u4f53\u6765\u8bf4\uff0cPET\u4e2d\u4f7f\u7528\u7684\u65b9\u6cd5\u4e3a\uff1a \u57fa\u4e8e\u786c\u6a21\u677f+ \u5168\u91cf\u5fae\u8c03</p> <p>\u5728PET\u65b9\u6cd5\u4e2d\uff0c\u4f7f\u7528\u7684\u65b9\u5f0f\u662fPrompt-Tuning+Fine-Tuning\u7684\u7ed3\u5408\u4f53\uff0c\u6240\u4ee5\u9884\u8bad\u7ec3\u6a21\u578b\u53c2\u6570\u662f\u4f1a\u53d8\u7684\u3002\u6240\u4ee5PET\u4e2d\u4f7f\u7528\u5fae\u8c03\u65b9\u6cd5\uff0c\u5b9e\u9645\u4e0a\u662f\u4e00\u79cdFine-Tuning\u7684\u5347\u7ea7\u7248\u3002\u867d\u7136 PET \u4e5f\u662f\u5728\u4f18\u5316\u6574\u4e2a\u6a21\u578b\u7684\u53c2\u6570\uff0c\u4f46\u662f\u76f8\u6bd4\u4e8e\u4f20\u7edf\u7684 Finetuning \u65b9\u6cd5\uff0c\u5bf9\u6570\u636e\u91cf\u9700\u6c42\u66f4\u5c11\u3002</p> <p>\u5168\u91cf\u5fae\u8c03\u65b9\u6cd5\u5728BERT\u7c7b\u76f8\u5bf9\u8f83\u5c0f\u7684\u6a21\u578b\u4e0a\u8868\u73b0\u8f83\u597d\uff0c\u4f46\u662f\u968f\u7740\u6a21\u578b\u8d8a\u6765\u8d8a\u5927\uff0c\u5982\u679c\u6bcf\u6b21\u9488\u5bf9\u4e0b\u6e38\u4efb\u52a1\u90fd\u9700\u8981\u66f4\u65b0\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u53c2\u6570\uff0c\u8d44\u6e90\u6210\u672c\u53ca\u65f6\u95f4\u6210\u672c\u90fd\u4f1a\u5f88\u9ad8\u3002\u56e0\u6b64\u540e\u7eed\u9646\u7eed\u63d0\u51fa\u4e86\u4e0d\u66f4\u65b0\u9884\u8bad\u7ec3\u6a21\u578b\u53c2\u6570\uff0c\u5355\u7eaf\u53ea\u9488\u5bf9prompt\u8fdb\u884c\u8c03\u4f18\u7684\u65b9\u6cd5\uff0c\u5373\u57fa\u4e8eSoft Prompt\u7684\u5fae\u8c03\u65b9\u6cd5\uff0c\u5982 Prompt Tuning\u3001P-tuning \u548c Prefix Tuning \u7b49\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u4efb\u52a1\u8fc1\u79fb\u6210\u672c</p> <p>\u53e6\u5916\uff0c\u5728PET\u65b9\u6cd5\u4e2d\u4f7f\u7528\u7684\u786c\u6a21\u7248\uff0c\u540c\u6837\u5b58\u5728\u5f88\u591a\u95ee\u9898\u3002</p> <p>Hard Prompt (\u79bb\u6563\u63d0\u793a)\uff1a\u662f\u4e00\u79cd\u56fa\u5b9a\u7684\u63d0\u793a\u6a21\u677f\uff0c\u901a\u8fc7\u5c06\u7279\u5b9a\u7684\u5173\u952e\u8bcd\u6216\u77ed\u8bed(\u771f\u5b9e\u7684\u6587\u672c\u5b57\u7b26\u4e32)\u76f4\u63a5\u5d4c\u5165\u5230\u6587\u672c\u4e2d\uff0c\u5f15\u5bfc\u6a21\u578b\u751f\u6210\u7b26\u5408\u8981\u6c42\u7684\u6587\u672c\u3002\u4e00\u822c\u6839\u636e\u7ecf\u9a8c\u7684\u4eba\u5de5\u8bbe\u8ba1\u6216\u81ea\u52a8\u5316\u641c\u7d22\u4ea7\u751f\u7684\u3002</p> <p>\u8fd9\u79cd\u63d0\u793a\u65b9\u6cd5\u7684\u7279\u70b9\u5728\u4e8e\uff0c\u63d0\u793a\u6a21\u677f\u662f\u56fa\u5b9a\u7684\uff0c\u4e0d\u80fd\u6839\u636e\u4e0d\u540c\u7684\u4efb\u52a1\u548c\u9700\u6c42\u8fdb\u884c\u8c03\u6574\u3002\u53e6\u5916\uff0c\u4ecePVP\u7684\u4ecb\u7ecd\u4e2d\u53ef\u4ee5\u770b\u51fa\u4f7f\u7528\u786c\u6a21\u677f\u65f6 \u6539\u53d8prompt\u6216\u6539\u53d8prompt\u4e2d\u7684\u5355\u4e2a\u5355\u8bcd \u4f1a\u7ed9\u5b9e\u9a8c\u7ed3\u679c\u5e26\u6765\u5de8\u5927\u7684\u5dee\u5f02\u3002</p> <p>\u4e0b\u9762\u662f\u5e38\u89c1\u4e0b\u6e38\u4efb\u52a1\u7684Prompt\u8bbe\u8ba1\uff1a</p> <p>\u6b63\u56e0\u4e3a\u786c\u6a21\u7248\u7684\u95ee\u9898\uff0c\u540e\u7eed\u7814\u7a76\u5bf9\u5b83\u8fdb\u884c\u4e86\u5f88\u591a\u4f18\u5316\uff0c\u540e\u6765\u7d22\u6027\u76f4\u63a5\u653e\u5f03\u786c\u6a21\u677f\uff0c\u4f18\u5316 prompt token embedding\uff0c\u5373\u4f7f\u7528Soft Prompt\u3002</p>"},{"location":"01-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BE%AE%E8%B0%83%E4%B8%BB%E8%A6%81%E6%96%B9%E5%BC%8F/01-%E5%A4%A7%E6%A8%A1%E5%9E%8BPrompt-Tuning%E6%96%B9%E6%B3%95.html#63-soft-prompt","title":"6.3 Soft Prompt\u53ca\u5fae\u8c03\u65b9\u6cd5","text":""},{"location":"01-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BE%AE%E8%B0%83%E4%B8%BB%E8%A6%81%E6%96%B9%E5%BC%8F/01-%E5%A4%A7%E6%A8%A1%E5%9E%8BPrompt-Tuning%E6%96%B9%E6%B3%95.html#631","title":"6.3.1 \u8fde\u7eed\u63d0\u793a\u6a21\u677f","text":"<p>Soft Prompt (\u8fde\u7eed\u63d0\u793a) \uff1a\u662f\u6307\u901a\u8fc7\u7ed9\u6a21\u578b\u8f93\u5165\u4e00\u4e2a\u53ef\u53c2\u6570\u5316\u7684\u63d0\u793a\u6a21\u677f\uff0c\u4ece\u800c\u5f15\u5bfc\u6a21\u578b\u751f\u6210\u7b26\u5408\u7279\u5b9a\u8981\u6c42\u7684\u6587\u672c\u3002\u8fd9\u79cd\u63d0\u793a\u65b9\u6cd5\u7684\u7279\u70b9\u5728\u4e8e\uff0c\u63d0\u793a\u6a21\u677f\u4e2d\u7684\u53c2\u6570\u53ef\u4ee5\u6839\u636e\u5177\u4f53\u4efb\u52a1\u548c\u9700\u6c42\u8fdb\u884c\u8c03\u6574\uff0c\u4ee5\u8fbe\u5230\u6700\u4f73\u7684\u751f\u6210\u6548\u679c\u3002</p> <p>Soft Prompt\u76ee\u7684\u5176**\u5c06\u6a21\u677f\u8f6c\u6362\u4e3a\u53ef\u4ee5\u8fdb\u884c\u4f18\u5316\u7684\u8fde\u7eed\u5411\u91cf**\uff0c\u6362\u53e5\u8bdd\u8bf4\uff0c\u6211\u4eec\u4e0d\u9700\u8981\u663e\u5f0f\u5730\u6307\u5b9a\u8fd9\u4e9b\u6a21\u677f\u4e2d\u5404\u4e2atoken\u5177\u4f53\u662f\u4ec0\u4e48\uff0c\u800c\u53ea\u9700\u8981\u5728\u8bed\u4e49\u7a7a\u95f4\u4e2d\u8868\u793a\u4e00\u4e2a\u5411\u91cf\u5373\u53ef\u3002</p> <p>\u8fd9\u6837\uff0c\u4e0d\u540c\u7684\u4efb\u52a1\u3001\u6570\u636e\u53ef\u4ee5\u81ea\u9002\u5e94\u5730\u5728\u8bed\u4e49\u7a7a\u95f4\u4e2d\u5bfb\u627e\u82e5\u5e72\u5408\u9002\u7684\u5411\u91cf\uff0c\u6765\u4ee3\u8868\u6a21\u677f\u4e2d\u7684\u6bcf\u4e00\u4e2a\u8bcd\uff0c\u76f8\u8f83\u4e8e\u663e\u5f0f\u7684token\uff0c\u8fd9\u7c7btoken\u79f0\u4e3a \u4f2a\u6807\u8bb0\uff08Pseudo Token\uff09 \u3002\u4e0b\u9762\u7ed9\u51fa\u57fa\u4e8e\u8fde\u7eed\u63d0\u793a\u7684\u6a21\u677f\u5b9a\u4e49\uff1a</p> <p>\u5047\u8bbe\u9488\u5bf9\u5206\u7c7b\u4efb\u52a1\uff0c\u7ed9\u5b9a\u4e00\u4e2a\u8f93\u5165\u53e5\u5b50x\uff0c\u8fde\u7eed\u63d0\u793a\u7684\u6a21\u677f\u53ef\u4ee5\u5b9a\u4e49\u4e3aT=[x],[v1],[v2],...,[vn][MASK]\uff1a\u5176\u4e2d[v1]\u5219\u662f\u4f2a\u6807\u8bb0\uff0c\u5176\u4ec5\u4ee3\u8868\u4e00\u4e2a\u62bd\u8c61\u7684token\uff0c\u5e76\u6ca1\u6709\u5b9e\u9645\u7684\u542b\u4e49\uff0c\u672c\u8d28\u4e0a\u662f\u4e00\u4e2a\u5411\u91cf\u3002</p> <p>\u603b\u7ed3\u6765\u8bf4\uff1aSoft Prompt\u65b9\u6cd5\uff0c\u662f\u5c06\u6a21\u677f\u53d8\u4e3a\u53ef\u8bad\u7ec3\u7684\u53c2\u6570\uff0c\u4e0d\u540c\u7684\u6837\u672c\u53ef\u4ee5\u5728\u8fde\u7eed\u7684\u5411\u91cf\u7a7a\u95f4\u4e2d\u5bfb\u627e\u5408\u9002\u7684\u4f2a\u6807\u8bb0\uff0c\u540c\u65f6\u4e5f\u589e\u52a0\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u3002\u56e0\u6b64\uff0c\u8fde\u7eed\u6cd5\u9700\u8981\u5f15\u5165\u5c11\u91cf\u7684\u53c2\u6570\u5e76\u5728\u8bad\u7ec3\u65f6\u8fdb\u884c\u53c2\u6570\u66f4\u65b0\uff0c\u4f46\u9884\u8bad\u7ec3\u6a21\u578b\u53c2\u6570\u662f\u4e0d\u53d8\u7684\uff0c\u53d8\u7684\u662fprompt token\u5bf9\u5e94\u7684\u8bcd\u5411\u91cf\uff08Word Embedding\uff09\u8868\u5f81\u53ca\u5176\u4ed6\u5f15\u5165\u7684\u5c11\u91cf\u53c2\u6570\u3002</p> <p>\u76ee\u524d\u57fa\u4e8e\u8fde\u7eed\u63d0\u793a\u7684Prompt-Tuning\u7684\u5b9e\u73b0\u65b9\u6cd5\uff0c\u4ee5\u4e0b\u5217\u4e09\u7bc7\u8bba\u6587\u4e3a\u4ee3\u8868\uff0c\u5206\u522b\u4f5c\u7b80\u8981\u4ecb\u7ecd\uff1a</p> <ul> <li>\u300aThe Power of Scale for Parameter-Efficient Prompt Tuning\u300b\uff1a\u4ee3\u8868\u65b9\u6cd5\u4e3aPrompt Tuning</li> <li>\u300aGPT Understands, Too\u300b\uff1a\u4ee3\u8868\u65b9\u6cd5\u4e3aP-tuning</li> <li>\u300aPPT: Pre-trained Prompt Tuning for Few-shot Learning\u300b\uff1a\u4ee3\u8868\u65b9\u6cd5PPT</li> </ul>"},{"location":"01-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BE%AE%E8%B0%83%E4%B8%BB%E8%A6%81%E6%96%B9%E5%BC%8F/01-%E5%A4%A7%E6%A8%A1%E5%9E%8BPrompt-Tuning%E6%96%B9%E6%B3%95.html#632-prompt-tuningnlg","title":"6.3.2 Prompt Tuning\uff08NLG\u4efb\u52a1\uff09","text":"<p>Prompt Tuning\uff08\u57fa\u4e8eT5\u6a21\u578b\u6765\u505a\u7684\uff09\u65b9\u6cd5\u4e3a\u6bcf\u4e00\u4e2a\u8f93\u5165\u6587\u672c\u5047\u8bbe\u4e00\u4e2a\u56fa\u5b9a\u524d\u7f00\u63d0\u793a\uff0c\u8be5\u63d0\u793a\u8868\u7531\u795e\u7ecf\u7f51\u7edc\u53c2\u6570\u5316\uff0c\u5e76\u5728\u4e0b\u6e38\u4efb\u52a1\u5fae\u8c03\u65f6\u8fdb\u884c\u66f4\u65b0\uff0c\u6574\u4e2a\u8fc7\u7a0b\u4e2d\u9884\u8bad\u7ec3\u7684\u5927\u6a21\u578b\u53c2\u6570\u88ab\u51bb\u7ed3\u3002</p> <p>\u5f62\u5f0f\u5316\u7684\u63cf\u8ff0\u5982\u4e0b\uff1a</p> <p>\u7ed9\u5b9a n\u4e2atokens\uff0c\u8bb0\u4f5cx1, ...,xn \uff0c\u901a\u8fc7\u4e00\u4e2a\u9884\u8bad\u7ec3\u6a21\u578b\u5bf9\u5e94\u7684embedding table\uff0c\u53ef\u4ee5\u5c06n\u4e2atoken\u8868\u793a\u4e3a\u4e00\u4e2a\u5411\u91cf\u77e9\u9635(X_e-&gt;R^{n*e})\uff0c\u5176\u4e2de\u662f\u5411\u91cf\u7684\u7ef4\u5ea6\uff08\u5176\u4e0e\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u914d\u7f6e\u6709\u5173\uff0c\u4f8b\u5982BERT-base\u662f768\uff09\u3002\u8fde\u7eed\u6a21\u677f\u4e2d\u7684\u6bcf\u4e2a\u4f2a\u6807\u8bb0v_i\u53ef\u4ee5\u89c6\u4e3a\u53c2\u6570\uff0c\u4e5f\u53ef\u4ee5\u89c6\u4e3a\u4e00\u4e2atoken\uff0c\u56e0\u6b64\uff0c\u53ef\u4ee5\u901a\u8fc7\u53e6\u4e00\u4e2aembedding table\u83b7\u5f97p\u4e2a\u4f2a\u6807\u8bb0token\u6807\u8bb0\u4e3a\u5411\u91cf\u77e9\u9635(P_e-&gt;R^{p*e})\uff0c\u7136\u540e\u5c06\u6587\u672c\u548cPrompt\u62fc\u63a5\u83b7\u5f97\u65b0\u7684\u8f93\u5165[P_e:X_e]-&gt;R^{(p+n)*e}.\u8fd9\u4e2a\u65b0\u7684\u8f93\u5165\u5c06\u4f1a\u9001\u5165\u9884\u8bad\u7ec3\u6a21\u578b\u4e2d\u8fdb\u884c\u8bad\u7ec3\u3002\u6ce8\u610f\uff0c\u53ea\u6709prompt\u5bf9\u5e94\u7684\u5411\u91cf\u8868\u5f81\u53c2\u6570P(P_e-&gt;R^{p*e})\u4f1a\u968f\u7740\u8bad\u7ec3\u8fdb\u884c\u66f4\u65b0\u3002 </p> <p>\u6bcf\u4e2a\u4f2a\u6807\u8bb0\u7684\u521d\u59cb\u5316\u53ef\u4ee5\u6709\u4e0b\u5217\u51e0\u79cd\u60c5\u51b5\uff1a</p> <ul> <li>\u6700\u7b80\u5355\u7684\u662f\u968f\u673a\u521d\u59cb\u5316\uff1a\u5373\u968f\u673a\u521d\u59cb\u5316\u4e00\u4e2a\u9762\u5411\u6240\u6709\u4f2a\u6807\u8bb0\u7684embedding table\uff0c\u53ef\u91c7\u7528\u6b63\u6001\u5206\u5e03\u6216\u8005\u5747\u5300\u5206\u5e03\u7b49\uff1b</li> <li>\u6bcf\u4e2atoken\u4f7f\u7528\u9884\u8bad\u7ec3\u6a21\u578b\u5df2\u6709\u7684embedding table\u8fdb\u884c\u521d\u59cb\u5316\uff0c\u6b64\u65f6\uff0c\u6bcf\u4e00\u4e2a\u4f2a\u6807\u8bb0\u5148\u968f\u673a\u6307\u5b9a\u8bcd\u8868\u4e2d\u7684\u4e00\u4e2a\u8bcd\uff0c\u5e76\u53d6\u5bf9\u5e94\u8bcd\u7684embedding\u4f5c\u4e3a\u8fd9\u4e2a\u4f2a\u6807\u8bb0\u7684\u521d\u59cb\u5316\uff1b</li> <li>\u5728\u5206\u7c7b\u4efb\u52a1\u4e0a\uff0c\u4f7f\u7528label word\uff08verbalizer\uff09\u5bf9\u5e94\u7684embedding\u4f5c\u4e3a\u521d\u59cb\u5316\uff0c\u53ef\u4ee5\u6709\u6548\u9650\u5236\u6a21\u578b\u8f93\u51fa\u7684\u662f\u9884\u8bbe\u7684\u8f93\u51fa\u7c7b\u5bf9\u5e94\u7684word\u3002</li> </ul> <p>\u56e0\u6b64\uff0c\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\uff0c\u6bcf\u4e2a\u4f2a\u6807\u8bb0\u5bf9\u5e94\u7684\u53c2\u6570\u90fd\u53ef\u4ee5\u5f97\u5230\u8bad\u7ec3\uff0c\u5bf9\u4e8e\u4e0d\u540c\u7684\u8f93\u5165\u53e5\u5b50 \uff0c\u8fd9\u4e9b\u4f2a\u6807\u8bb0\u5bf9\u5e94\u7684embedding\u4e5f\u5404\u4e0d\u76f8\u540c\uff0c\u8fbe\u5230\u4e86\u9884\u671f\u7684\u76ee\u7684\u3002</p> <p>Prompt Tuning\u7279\u70b9\uff1a</p> <ul> <li>\u4f18\u70b9\uff1a<ul> <li>\u5927\u6a21\u578b\u7684\u5fae\u8c03\u65b0\u8303\u5f0f</li> <li>\u6a21\u578b\u53c2\u6570\u89c4\u6a21\u5927\u4e86\u4e4b\u540e\uff0c\u53ef\u4ee5\u5c06\u5927\u6a21\u578b\u53c2\u6570\u56fa\u5b9a\uff0c\u6307\u5b9a\u9644\u52a0\u53c2\u6570\u6765\u9002\u914d\u4e0b\u6e38\u4efb\u52a1\uff0c\u800c\u4e14\u9002\u914d\u6027\u80fd\u57fa\u672c\u548c\u5168\u53c2\u6570\u5fae\u8c03\u76f8\u5f53\u3002</li> </ul> </li> <li>\u7f3a\u70b9\uff1a<ul> <li>\u5728\u5c0f\u6837\u672c\u5b66\u4e60\u573a\u666f\u4e0a\u8868\u73b0\u4e0d\u592a\u884c</li> <li>\u6536\u655b\u901f\u5ea6\u6bd4\u8f83\u6162</li> <li>\u8c03\u53c2\u6bd4\u8f83\u590d\u6742</li> </ul> </li> </ul>"},{"location":"01-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BE%AE%E8%B0%83%E4%B8%BB%E8%A6%81%E6%96%B9%E5%BC%8F/01-%E5%A4%A7%E6%A8%A1%E5%9E%8BPrompt-Tuning%E6%96%B9%E6%B3%95.html#633-p-tuningnlu","title":"6.3.3 P-tuning\uff08NLU\u4efb\u52a1\uff09","text":"<p>P-tuning\u7684\u8be6\u7ec6\u5185\u5bb9\u8bf7\u53c2\u8003\u8bba\u6587\u89e3\u8bfb\uff1aGPT Understands, Too\u3002</p> <p>P-tuning\u662f\u53e6\u4e00\u4e2a\u5177\u6709\u4ee3\u8868\u6027\u7684\u8fde\u7eed\u63d0\u793a\u65b9\u6cd5\uff0c\u4e3b\u8981\u9488\u5bf9\u7684\u662fNLU\u4efb\u52a1\uff0c\u65b9\u6cd5\u56fe\u5982\u4e0b\u6240\u793a\uff08\u56fe\u4e2d\u7684P_i\u7b49\u4ef7\u4e8e\u4e0a\u6587\u7684v_i \uff0c\u8868\u793a\u4f2a\u6807\u8bb0\uff09, \u8c37\u6b4c\u4e8e2021\u5e74\u53d1\u8868\u3002</p> <p>P-tuning \u7684\u6838\u5fc3\u601d\u60f3\u662f\uff1a\u7528\u4e00\u4e2a\u5c0f\u7684\u53ef\u8bad\u7ec3\u6a21\u5757\u628a\u4e00\u7ec4\u201c\u8fde\u7eed\u63d0\u793a\u5411\u91cf\u201d\u751f\u6210\u5e76\u63d2\u5165\u5230\u539f\u59cb\u8f93\u5165 embedding \u4e2d\uff0c\u4ee4**\u51bb\u7ed3\u7684\u9884\u8bad\u7ec3\u6a21\u578b**\u5728\u4e0b\u6e38\u4efb\u52a1\u4e0a\u4ea7\u751f\u6b63\u786e\u8f93\u51fa\uff0c\u8bad\u7ec3\u65f6\u4ec5\u66f4\u65b0 prompt encoder\uff08\u6216\u63d0\u793a\u5411\u91cf\uff09\uff0c\u4ece\u800c\u5b9e\u73b0\u4f4e\u6210\u672c\u9ad8\u6548\u7684\u8c03\u4f18\u3002</p> <p>P-Tuning\u65b9\u6cd5\u4e2d\u56db\u4e2a\u6280\u5de7\u70b9\uff1a</p> <ul> <li>\u8003\u8651\u5230\u8fd9\u4e9b\u4f2a\u6807\u8bb0\u7684\u76f8\u4e92\u4f9d\u8d56\u5173\u7cfb \uff1aP-Tuning v1\u63d0\u51fa**\u5c06 Prompt \u8f6c\u6362\u4e3a\u53ef\u4ee5\u5b66\u4e60\u7684 Embedding \u5c42**\uff0c\u5b83\u8ba4\u4e3a[P_1] \u4e0e [P_2]\u662f\u6709\u5148\u540e\u5173\u7cfb\u7684\uff0c\u56e0\u6b64\u5f15\u5165Prompt Encoder\uff0c\u5b9e\u9645\u8fc7\u7a0b\u4e2d\u91c7\u7528Bi-LSTM+\u524d\u9988\u795e\u7ecf\u7f51\u7edc\u7ec4\u6210\uff0c\u7f16\u7801\u4e4b\u540e\u4e0e\u5176\u4ed6\u5411\u91cf\u8fdb\u884c\u62fc\u63a5\u4e4b\u540e\u6b63\u5e38\u8f93\u5165 LLM\u3002</li> <li>\u6307\u5b9a\u4e0a\u4e0b\u6587\u8bcd \uff1a\u5982\u679c\u6a21\u677f\u5168\u90e8\u662f\u4f2a\u6807\u8bb0\uff0c\u5728\u8bad\u7ec3\u65f6\u65e0\u6cd5\u5f88\u597d\u5730\u63a7\u5236\u8fd9\u4e9b\u6a21\u677f\u671d\u7740\u4e0e\u5bf9\u5e94\u53e5\u5b50\u76f8\u4f3c\u7684\u8bed\u4e49\u4e0a\u4f18\u5316\uff0c\u56e0\u6b64\u9009\u5b9a\u90e8\u5206\u5177\u6709\u4e0e\u5f53\u524d\u53e5\u5b50\u8bed\u4e49\u4ee3\u8868\u6027\u7684\u4e00\u4e9b\u8bcd\u4f5c\u4e3a\u4e00\u4e9b\u4f2a\u6807\u8bb0\u7684\u521d\u59cb\u5316\uff08\u4f8b\u5982\u4e0a\u56fe\u4e2d\u201ccapital\u201d\uff09\uff1b</li> <li>\u91cd\u53c2\u6570\uff08Reparameterization\uff09 \uff1a\u5177\u4f53\u5230\u4ee3\u7801\u5b9e\u73b0\u4e0a\uff0cP-tuning\u5148\u901a\u8fc7\u4e00\u4e2aPrompt Encoder\u8868\u5f81\u8fd9\u4e9b\u4f2a\u6807\u8bb0\u540e\uff0c\u76f4\u63a5\u5c06\u8fd9\u4e9b\u65b0\u7684\u8868\u5f81\u8986\u76d6\u5230\u5bf9\u5e94\u7684embedding table\u4e0a\uff0c\u6362\u53e5\u8bdd\u8bf4\uff0cPrompt Encoder\u53ea\u5728\u8bad\u7ec3\u65f6\u5019\u4f1a\u4f7f\u7528\u5230\uff0c\u800c\u5728\u63a8\u7406\u9636\u6bb5\u5219\u4e0d\u518d\u4f7f\u7528\u3002</li> <li>\u6df7\u5408\u63d0\u793a\uff08Hydride Prompt\uff09 \uff1a\u5c06\u8fde\u7eed\u63d0\u793a\u4e0e\u79bb\u6563token\u8fdb\u884c\u6df7\u5408\uff0c\u4f8b\u5982[x1][Britain][x2][mask]</li> </ul> <p>P-tuning\u7684\u7279\u70b9\uff1a</p> <ul> <li>\u4f18\u70b9\uff1a<ul> <li>\u5f15\u5165\u4e86\u4e00\u4e2a LSTM +MLP\u6a21\u5757\u5bf9 soft prompt \u8fdb\u884c\u5efa\u6a21\uff0c\u80fd\u6355\u6349 token \u4e4b\u95f4\u7684\u987a\u5e8f\u548c\u8bed\u4e49\u5173\u7cfb</li> <li>\u6539\u8fdb\u4e86\u79bb\u6563 prompt\u7684\u4e0d\u7a33\u5b9a\u6027\u95ee\u9898\uff0c\u6536\u655b\u901f\u5ea6\u66f4\u5feb</li> </ul> </li> <li>\u7f3a\u70b9<ul> <li>\u4ec5\u653e\u5728\u8f93\u5165\u5c42\u65f6\uff0c\u5bf9\u6a21\u578b\u5185\u90e8\u6df1\u5c42\u8868\u5f81\u7684\u5f71\u54cd\u6709\u9650\uff0c\u9762\u5bf9\u4e00\u4e9b\u9700\u8981\u6df1\u5c42\u8868\u793a\u8c03\u6574\u7684 NLU/\u5e8f\u5217\u6807\u6ce8\u4efb\u52a1\u8868\u73b0\u5e76\u4e0d\u7a33\u5b9a\u6216\u4e0d\u8db3</li> <li>\u5728\u4e2d\u5c0f\u6a21\u578b\uff08100M\u20131B\uff09\u8868\u73b0\u8f83\u5dee</li> </ul> </li> </ul> <p>P-Tuning v2\u662f\u5347\u7ea7\u7248\u672c\uff0c\u4e3b\u8981\u89e3\u51b3**P-Tuning v1 \u5728\u5c0f\u53c2\u6570\u91cf\u6a21\u578b\u4e0a\u8868\u73b0\u5dee\u7684\u95ee\u9898**\u3002 \u8be6\u7ec6\u4fe1\u606f\u53ef\u53c2\u8003\u300a[P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks\u300b\u3002</p> <p>P-Tuning v2 \u7684\u76ee\u6807\u5c31\u662f\u8981\u8ba9 Prompt Tuning \u80fd\u591f\u5728\u4e0d\u540c\u53c2\u6570\u89c4\u6a21\u7684\u9884\u8bad\u7ec3\u6a21\u578b\u3001\u9488\u5bf9\u4e0d\u540c\u4e0b\u6e38\u4efb\u52a1\u7684\u7ed3\u679c\u4e0a\u90fd\u8fbe\u5230\u5339\u654c Fine-tuning \u7684\u7ed3\u679c\u3002 \u8be5\u65b9\u6cd5**\u5728\u6a21\u578b\u7684\u6bcf\u4e00\u5c42\u90fd\u5e94\u7528\u8fde\u7eed\u7684 prompts \u5e76\u5bf9 prompts \u53c2\u6570\u8fdb\u884c\u66f4\u65b0\u4f18\u5316**\u3002</p> <p>P-tuning v2\u7684\u7279\u70b9\uff1a</p> <ul> <li>\u4f18\u70b9\uff1a<ul> <li>\u628a soft prompts \u6ce8\u5165\u6bcf\u5c42\uff0c\u80fd\u5728\u591a\u79cd\u89c4\u6a21\u4e0e\u4efb\u52a1\u4e0a\u63a5\u8fd1\u5168\u91cf\u5fae\u8c03\u6548\u679c</li> </ul> </li> <li>\u7f3a\u70b9<ul> <li>\u6df1\u5c42 prompt \u6216\u957f soft prompt \u4f1a\u5360\u7528\u8f83\u591a token / \u8f93\u5165\u7a7a\u95f4</li> <li>P-tuning \u7684 soft prompt \u662f\u9488\u5bf9\u6bcf\u4e2a\u4e0b\u6e38\u4efb\u52a1\u72ec\u7acb\u8bad\u7ec3\u7684\uff0c\u65e0\u6cd5\u76f4\u63a5\u8fc1\u79fb\u5230\u5176\u4ed6\u4efb\u52a1\u4e0a\u4f7f\u7528</li> </ul> </li> </ul>"},{"location":"01-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BE%AE%E8%B0%83%E4%B8%BB%E8%A6%81%E6%96%B9%E5%BC%8F/01-%E5%A4%A7%E6%A8%A1%E5%9E%8BPrompt-Tuning%E6%96%B9%E6%B3%95.html#634-pptpre-trained-prompt-tuning","title":"6.3.4 PPT\uff08Pre-trained Prompt Tuning\uff09","text":"<p>Prompt-Tuning\u901a\u5e38\u9002\u7528\u4e8e\u4f4e\u8d44\u6e90\u573a\u666f\uff0c\u4f46\u662f\u7531\u4e8e\u8fde\u7eed\u7684\u6a21\u677f\u662f\u968f\u673a\u521d\u59cb\u5316\u7684\uff0c\u5373\u5176\u5b58\u5728\u65b0\u7684\u53c2\u6570\uff0c\u5c11\u91cf\u6837\u672c\u53ef\u80fd\u4f9d\u7136\u5f88\u96be\u786e\u4fdd\u8fd9\u4e9b\u6a21\u677f\u88ab\u5f88\u597d\u5730\u4f18\u5316\u3002\u56e0\u6b64\u7b80\u5355\u7684\u65b9\u6cd5\u5c31\u662f\u5bf9\u8fd9\u4e9b\u8fde\u7eed\u7684\u6a21\u677f\u4e5f\u8fdb\u884c\u9884\u8bad\u7ec3\u3002PPT\u65e8\u5728\u901a\u8fc7\u5148\u8ba9\u8fd9\u4e9b\u8fde\u7eed\u63d0\u793a\u5728\u5927\u91cf\u65e0\u6807\u6ce8\u7684\u9884\u8bad\u7ec3\u8bed\u6599\u8fdb\u884c\u9884\u8bad\u7ec3\uff08\u6ce8\u610f\uff0c\u9884\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\uff0cPre-train-model\u53c2\u6570\u56fa\u5b9a\u4e0d\u53d8\uff0c\u53ea\u6539\u53d8soft prompt\uff09\uff0c\u7136\u540e\u5c06\u5176\u52a0\u8f7d\u5230\u5bf9\u5e94\u4e0b\u6e38\u4efb\u52a1\u7684PLM\u4e0a\u8fdb\u884c\u5fae\u8c03\u540e\u4f7f\u7528\u3002\u5982\u4e0b\u56fe\u6240\u793a\uff08\u56fe\u4e2d\u7684P\u5373\u8fde\u7eed\u7684\u63d0\u793a\u6a21\u677f\uff0c&lt;X&gt;\u5e76\u8868\u793a\u4e3amask token\uff09\uff1a</p> <p>\u9996\u5148\u4f1a\u9884\u8bad\u7ec3\u4e00\u4e2asoft Prompt\uff0c\u7136\u540e\u9884\u8bad\u7ec3\u540e\u7684soft Prompt\u53ef\u4ee5\u76f4\u63a5\u8fd0\u7528\u5230\u76f8\u4f3c\u4efb\u52a1\u4e2d</p> <p>\u5177\u4f53\u6b65\u9aa4\u5982\u4e0b\uff1a</p> <p>PPT\u7684\u7279\u70b9\uff1a</p> <ul> <li>\u4f18\u70b9\uff1a<ul> <li>\u9884\u8bad\u7ec3soft-prompt\u5e26\u6765\u4e86 \u5c0f\u6837\u672c\u5b66\u4e60\u573a\u666f\u4e0a\u7684\u663e\u8457\u63d0\u5347</li> <li>\u7f13\u89e3\u4e86prompt-tuning\u6536\u655b\u6162\u7684\u95ee\u9898</li> </ul> </li> <li>\u7f3a\u70b9<ul> <li>\u9ad8\u5ea6\u4f9d\u8d56\u4e8e\u6e90\u4efb\u52a1\u96c6\u7684\u8986\u76d6\u5ea6\u4e0e\u591a\u6837\u6027(\u4e00\u65e6\u76ee\u6807\u4efb\u52a1\u4e0e\u9884\u8bad\u7ec3\u65f6\u7528\u5230\u7684\u6e90\u4efb\u52a1\u5728\u5206\u5e03\u3001\u683c\u5f0f\u6216\u8bed\u4e49\u4e0a\u5dee\u5f02\u8f83\u5927\uff0c\u901a\u7528\u63d0\u793a \ud835\udc43\u5c31\u96be\u4ee5\u63d0\u4f9b\u6709\u6548\u7684\u521d\u59cb\u5f15\u5bfc\uff0c\u5bfc\u81f4\u4e0b\u6e38\u5fae\u8c03\u6548\u679c\u5927\u5e45\u4e0b\u964d)</li> </ul> </li> </ul>"},{"location":"01-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BE%AE%E8%B0%83%E4%B8%BB%E8%A6%81%E6%96%B9%E5%BC%8F/01-%E5%A4%A7%E6%A8%A1%E5%9E%8BPrompt-Tuning%E6%96%B9%E6%B3%95.html#_2","title":"\u5c0f\u7ed3\u603b\u7ed3","text":"<ul> <li>\u672c\u5c0f\u8282\u4e3b\u8981\u4ecb\u7ecd\u4e86NLP\u53d1\u5c55\u7684\u56db\u79cd\u8303\u5f0f\u3001Fine-Tuning\u4ee5\u53caPrompt-Tuning\u7684\u57fa\u672c\u601d\u60f3\u548c\u539f\u7406</li> <li>\u672c\u7ae0\u8282\u5185\u5bb9\u8be6\u7ec6\u53d9\u8ff0\u4e86Prompt-Tuning\u4e3b\u8981\u4ee3\u8868\u65b9\u6cd5</li> </ul>"},{"location":"01-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BE%AE%E8%B0%83%E4%B8%BB%E8%A6%81%E6%96%B9%E5%BC%8F/02-%E5%A4%A7%E6%A8%A1%E5%9E%8BPEFT%E5%BE%AE%E8%B0%83%E6%96%B9%E6%B3%95.html","title":"LLM\u7684PEFT\u5fae\u8c03\u65b9\u6cd5","text":""},{"location":"01-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BE%AE%E8%B0%83%E4%B8%BB%E8%A6%81%E6%96%B9%E5%BC%8F/02-%E5%A4%A7%E6%A8%A1%E5%9E%8BPEFT%E5%BE%AE%E8%B0%83%E6%96%B9%E6%B3%95.html#_1","title":"\u5b66\u4e60\u76ee\u6807","text":"<ul> <li>\u7406\u89e3Prefix-Tuning\u3001Adapter-Tuning\u3001LoRA\u4e09\u79cd\u5927\u6a21\u578b\u53c2\u6570\u5fae\u8c03\u65b9\u6cd5\u7684\u539f\u7406</li> </ul>"},{"location":"01-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BE%AE%E8%B0%83%E4%B8%BB%E8%A6%81%E6%96%B9%E5%BC%8F/02-%E5%A4%A7%E6%A8%A1%E5%9E%8BPEFT%E5%BE%AE%E8%B0%83%E6%96%B9%E6%B3%95.html#peft","title":"PEFT(\u5927\u6a21\u578b\u53c2\u6570\u9ad8\u6548\u5fae\u8c03)","text":"<p>\u76ee\u524d\u5728\u5de5\u4e1a\u754c\u5e94\u7528\u5927\u6a21\u578b\u4e3b\u6d41\u65b9\u5f0f\uff1a\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u65b9\u6cd5\uff08Parameter-Efficient Fine-Tuning\uff0cPEFT\uff09\uff0cPEFT \u65b9\u6cd5\u4ec5\u5fae\u8c03\u5c11\u91cf\u6216\u989d\u5916\u7684\u6a21\u578b\u53c2\u6570\uff0c\u56fa\u5b9a\u5927\u90e8\u5206\u9884\u8bad\u7ec3\u53c2\u6570\uff0c\u5927\u5927\u964d\u4f4e\u4e86\u8ba1\u7b97\u548c\u5b58\u50a8\u6210\u672c\uff0c\u540c\u65f6\u6700\u5148\u8fdb\u7684 PEFT \u6280\u672f\u4e5f\u80fd\u5b9e\u73b0\u4e86\u4e0e\u5168\u91cf\u5fae\u8c03\u76f8\u5f53\u7684\u6027\u80fd\u3002</p> <p>\u8be5\u65b9\u6cd5\u53ef\u4ee5\u4f7f PLM \u9ad8\u6548\u9002\u5e94\u5404\u79cd\u4e0b\u6e38\u5e94\u7528\u4efb\u52a1\uff0c\u800c\u65e0\u9700\u5fae\u8c03\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u6240\u6709\u53c2\u6570\uff0c\u4e14\u8ba9\u5927\u6a21\u578b\u5728\u6d88\u8d39\u7ea7\u786c\u4ef6\u4e0a\u8fdb\u884c\u5168\u91cf\u5fae\u8c03\uff08Full Fine-Tuning\uff09\u53d8\u5f97\u53ef\u884c\u3002</p> <p>\u76ee\u524d\u5e94\u7528\u8f83\u591a\u7684PEFT\u65b9\u6cd5\u4e3b\u8981\u5206\u4e3a\u4e09\u5927\u7c7b\uff1a</p> <ul> <li>Prefix/Prompt-Tuning\uff1a\u5728\u6a21\u578b\u7684\u8f93\u5165\u6216\u9690\u5c42\u6dfb\u52a0 k\u4e2a\u989d\u5916\u53ef\u8bad\u7ec3\u7684\u524d\u7f00 tokens\uff08\u8fd9\u4e9b\u524d\u7f00\u662f\u8fde\u7eed\u7684\u4f2a tokens\uff0c\u4e0d\u5bf9\u5e94\u771f\u5b9e\u7684 tokens\uff09\uff0c\u53ea\u8bad\u7ec3\u8fd9\u4e9b\u524d\u7f00\u53c2\u6570\uff1b</li> <li>Adapter-Tuning\uff1a\u5c06\u8f83\u5c0f\u7684\u795e\u7ecf\u7f51\u7edc\u5c42\u6216\u6a21\u5757\u63d2\u5165\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u6bcf\u4e00\u5c42\uff0c\u8fd9\u4e9b\u65b0\u63d2\u5165\u7684\u795e\u7ecf\u6a21\u5757\u79f0\u4e3a adapter\uff08\u9002\u914d\u5668\uff09\uff0c\u4e0b\u6e38\u4efb\u52a1\u5fae\u8c03\u65f6\u4e5f\u53ea\u8bad\u7ec3\u8fd9\u4e9b\u9002\u914d\u5668\u53c2\u6570\uff1b</li> <li>LoRA\uff1a\u901a\u8fc7\u5b66\u4e60\u5c0f\u53c2\u6570\u7684\u4f4e\u79e9\u77e9\u9635\u6765\u8fd1\u4f3c\u6a21\u578b\u6743\u91cd\u77e9\u9635 W\u7684\u53c2\u6570\u66f4\u65b0\uff0c\u8bad\u7ec3\u65f6\u53ea\u4f18\u5316\u4f4e\u79e9\u77e9\u9635\u53c2\u6570\uff1b</li> </ul> <p>\u6b64\u5916Huggface \u5f00\u6e90\u7684\u4e00\u4e2a\u9ad8\u6548\u5fae\u8c03\u5927\u6a21\u578b\u7684\u5e93PEFT\uff0c\u8be5\u7b97\u6cd5\u5e93\u652f\u6301\u4e0a\u8ff0\u4e09\u7c7b\u65b9\u6cd5\uff0c\u53ef\u4ee5\u76f4\u63a5\u8c03\u7528\u3002</p>"},{"location":"01-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BE%AE%E8%B0%83%E4%B8%BB%E8%A6%81%E6%96%B9%E5%BC%8F/02-%E5%A4%A7%E6%A8%A1%E5%9E%8BPEFT%E5%BE%AE%E8%B0%83%E6%96%B9%E6%B3%95.html#1-prefix-tuning","title":"1. Prefix Tuning","text":"<p>Prefix-Tuning \u5728\u6a21\u578b\u8f93\u5165\u524d\u6dfb\u52a0\u4e00\u4e2a\u8fde\u7eed\u7684\u4e14\u4efb\u52a1\u7279\u5b9a\u7684\u5411\u91cf\u5e8f\u5217\uff08continuous task-specific vectors\uff09\uff0c\u79f0\u4e4b\u4e3a\u524d\u7f00\uff08prefix\uff09\u3002\u524d\u7f00\u88ab\u89c6\u4e3a\u4e00\u7cfb\u5217\u201c\u865a\u62df tokens\u201d\uff0c\u4f46\u662f\u5b83\u7531\u4e0d\u5bf9\u5e94\u4e8e\u771f\u5b9e tokens \u7684\u81ea\u7531\u53c2\u6570\u7ec4\u6210\u3002\u4e0e\u66f4\u65b0\u6240\u6709 PLM \u53c2\u6570\u7684\u5168\u91cf\u5fae\u8c03\u4e0d\u540c\uff0cPrefix-Tuning \u56fa\u5b9a PLM \u7684\u6240\u6709\u53c2\u6570\uff0c\u53ea\u66f4\u65b0\u4f18\u5316\u7279\u5b9a\u4efb\u52a1\u7684 prefix\u3002\u56e0\u6b64\uff0c\u5728\u751f\u4ea7\u90e8\u7f72\u65f6\uff0c\u53ea\u9700\u8981\u5b58\u50a8\u4e00\u4e2a\u5927\u578b PLM \u7684\u526f\u672c\u548c\u4e00\u4e2a\u5b66\u4e60\u5230\u7684\u7279\u5b9a\u4efb\u52a1\u7684 prefix\uff0c\u6bcf\u4e2a\u4e0b\u6e38\u4efb\u52a1\u53ea\u4ea7\u751f\u975e\u5e38\u5c0f\u7684\u989d\u5916\u7684\u8ba1\u7b97\u548c\u5b58\u50a8\u5f00\u9500\u3002</p> <p>Fine-tuning \u66f4\u65b0\u6240\u6709 PLM \u53c2\u6570\uff0c\u5e76\u4e14\u9700\u8981\u4e3a\u6bcf\u4e2a\u4efb\u52a1\u5b58\u50a8\u5b8c\u6574\u7684\u6a21\u578b\u526f\u672c\u3002Prefix-tuning \u51bb\u7ed3\u4e86 PLM \u53c2\u6570\u5e76\u4e14\u53ea\u4f18\u5316\u4e86 prefix\u3002\u56e0\u6b64\uff0c\u53ea\u9700\u8981\u4e3a\u6bcf\u4e2a\u4efb\u52a1\u5b58\u50a8\u7279\u5b9a prefix\uff0c\u4f7f Prefix-tuning \u6a21\u5757\u5316\u4e14\u8282\u7701\u5b58\u50a8\u7a7a\u95f4\u3002</p> <p>\u5177\u4f53\u5b9e\u73b0\u6d41\u7a0b\u5982\u4e0b\uff1a</p> <p>\uff081\uff09\u786e\u5b9a\u4efb\u52a1\u4e0e\u57fa\u6a21\u578b</p> <ul> <li>\u4efb\u52a1\uff1a\u6761\u4ef6\u751f\u6210\uff08\u5982\u6458\u8981\u3001\u8868\u683c\u5230\u6587\u672c\u3001\u5bf9\u8bdd\uff09\u6216 seq2seq\u3002</li> <li>\u9009\u6a21\u578b\uff1aGPT-2/decoder-only \u6216 BART/T5\uff08encoder-decoder\uff09\u3002</li> </ul> <p>\uff082\uff09\u8bbe\u8ba1 prefix \u914d\u7f6e</p> <ul> <li>\u51b3\u5b9a <code>num_prefix</code>\uff08\u6bcf\u5c42\u7684\u865a\u62df token \u6570\uff0c\u5e38\u89c1 10\u2013100\uff09\uff0c\u4ee5\u53ca\u662f\u5426\u5bf9\u6240\u6709\u5c42\u90fd\u4f7f\u7528 prefix\uff08\u8bba\u6587\u5bf9\u6bcf\u5c42\u90fd\u7528\u4e86 prefix\uff0c\u4f46\u53ef\u505a\u53ea\u5bf9\u90e8\u5206\u5c42\uff09\u3002</li> </ul> <p>\uff083\uff09\u6784\u9020\u53ef\u8bad\u7ec3\u53c2\u6570\uff08\u521d\u59cb\u5316\uff09</p> <p>\u53ef\u4ee5\u4e3a\u6bcf\u5c42\u521b\u5efa\u4e00\u4e2a\u53ef\u4ee5\u8bad\u7ec3\u7684\u77e9\u9635P_\u03b8 \uff0c\u4f5c\u4e3a\u524d\u7f00\u5411\u91cf\u62fc\u63a5\u5230\u539f\u5411\u91cf\u4e2d\u3002\u4f46\u662f\u8bba\u6587\u4e2d\u63d0\u51fa\u76f4\u63a5\u4f18\u5316 P_\u03b8 \u4f1a\u5bfc\u81f4\u8bad\u7ec3\u4e0d\u7a33\u5b9a\uff0c\u53ef\u4ee5\u901a\u8fc7\u4e00\u4e2a\u66f4\u5c0f\u7684\u77e9\u9635 P_w\u548c\u4e00\u4e2a\u66f4\u5927\u7684\u524d\u9988\u795e\u7ecf\u7f51\u7edcMLP_\u03b8 \u5bf9P_\u03b8 \u8fdb\u884c\u91cd\u53c2\u6570\u5316: P_\u03b8[i,:]=MLP_\u03b8(P_w[i,:]) \u3002</p> <p>\u6240\u4ee5\u76ee\u524d\u5b9e\u9645\u5b9e\u73b0\u65f6\uff0c\u901a\u5e38\u4f1a\u5148\u8bad\u7ec3\u4e00\u4e2a <code>(num_layers, num_prefix, hidden_dim)</code> \u7684 prefix embedding\uff0c\u7136\u540e\u901a\u8fc7\u4e00\u4e2a\u5c0f\u7684 MLP \u6295\u5f71\u6210 <code>(num_layers, num_prefix, 2 * head_dim * num_heads)</code>\uff0c\u518d reshape \u6210 <code>(num_layers, num_heads, num_prefix, head_dim)</code>\uff0c\u5206\u522b\u62c6\u6210 K \u548c V\u3002\u56e0\u6b64\u9700\u8981\u521b\u5efa\u4e00\u4e2a\u53ef\u8bad\u7ec3\u7684\u77e9\u9635 P_\u03b8  \uff0c\u4ee5\u53ca\u4e00\u4e2aMLP\u6a21\u578b\u3002</p> <p>\uff084\uff09\u4fee\u6539\u6a21\u578b\u524d\u5411\uff08\u63d2\u5165 prefix\uff09</p> <ul> <li>\u5728 Transformer \u7684\u6bcf\u4e00\u5c42\u6ce8\u610f\u529b\u91cc\uff0c\u5c06 prefix \u5bf9\u5e94\u7684 key/value \u62fc\u63a5\u5230\u539f\u59cb\u7684 key/value\u2014\u2014\u8fd9\u6837\u540e\u7eed token \u53ef\u4ee5\u50cf\u201c\u770b\u5230\u771f\u5b9e tokens\u201d\u4e00\u6837 attend \u5230 prefix\uff0c\u7136\u540e\u4e00\u8d77\u8fdb\u884c\u8bad\u7ec3\u3002</li> </ul> <p>\uff085\uff09\u8bad\u7ec3\u8bbe\u7f6e</p> <ul> <li>\u51bb\u7ed3\u539f\u6a21\u578b\u53c2\u6570\uff08<code>requires_grad=False</code>\uff09\uff0c\u53ea\u5bf9 prefix \u53c2\u6570\u505a\u4f18\u5316\u3002</li> <li>\u635f\u5931\u51fd\u6570\u901a\u5e38\u662f\u6807\u51c6\u7684\u4ea4\u53c9\u71b5\uff0c\u8bad\u7ec3\u5668\u53ea\u66f4\u65b0 prefix\u3002 </li> </ul> <p>\uff086\uff09\u63a8\u7406</p> <ul> <li>\u63a8\u7406\u65f6\u628a\u8bad\u7ec3\u597d\u7684 prefix \u9644\u52a0\u5230\u6bcf\u5c42\uff08\u540c\u8bad\u7ec3\u65f6\uff09\uff0c\u7136\u540e\u7528\u5e38\u89c1\u7684\u89e3\u7801\u7b56\u7565\u8fdb\u884c\u751f\u6210\u3002</li> </ul> <p>\u5982\u4e0b\u56fe\u6240\u793a\uff0c\u4ee5 GPT2 \u7684\u81ea\u56de\u5f52\u8bed\u8a00\u6a21\u578b\u4e3a\u4f8b\uff0c\u5c06\u8f93\u5165 x \u548c\u8f93\u51fa y \u62fc\u63a5\u4e3a z=[x;y] \uff0c\u7ecf\u8fc7 LM \u7684\u67d0\u4e00\u5c42\u8ba1\u7b97\u9690\u5c42\u8868\u793ah=[h_1,...,h_i,....,h_n] \uff0c h_i=LM_\u00d8(z_i, h&lt;i) \uff0c\u5176\u4e2d\uff0c X_{idx} \u548cY_{idx}\u5206\u522b\u4e3a\u8f93\u5165\u548c\u8f93\u51fa\u5e8f\u5217\u7684\u7d22\u5f15\u3002</p> <p>Prefix-Tuning \u5728\u8f93\u5165\u524d\u6dfb\u52a0\u524d\u7f00\uff0c\u5373z=[Prefix,x,y] \uff0cP_{idx}\u4e3a\u524d\u7f00\u5e8f\u5217\u7684\u7d22\u5f15\uff0c|P_{idx}| \u4e3a\u524d\u7f00\u7684\u957f\u5ea6\u3002\u524d\u7f00\u7d22\u5f15\u5bf9\u5e94\u7740\u7531\u03b8\u53c2\u6570\u5316\u7684\u5411\u91cf\u77e9\u9635 P_\u03b8 \uff0c\u7ef4\u5ea6\u4e3a|P_{idx}|\u00d7dim(h_i)\u3002</p> <p></p> <p>\u5728\u8bad\u7ec3\u65f6\uff0cLM \u7684\u53c2\u6570 \u00d8 \u88ab\u56fa\u5b9a\uff0c\u53ea\u6709\u524d\u7f00\u53c2\u6570 \u03b8 \u4e3a\u53ef\u8bad\u7ec3\u7684\u53c2\u6570\u3002\u8bad\u7ec3\u5b8c\u6210\u540e\uff0c\u53ea\u6709\u524d\u7f00P_\u03b8\u88ab\u4fdd\u5b58\u3002</p> <p>Prefix Tuning\u7684\u7279\u70b9\uff1a</p> <ul> <li>\u4f18\u70b9\uff1a<ul> <li>\u53ea\u8bad\u7ec3\u5c11\u91cf prefix \u53c2\u6570\uff0c\u76f8\u5bf9\u5168\u91cf\u5fae\u8c03\u7684\u5b58\u50a8\u548c\u8bad\u7ec3\u6210\u672c\u4f4e\u3002</li> <li>\u4e0d\u540c\u4efb\u52a1\u53ea\u9700\u5207\u6362 prefix\uff0c\u65e0\u9700\u4fdd\u5b58\u591a\u4e2a\u5b8c\u6574\u6a21\u578b\u3002</li> </ul> </li> <li>\u7f3a\u70b9<ul> <li>\u5c0f\u6a21\u578b\u8868\u73b0\u5dee\uff1a\u5728 BERT-base \u7b49\u5c0f\u6a21\u578b\u4e0a\u6548\u679c\u4e0d\u4f73</li> <li>\u9700\u5728\u6bcf\u5c42\u6ce8\u5165 prefix\uff0c\u4f1a\u5360\u7528\u8f93\u5165\u5e8f\u5217\u7684\u957f\u5ea6</li> <li>\u5728\u5224\u522b\u5f0f\u4efb\u52a1\u4e0a\u5e38\u900a\u4e8e LoRA\u3001P-Tuning v2</li> </ul> </li> </ul>"},{"location":"01-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BE%AE%E8%B0%83%E4%B8%BB%E8%A6%81%E6%96%B9%E5%BC%8F/02-%E5%A4%A7%E6%A8%A1%E5%9E%8BPEFT%E5%BE%AE%E8%B0%83%E6%96%B9%E6%B3%95.html#2-adapter-tuning","title":"2. Adapter Tuning","text":"<p>\u4e0e Prefix Tuning \u548c Prompt Tuning \u8fd9\u7c7b\u5728\u8f93\u5165\u524d\u53ef\u8bad\u7ec3\u6dfb\u52a0 prompt embedding \u53c2\u6570\u6765\u4ee5\u5c11\u91cf\u53c2\u6570\u9002\u914d\u4e0b\u6e38\u4efb\u52a1\u4e0d\u540c\uff0cAdapter Tuning \u5219\u662f\u5728\u9884\u8bad\u7ec3\u6a21\u578b\u5185\u90e8\u7684\u7f51\u7edc\u5c42\u4e4b\u95f4\u6dfb\u52a0\u65b0\u7684\u7f51\u7edc\u5c42\u6216\u6a21\u5757\u6765\u9002\u914d\u4e0b\u6e38\u4efb\u52a1\u3002</p> <p>\u5047\u8bbe\u9884\u8bad\u7ec3\u6a21\u578b\u51fd\u6570\u8868\u793a\u4e3a\u00d8_w(x)\uff0c\u5bf9\u4e8e Adapter Tuning \uff0c\u6dfb\u52a0\u9002\u914d\u5668\u4e4b\u540e\u6a21\u578b\u51fd\u6570\u66f4\u65b0\u4e3a\u00d8_{w,w_0}(x)\uff0c w\u662f\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u53c2\u6570\uff0c w_0\u662f\u65b0\u6dfb\u52a0\u7684\u9002\u914d\u5668\u7684\u53c2\u6570\uff0c\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\uff0c w\u88ab\u56fa\u5b9a\uff0c\u53ea\u6709 w_0\u88ab\u66f4\u65b0\u3002|w_0|&lt;&lt;|w| \uff0c\u8fd9\u4f7f\u5f97\u4e0d\u540c\u4e0b\u6e38\u4efb\u52a1\u53ea\u9700\u8981\u6dfb\u52a0\u5c11\u91cf\u53ef\u8bad\u7ec3\u7684\u53c2\u6570\u5373\u53ef\uff0c\u8282\u7701\u8ba1\u7b97\u548c\u5b58\u50a8\u5f00\u9500\uff0c\u540c\u65f6\u5171\u4eab\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u6a21\u578b\u3002</p> <p>Series Adapter\u7684\u9002\u914d\u5668\u7ed3\u6784\u548c\u4e0e Transformer \u7684\u96c6\u6210\u5982\u4e0a\u56fe\u6240\u793a\u3002\u9002\u914d\u5668\u6a21\u5757\u88ab\u6dfb\u52a0\u5230\u6bcf\u4e2a Transformer \u5c42\u4e24\u6b21\uff1a\u591a\u5934\u6ce8\u610f\u529b\u6620\u5c04\u4e4b\u540e\u548c\u4e24\u5c42\u524d\u9988\u795e\u7ecf\u7f51\u7edc\u4e4b\u540e\u3002\u9002\u914d\u5668\u662f\u4e00\u4e2a bottleneck\uff08\u74f6\u9888\uff09\u7ed3\u6784\u7684\u6a21\u5757\uff0c\u7531\u4e00\u4e2a\u4e24\u5c42\u7684\u524d\u9988\u795e\u7ecf\u7f51\u7edc\uff08\u7531\u5411\u4e0b\u6295\u5f71\u77e9\u9635\u3001\u975e\u7ebf\u6027\u51fd\u6570\u548c\u5411\u4e0a\u6295\u5f71\u77e9\u9635\u6784\u6210\uff09\u548c\u4e00\u4e2a\u8f93\u51fa\u8f93\u51fa\u4e4b\u95f4\u7684\u6b8b\u5dee\u8fde\u63a5\u7ec4\u6210\u3002</p> <p>Adapter Tuning\u7684\u7279\u70b9\uff1a</p> <ul> <li>\u4f18\u70b9\uff1a<ul> <li>\u53ea\u8bad\u7ec3\u5c11\u91cf adapter \u53c2\u6570\uff0c\u76f8\u5bf9\u5168\u91cf\u5fae\u8c03\u7684\u5b58\u50a8\u548c\u8bad\u7ec3\u6210\u672c\u4f4e\u3002</li> <li>\u53ef\u4ee5\u4e3a\u591a\u4e2a\u4efb\u52a1\u4fdd\u5b58\u4e0d\u540c\u7684 adapter\uff0c\u800c\u5171\u4eab\u4e00\u4e2a\u5927\u6a21\u578b\u3002</li> <li>\u591a\u4e2a\u4efb\u52a1\u7684 adapters \u53ef\u4ee5\u5728\u63a8\u7406\u65f6\u8fdb\u884c\u5207\u6362\u6216\u878d\u5408\uff0c\u63d0\u5347\u8fc1\u79fb\u548c\u6cdb\u5316\u80fd\u529b\u3002</li> </ul> </li> <li>\u7f3a\u70b9<ul> <li>\u56e0\u4e3a\u5927\u90e8\u5206\u53c2\u6570\u88ab\u51bb\u7ed3\uff0cadapter \u7684\u5bb9\u91cf\u6709\u9650\uff0c\u5bf9\u590d\u6742\u4efb\u52a1\u6216\u9700\u8981\u5927\u89c4\u6a21\u53c2\u6570\u8c03\u6574\u7684\u4efb\u52a1\u53ef\u80fd\u6548\u679c\u4e0d\u5982\u5168\u91cf\u5fae\u8c03\u3002</li> <li>Adapter \u7684\u7ef4\u5ea6\u5927\u5c0f\uff08\u74f6\u9888\u5c42\u5927\u5c0f\uff09\u3001\u63d2\u5165\u4f4d\u7f6e\u7b49\u8d85\u53c2\u6570\u5bf9\u6027\u80fd\u5f71\u54cd\u8f83\u5927\uff0c\u8c03\u53c2\u590d\u6742\u5ea6\u8f83\u9ad8\u3002</li> <li>PLM \u57fa\u7840\u4e0a\u6dfb\u52a0\u9002\u914d\u5668\u5c42\u4f1a\u5f15\u5165\u989d\u5916\u7684\u8ba1\u7b97\uff0c\u5e26\u6765\u63a8\u7406\u5ef6\u8fdf\u95ee\u9898</li> </ul> </li> </ul>"},{"location":"01-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BE%AE%E8%B0%83%E4%B8%BB%E8%A6%81%E6%96%B9%E5%BC%8F/02-%E5%A4%A7%E6%A8%A1%E5%9E%8BPEFT%E5%BE%AE%E8%B0%83%E6%96%B9%E6%B3%95.html#3-lora","title":"3. LoRA","text":"<p>\u4e0a\u8ff0Adapter Tuning \u65b9\u6cd5\u5728 PLM \u57fa\u7840\u4e0a\u6dfb\u52a0\u9002\u914d\u5668\u5c42\u4f1a\u5f15\u5165\u989d\u5916\u7684\u8ba1\u7b97\uff0c\u5e26\u6765\u63a8\u7406\u5ef6\u8fdf\u95ee\u9898\uff1b\u800c Prefix Tuning \u65b9\u6cd5\u96be\u4ee5\u4f18\u5316\uff0c\u5176\u6027\u80fd\u968f\u53ef\u8bad\u7ec3\u53c2\u6570\u89c4\u6a21\u975e\u5355\u8c03\u53d8\u5316\uff0c\u66f4\u6839\u672c\u7684\u662f\uff0c\u4e3a\u524d\u7f00\u4fdd\u7559\u90e8\u5206\u5e8f\u5217\u957f\u5ea6\u5fc5\u7136\u4f1a\u51cf\u5c11\u7528\u4e8e\u5904\u7406\u4e0b\u6e38\u4efb\u52a1\u7684\u5e8f\u5217\u957f\u5ea6\u3002\u56e0\u6b64\u5fae\u8f6f\u63a8\u51fa\u4e86LoRA\u65b9\u6cd5\u3002</p> <p>LORA\u7684\u5b9e\u73b0\u65b9\u5f0f</p> <p>\u6211\u4eec\u5148\u601d\u8003\u4e24\u4e2a\u95ee\u9898\uff1a\u4e3a\u4f55\u7528\u6570\u5343\u7684\u6837\u672c\u5c31\u80fd\u5c06\u4e00\u4e2a\u6570\u5341\u4ebf\u53c2\u6570\u7684\u6a21\u578b\u5fae\u8c03\u5f97\u6bd4\u8f83\u597d\uff1f\u4e3a\u4f55\u5927\u6a21\u578b\u8868\u73b0\u51fa\u5f88\u597d\u7684few-shot\u80fd\u529b\uff1f</p> <p>Aghajanyan\u7684\u7814\u7a76\uff08\u300aIntrinsic Dimensionality Explains the Effectiveness of Language Model Fine-Tuning\u300b\uff09\u8868\u660e\uff1a\u9884\u8bad\u7ec3\u6a21\u578b\u62e5\u6709\u6781\u5c0f\u7684\u5185\u5728\u7ef4\u5ea6(instrisic dimension)\uff0c\u5373\u5b58\u5728\u4e00\u4e2a\u6781\u4f4e\u7ef4\u5ea6\u7684\u53c2\u6570\uff0c\u5fae\u8c03\u5b83\u548c\u5728\u5168\u53c2\u6570\u7a7a\u95f4\u4e2d\u5fae\u8c03\u80fd\u8d77\u5230\u76f8\u540c\u7684\u6548\u679c\u3010\u5927\u6a21\u578b\u5728\u9884\u8bad\u7ec3\u9636\u6bb5\u5df2\u7ecf\u5b66\u5230\u4e86\u66f4\u591a\u7684\u201c\u901a\u7528\u7279\u5f81\u201d\uff0c\u6240\u4ee5\u5728\u5fae\u8c03\u65f6\uff0c\u53ea\u9700\u8981\u5728\u4e00\u4e2a\u66f4\u5c0f\u7684\u65b9\u5411\u7a7a\u95f4\u4e2d\u201c\u5bf9\u9f50\u201d\u6216\u201c\u4fee\u6b63\u201d\u5373\u53ef\u3011\u3002</p> <p>\u540c\u65f6Aghajanyan\u53d1\u73b0\u5728\u9884\u8bad\u7ec3\u540e\uff0c\u8d8a\u5927\u7684\u6a21\u578b\u6709\u8d8a\u5c0f\u7684\u5185\u5728\u7ef4\u5ea6\uff0c\u8fd9\u4e5f\u89e3\u91ca\u4e86\u4e3a\u4f55\u5927\u6a21\u578b\u90fd\u62e5\u6709\u5f88\u597d\u7684few-shot\u80fd\u529b\u3010\u56e0\u4e3a\u5b83\u4eec\u5df2\u7ecf\u8986\u76d6\u4e86\u5927\u90e8\u5206\u8bed\u8a00\u77e5\u8bc6\uff0c\u5c11\u91cf\u53c2\u6570\u66f4\u65b0\uff08\u751a\u81f3\u51e0\u6761\u793a\u4f8b in-context\uff09\u5c31\u80fd\u628a\u8f93\u51fa\u65b9\u5411\u8c03\u6574\u5230\u76ee\u6807\u4efb\u52a1\u3011\u3002</p> <p>\u53d7instrisic dimension\u5de5\u4f5c\u7684\u542f\u53d1\uff0c\u4f5c\u8005\u8ba4\u4e3a\u53c2\u6570\u66f4\u65b0\u8fc7\u7a0b\u4e2d\u4e5f\u5b58\u5728\u4e00\u4e2a\u2018\u5185\u5728\u79e9\u2019\u3002\u5bf9\u4e8e\u9884\u8bad\u7ec3\u6743\u91cd\u77e9\u9635 $ W_0$ \uff0c\u6211\u4eec\u53ef\u4ee5\u7528\u4e00\u4e2a\u4f4e\u79e9\u5206\u89e3 $ \\Delta W $ \u6765\u8868\u793a\u53c2\u6570\u66f4\u65b0\uff0c\u5373\uff1a</p> <p>\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u51bb\u7ed3\u53c2\u6570 $ W_0$ \uff0c\u4ec5\u8bad\u7ec3A\u548cB\u4e2d\u7684\u53c2\u6570\u3002\u5982\u4e0a\u56fe\u6240\u793a\uff0c\u524d\u5411\u4f20\u64ad\u8fc7\u7a0b\u53d8\u4e3a</p> <p>\u8fd9\u79cd\u65b9\u5f0f\u5219\u79f0\u4e3a\u4f4e\u79e9\u9002\u5e94\uff08Low-Rank Adaptation\uff09\uff0c\u5176\u6838\u5fc3\u601d\u60f3\u662f\u5bf9\u5927\u578b\u6a21\u578b\u7684\u6743\u91cd\u77e9\u9635\u8fdb\u884c\u9690\u5f0f\u7684\u4f4e\u79e9\u8f6c\u6362\uff0c\u4e5f\u5c31\u662f\uff1a\u901a\u8fc7\u4e00\u4e2a\u8f83\u4f4e\u7ef4\u5ea6\u7684\u8868\u793a\u6765\u8fd1\u4f3c\u8868\u793a\u4e00\u4e2a\u9ad8\u7ef4\u77e9\u9635\u6216\u6570\u636e\u96c6\u3002</p> <p>\u57fa\u672c\u539f\u7406\uff1aLoRA\u6280\u672f\u51bb\u7ed3\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u6743\u91cd\uff0c\u5e76\u5728\u6bcf\u4e2aTransformer\u5757\u4e2d\u6ce8\u5165\u53ef\u8bad\u7ec3\u5c42\uff08\u79f0\u4e3a\u79e9\u5206\u89e3\u77e9\u9635\uff09\uff0c\u5373\u5728\u6a21\u578b\u7684Linear\u5c42\u7684\u65c1\u8fb9\u589e\u52a0\u4e00\u4e2a\u201c\u65c1\u652f\u201dA\u548cB\u3002\u5176\u4e2d\uff0cA\u5c06\u6570\u636e\u4eced\u7ef4\u964d\u5230r\u7ef4\uff0c\u8fd9\u4e2ar\u662fLoRA\u7684\u79e9\uff0c\u662f\u4e00\u4e2a\u91cd\u8981\u7684\u8d85\u53c2\u6570\uff1bB\u5c06\u6570\u636e\u4ecer\u7ef4\u5347\u5230d\u7ef4\uff0cB\u90e8\u5206\u7684\u53c2\u6570\u521d\u59cb\u4e3a0\u3002\u6a21\u578b\u8bad\u7ec3\u7ed3\u675f\u540e\uff0c\u9700\u8981\u5c06A+B\u90e8\u5206\u7684\u53c2\u6570\u4e0e\u539f\u5927\u6a21\u578b\u7684\u53c2\u6570\u5408\u5e76\u5728\u4e00\u8d77\u4f7f\u7528\u3002</p> <p>python\u4f2a\u4ee3\u7801</p> <pre><code>input_dim = 768 # \u4f8b\u5982\uff0c\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u9690\u85cf\u5927\u5c0f\noutput_dim = 768 # \u4f8b\u5982\uff0c\u5c42\u7684\u8f93\u51fa\u5927\u5c0f\nrank = 8 # \u4f4e\u79e9\u9002\u5e94\u7684\u7b49\u7ea7'r'\nW = ... # \u6765\u81ea\u9884\u8bad\u7ec3\u7f51\u7edc\u7684\u6743\u91cd\uff0c\u5f62\u72b6\u4e3a input_dim x output_dim\nW_A = nn.Parameter(torch.empty(input_dim, rank)) # LoRA\u6743\u91cdA\nW_B = nn.Parameter(torch.empty(rank, output_dim)) # LoRA\u6743\u91cdB\u521d\u59cb\u5316LoRA\u6743\u91cd\nnn.init.kaiming_uniform_(W_A, a=math.sqrt(5))\nnn.init.zeros_(W_B)\n\ndef regular_forward_matmul(x, W):\n  h = x @ W\n  return h\n\ndef lora_forward_matmul(x, W, W_A, W_B):\n  h = x @ W # \u5e38\u89c4\u77e9\u9635\u4e58\u6cd5\n  h += x @ W_A @ W_B * alpha # \u4f7f\u7528\u7f29\u653e\u7684LoRA\u6743\u91cd,alpha\u7f29\u653e\u56e0\u5b50\n  return h\n</code></pre> <p>LoRA\u65b9\u6cd5\u662f\u76ee\u524d\u6700\u901a\u7528\u3001\u540c\u65f6\u4e5f\u662f\u6548\u679c\u6700\u597d\u7684\u5fae\u8c03\u65b9\u6cd5\u4e4b\u4e00\u3002</p> <p>LoRA\u7684\u7279\u70b9\uff1a</p> <ul> <li>\u4f18\u70b9\uff1a<ul> <li>\u53ea\u8bad\u7ec3\u6781\u5c11\u53c2\u6570\uff0c\u76f8\u5bf9\u5168\u91cf\u5fae\u8c03\u7684\u5b58\u50a8\u548c\u8bad\u7ec3\u6210\u672c\u4f4e\u3002</li> <li>\u6548\u679c\u63a5\u8fd1\u5168\u53c2\u6570\u5fae\u8c03\uff0c\u4e14\u4fdd\u7559\u539f\u6a21\u578b\u80fd\u529b\u3002</li> <li>\u4e0d\u540c\u4efb\u52a1\u7684 LoRA \u6a21\u5757\u53ef\u63d2\u62d4\uff0c\u4fbf\u4e8e\u591a\u4efb\u52a1\u90e8\u7f72\u3002</li> </ul> </li> <li>\u7f3a\u70b9<ul> <li>LoRA \u672c\u8d28\u662f\u7528\u4f4e\u79e9\u5206\u89e3\u903c\u8fd1\u6743\u91cd\u66f4\u65b0\u77e9\u9635\uff0c\u8fd9\u5bf9\u53c2\u6570\u7a7a\u95f4\u7684\u8868\u8fbe\u80fd\u529b\u6709\u9650\u5236\uff0c\u53ef\u80fd\u65e0\u6cd5\u62df\u5408\u67d0\u4e9b\u590d\u6742\u4efb\u52a1\u6240\u9700\u7684\u9ad8\u79e9\u53d8\u5316\u3002</li> <li>LoRA \u901a\u5e38\u52a0\u5728 attention \u7684\u6295\u5f71\u77e9\u9635\uff08Wq/Wv\uff09\u4e0a\uff0c\u4f46\u4e0d\u540c\u4efb\u52a1\u53ef\u80fd\u5bf9\u4f4d\u7f6e\u654f\u611f\uff0c\u9009\u62e9\u4e0d\u597d\u4f1a\u5f71\u54cd\u6027\u80fd\u3002</li> </ul> </li> </ul>"},{"location":"01-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BE%AE%E8%B0%83%E4%B8%BB%E8%A6%81%E6%96%B9%E5%BC%8F/02-%E5%A4%A7%E6%A8%A1%E5%9E%8BPEFT%E5%BE%AE%E8%B0%83%E6%96%B9%E6%B3%95.html#4-qlora","title":"4. QLoRA","text":"<p>QLoRA\u51fa\u73b0\u7684\u80cc\u666f\u548c\u52a8\u673a:</p> <p>\u5c3d\u7ba1LoRA\u5df2\u7ecf\u5927\u5927\u964d\u4f4e\u4e86\u5fae\u8c03\u7684\u53c2\u6570\u91cf\u548c\u5b58\u50a8\u9700\u6c42\uff0c\u4f46\u5bf9\u4e8eGPT-3 (175B)\u3001LLaMA-65B (65B)\u3001Falcon-40B (40B) \u8fd9\u6837\u53c2\u6570\u91cf\u5de8\u5927\u7684\u6a21\u578b\uff0c\u5373\u4f7f\u662f\u52a0\u8f7d\u6a21\u578b\u672c\u8eab\uff08\u5373\u4f7f\u51bb\u7ed3\uff09\u4e5f\u9700\u8981\u5de8\u5927\u7684GPU\u5185\u5b58\u3002\u4f8b\u5982\uff0c\u52a0\u8f7d\u4e00\u4e2a65B\u53c2\u6570\u768416-bit\u6a21\u578b\u5c31\u9700\u8981\u5927\u7ea6130GB \u7684 VRAM\uff0c\u8fd9\u8d85\u51fa\u4e86\u5927\u591a\u6570\u6d88\u8d39\u7ea7GPU\u7684\u80fd\u529b\u3002</p> <p>\u4e8e\u662fDettmers et al. \u4e8e 2023 \u5e74\u5728\u8bba\u6587 QLoRA: Efficient Finetuning of Quantized LLMs \u4e2d\u63d0\u51fa**QLoRA (Quantized LoRA)** \u3002</p> <p>\u5b83\u7684\u6838\u5fc3\u601d\u60f3\u662f\uff1a\u901a\u8fc7\u5bf9\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\uff08PLM\uff09\u8fdb\u884c\u91cf\u5316\uff08\u901a\u5e38\u662f 4-bit NormalFloat\uff09\uff0c\u5e76\u7ed3\u5408 LoRA \u6280\u672f\u8fdb\u884c\u5fae\u8c03\uff0c\u4ece\u800c\u5728\u6781\u4f4e\u7684\u5185\u5b58\u6d88\u8017\u4e0b\uff0c\u4ecd\u7136\u80fd\u591f\u9ad8\u6548\u5730\u5fae\u8c03\u5de8\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u540c\u65f6\u4fdd\u6301\u751a\u81f3\u8d85\u8d8a\u5168\u91cf 16-bit LoRA \u7684\u6027\u80fd\u3002</p> <p>QLoRA\u6781\u5927\u5730\u964d\u4f4e\u4e86\u5fae\u8c03\u5927\u578b\u6a21\u578b\u7684\u786c\u4ef6\u95e8\u69db\uff0c\u4f7f\u5f97\u5728\u6d88\u8d39\u7ea7 GPU \u4e0a\u5fae\u8c03\u6570\u5341\u4ebf\u751a\u81f3\u5343\u4ebf\u53c2\u6570\u7684\u6a21\u578b\u6210\u4e3a\u53ef\u80fd\u3002</p> <p>QLoRA\u7684\u6838\u5fc3\u5728\u4e8e\u5f15\u5165\u4e86\u4e24\u4e2a\u5173\u952e\u7684\u521b\u65b0\u70b9\uff1a</p> <ol> <li>4-bit NormalFloat (NF4) \u91cf\u5316\uff1a<ul> <li>\u4f20\u7edf\u7684\u91cf\u5316\u65b9\u6cd5\uff08\u5982 int8\uff09\u53ef\u80fd\u5bfc\u81f4\u7cbe\u5ea6\u635f\u5931\u3002QLoRA \u63d0\u51fa\u4e86\u4e00\u79cd**\u65b0\u76844-bit \u6570\u636e\u7c7b\u578bNormalFloat (NF4)**\uff0c\u5b83\u662f\u4e00\u79cd\u4fe1\u606f\u7406\u8bba\u6700\u4f18\u7684\u91cf\u5316\u65b9\u6848\uff0c\u4e13\u4e3a\u6b63\u6001\u5206\u5e03\u7684\u6570\u636e\u8bbe\u8ba1\uff0c\u80fd\u591f\u66f4\u6709\u6548\u5730\u4fdd\u7559\u6a21\u578b\u7684\u7cbe\u5ea6\u3002</li> <li>\u53cc\u91cf\u5316 (Double Quantization)\uff1a\u8fd9\u662f QLoRA \u7684\u4e00\u4e2a\u91cd\u8981\u7ec6\u8282\u3002\u5b83\u5bf9\u7b2c\u4e00\u6b21\u91cf\u5316\u5f97\u5230\u7684\u91cf\u5316\u5e38\u6570\uff08\u5373\u5757\u91cf\u5316\u4e2d\u6bcf\u4e2a\u5757\u7684\u6bd4\u4f8b\u56e0\u5b50\uff09\u518d\u6b21\u8fdb\u884c\u91cf\u5316\u3002\u8fd9\u80fd\u8fdb\u4e00\u6b65\u51cf\u5c11\u5185\u5b58\u5360\u7528\uff0c\u56e0\u4e3a\u5b83\u51cf\u5c11\u4e86\u5b58\u50a8\u91cf\u5316\u5e38\u6570\u6240\u9700\u7684\u4f4d\u6570\u3002</li> <li>\u5206\u9875\u4f18\u5316\u5668 (Paged Optimizers)\uff1aQLoRA \u5229\u7528 NVIDIA \u7684\u7edf\u4e00\u5185\u5b58\u529f\u80fd\uff0c\u5728 GPU \u5185\u5b58\u4e0d\u8db3\u65f6\uff0c\u5c06\u4f18\u5316\u5668\u72b6\u6001\uff08optimizer states\uff09\u5206\u9875\u5230 CPU RAM\uff0c\u4ece\u800c\u907f\u514d\u4e86 GPU \u5185\u5b58\u6ea2\u51fa\uff08OOM\uff09\u9519\u8bef\u3002</li> </ul> </li> <li>LoRA \u5fae\u8c03\u673a\u5236\u4fdd\u6301\u4e0d\u53d8\uff1a<ul> <li>\u5728\u91cf\u5316\u540e\u7684\u51bb\u7ed3PLM\u4e0a\uff0cLoRA\u7684\u5fae\u8c03\u673a\u5236\u4fdd\u6301\u4e0d\u53d8\u3002\u8fd9\u610f\u5473\u7740\u6211\u4eec\u4ecd\u7136\u53ea\u8bad\u7ec3\u548c\u66f4\u65b0\u5c11\u91cf\u7684\u4f4e\u79e9 A \u548c B \u77e9\u9635\u3002</li> <li>16-bit LoRA \u6743\u91cd \uff1a\u5c3d\u7ba1\u57fa\u7840\u6a21\u578b\u662f 4-bit \u91cf\u5316\u7684\uff0c\u4f46 LoRA \u6743\u91cd\u672c\u8eab\uff08A \u548c B \u77e9\u9635\uff09\u4ee5\u53ca\u4f18\u5316\u5668\u72b6\u6001**\u4ecd\u7136\u4fdd\u6301 16-bit \u7cbe\u5ea6**\uff08\u901a\u5e38\u662f float16 \u6216 bfloat16\uff09\u3002\u8fd9\u662f QLoRA \u80fd\u591f\u4fdd\u6301\u9ad8\u6027\u80fd\u7684\u5173\u952e\u3002\u8fd9\u662f\u56e0\u4e3a\u53ea\u8bad\u7ec3\u5c11\u91cf\u7684 LoRA \u6743\u91cd\uff0c\u5c06\u5176\u4fdd\u6301\u9ad8\u7cbe\u5ea6\u4e0d\u4f1a\u663e\u8457\u589e\u52a0\u5185\u5b58\u8d1f\u62c5\uff0c\u4f46\u5bf9\u68af\u5ea6\u8ba1\u7b97\u548c\u4f18\u5316\u8fc7\u7a0b\u7684\u7a33\u5b9a\u6027\u81f3\u5173\u91cd\u8981\u3002</li> </ul> </li> </ol> <p>\u5de5\u4f5c\u539f\u7406:</p> <ol> <li>\u52a0\u8f7d\u548c 4-bit \u91cf\u5316\u57fa\u7840 PLM\uff1a<ul> <li>\u4f7f\u7528<code>bitsandbytes</code>\u5e93\u52a0\u8f7d\u9884\u8bad\u7ec3\u6a21\u578b\u3002\u5728\u52a0\u8f7d\u65f6\uff0c\u6a21\u578b\u7684\u5927\u90e8\u5206\u6743\u91cd\u4f1a\u88ab\u7acb\u5373\u91cf\u5316\u4e3a 4-bit NormalFloat (NF4)\u3002\u8fd9\u4e2a\u91cf\u5316\u8fc7\u7a0b\u901a\u5e38\u5305\u542b\u53cc\u91cf\u5316\u3002</li> <li>\u6a21\u578b\u7684\u53c2\u6570\u88ab\u8bbe\u7f6e\u4e3a\u4e0d\u53ef\u8bad\u7ec3\uff08<code>requires_grad=False</code>\uff09\u3002</li> </ul> </li> <li>\u5728\u91cf\u5316\u6a21\u578b\u4e0a\u6ce8\u5165 LoRA \u6a21\u5757\uff1a<ul> <li>\u50cf\u6807\u51c6 LoRA \u4e00\u6837\uff0c\u9009\u62e9\u6a21\u578b\u4e2d\u7684\u76ee\u6807\u7ebf\u6027\u5c42\uff08\u5982 Attention \u7684 Q\u3001K\u3001V \u6295\u5f71\u5c42\u548c FFN\uff09\uff0c\u5e76\u5728\u5b83\u4eec\u65c1\u8fb9\u5e76\u884c\u5730\u6ce8\u5165\u53ef\u8bad\u7ec3\u7684\u4f4e\u79e9 A \u548c B \u77e9\u9635\u3002</li> <li>\u8fd9\u4e9b A \u548c B \u77e9\u9635\u4ee5\u53ca\u5b83\u4eec\u7684\u4f18\u5316\u5668\u72b6\u6001\u4fdd\u6301 16-bit \u7cbe\u5ea6\u3002</li> </ul> </li> <li>\u524d\u5411\u4f20\u64ad\uff1a<ul> <li>\u5f53\u8fdb\u884c\u524d\u5411\u4f20\u64ad\u65f6\uff0c\u91cf\u5316\u540e\u7684 4-bit \u6743\u91cd\u4f1a\u88ab**\u5373\u65f6\u53cd\u91cf\u5316 (dequantize)** \u5230 16-bit \u7cbe\u5ea6\uff0c\u7136\u540e\u4e0e\u8f93\u5165\u76f8\u4e58\u3002</li> <li>LoRA \u6a21\u5757\u7684 16-bit \u6743\u91cd BA \u4e5f\u4f1a\u4e0e\u8f93\u5165\u76f8\u4e58\u3002</li> <li>\u4e24\u4e2a\u7ed3\u679c\uff08\u6765\u81ea\u57fa\u7840\u6a21\u578b\u7684\u548c\u6765\u81ea LoRA \u6a21\u5757\u7684\uff09\u76f8\u52a0\u3002</li> </ul> </li> <li>\u53cd\u5411\u4f20\u64ad\uff1a<ul> <li>\u68af\u5ea6\u4f1a\u6d41\u5411 LoRA \u6a21\u5757\u7684 A \u548c B \u77e9\u9635\u3002</li> <li>\u7531\u4e8e\u57fa\u7840\u6a21\u578b\u662f\u51bb\u7ed3\u4e14\u91cf\u5316\u7684\uff0c\u68af\u5ea6\u4e0d\u4f1a\u6d41\u5411\u5176 4-bit \u6743\u91cd\u3002</li> </ul> </li> <li>\u5206\u9875\u4f18\u5316\u5668\uff1a<ul> <li>\u4f18\u5316\u5668\uff08\u5982 AdamW\uff09\u7684\u72b6\u6001\u901a\u5e38\u4f1a\u5360\u7528\u5927\u91cf\u5185\u5b58\u3002QLoRA \u4f7f\u7528\u5206\u9875\u4f18\u5316\u5668\uff0c\u5728 GPU \u5185\u5b58\u4e0d\u8db3\u65f6\uff0c\u5c06\u4f18\u5316\u5668\u72b6\u6001\u5728 GPU \u548c CPU RAM \u4e4b\u95f4\u8fdb\u884c\u5206\u9875\uff0c\u4ee5\u907f\u514d OOM \u9519\u8bef\u3002</li> </ul> </li> </ol> <p>QLoRA\u7684\u7279\u70b9\uff1a</p> <ul> <li>\u4f18\u70b9\uff1a<ul> <li>\u6781\u4f4e\u7684\u5185\u5b58\u6d88\u8017\u3002\u8fd9\u662f QLoRA \u6700\u663e\u8457\u7684\u4f18\u52bf\u3002\u53ef\u4ee5\u5c06\u8bad\u7ec3\u5de8\u578b\u6a21\u578b\u7684\u5185\u5b58\u9700\u6c42\u964d\u4f4e 3-4 \u500d\uff0c\u4f7f\u5f97\u5728\u5355\u5f20\u6d88\u8d39\u7ea7 GPU \u4e0a\uff08\u5982 24GB VRAM \u7684 RTX 3090/4090\uff09\u5fae\u8c03 65B \u751a\u81f3 70B \u53c2\u6570\u7684\u6a21\u578b\u6210\u4e3a\u53ef\u80fd\u3002</li> <li>\u6027\u80fd\u4f18\u5f02\uff1a\u5c3d\u7ba1\u8fdb\u884c\u4e86 4-bit \u91cf\u5316\uff0c\u4f46\u7531\u4e8e 16-bit \u7684 LoRA \u6743\u91cd\u548c\u4f18\u5316\u5668\u72b6\u6001\uff0cQLoRA \u5728\u8bb8\u591a\u4efb\u52a1\u4e0a\u80fd\u591f\u4fdd\u6301\u4e0e 16-bit LoRA \u751a\u81f3\u5168\u91cf\u5fae\u8c03\u76f8\u5ab2\u7f8e\u7684\u6027\u80fd\u3002</li> <li>\u8bad\u7ec3\u901f\u5ea6\u5feb\uff1a\u7531\u4e8e\u53ea\u8bad\u7ec3\u5c11\u91cf\u53c2\u6570\u4e14\u5185\u5b58\u6548\u7387\u9ad8\uff0c\u8bad\u7ec3\u901f\u5ea6\u975e\u5e38\u5feb\u3002</li> </ul> </li> <li>\u7f3a\u70b9<ul> <li>\u867d\u7136 NF4 \u4f18\u5316\u4e86\u7cbe\u5ea6\uff0c\u4f46\u6781\u7aef\u4efb\u52a1\u6216\u654f\u611f\u4efb\u52a1\u53ef\u80fd\u4ecd\u53d7 4-bit \u91cf\u5316\u5f71\u54cd\u3002</li> <li>\u7531\u4e8e\u91cf\u5316\u548c\u5206\u9875\u673a\u5236\u7684\u5b58\u5728\uff0c\u8bad\u7ec3\u548c\u95ee\u9898\u8c03\u8bd5\u4f1a\u6bd4\u6807\u51c6 LoRA \u66f4\u590d\u6742\u3002</li> </ul> </li> </ul>"},{"location":"01-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BE%AE%E8%B0%83%E4%B8%BB%E8%A6%81%E6%96%B9%E5%BC%8F/02-%E5%A4%A7%E6%A8%A1%E5%9E%8BPEFT%E5%BE%AE%E8%B0%83%E6%96%B9%E6%B3%95.html#_2","title":"\u5c0f\u7ed3\u603b\u7ed3","text":"<ul> <li>\u672c\u5c0f\u8282\u4e3b\u8981\u4ecb\u7ecd\u4e86\u76ee\u524d\u4e3b\u6d41\u7684PEFT\u65b9\u5f0f\u7684\u539f\u7406\u3002</li> </ul>"},{"location":"02-%E5%9F%BA%E4%BA%8EGPT2%E7%9A%84%E5%8C%BB%E7%96%97%E9%97%AE%E8%AF%8A%E6%9C%BA%E5%99%A8%E4%BA%BA/01-%E5%8C%BB%E7%96%97%E9%97%AE%E8%AF%8A%E6%9C%BA%E5%99%A8%E4%BA%BA%E5%AE%9E%E7%8E%B0.html","title":"\u57fa\u4e8eGPT2\u642d\u5efa\u533b\u7597\u95ee\u8bca\u673a\u5668\u4eba","text":""},{"location":"02-%E5%9F%BA%E4%BA%8EGPT2%E7%9A%84%E5%8C%BB%E7%96%97%E9%97%AE%E8%AF%8A%E6%9C%BA%E5%99%A8%E4%BA%BA/01-%E5%8C%BB%E7%96%97%E9%97%AE%E8%AF%8A%E6%9C%BA%E5%99%A8%E4%BA%BA%E5%AE%9E%E7%8E%B0.html#_1","title":"\u5b66\u4e60\u76ee\u6807","text":"<ul> <li>\u7406\u89e3\u533b\u7597\u95ee\u8bca\u673a\u5668\u4eba\u7684\u5f00\u53d1\u80cc\u666f.</li> <li>\u4e86\u89e3\u4f01\u4e1a\u4e2d\u804a\u5929\u673a\u5668\u4eba\u7684\u5e94\u7528\u573a\u666f</li> <li>\u638c\u63e1\u57fa\u4e8eGPT2\u6a21\u578b\u642d\u5efa\u533b\u7597\u95ee\u8bca\u673a\u5668\u4eba\u7684\u5b9e\u73b0\u8fc7\u7a0b</li> </ul>"},{"location":"02-%E5%9F%BA%E4%BA%8EGPT2%E7%9A%84%E5%8C%BB%E7%96%97%E9%97%AE%E8%AF%8A%E6%9C%BA%E5%99%A8%E4%BA%BA/01-%E5%8C%BB%E7%96%97%E9%97%AE%E8%AF%8A%E6%9C%BA%E5%99%A8%E4%BA%BA%E5%AE%9E%E7%8E%B0.html#1","title":"1. \u9879\u76ee\u4ecb\u7ecd","text":""},{"location":"02-%E5%9F%BA%E4%BA%8EGPT2%E7%9A%84%E5%8C%BB%E7%96%97%E9%97%AE%E8%AF%8A%E6%9C%BA%E5%99%A8%E4%BA%BA/01-%E5%8C%BB%E7%96%97%E9%97%AE%E8%AF%8A%E6%9C%BA%E5%99%A8%E4%BA%BA%E5%AE%9E%E7%8E%B0.html#11","title":"1.1 \u9879\u76ee\u80cc\u666f","text":"<ul> <li> <p>\u804a\u5929\u673a\u5668\u4eba\u662f\u4e00\u79cd\u57fa\u4e8e\u81ea\u7136\u8bed\u8a00\u5904\u7406\u6280\u672f\u7684\u667a\u80fd\u5bf9\u8bdd\u7cfb\u7edf\uff0c\u80fd\u591f\u6a21\u62df\u4eba\u7c7b\u7684\u81ea\u7136\u8bed\u8a00\u4ea4\u6d41\uff0c\u4e0e\u7528\u6237\u8fdb\u884c\u5bf9\u8bdd\u548c\u4e92\u52a8\u3002\u804a\u5929\u673a\u5668\u4eba\u80fd\u591f\u7406\u89e3\u7528\u6237\u7684\u95ee\u9898\u6216\u6307\u4ee4\uff0c\u5e76\u7ed9\u51fa\u76f8\u5e94\u7684\u56de\u7b54\u6216\u5efa\u8bae\u3002\u5176\u76ee\u6807\u662f\u63d0\u4f9b\u53cb\u597d\u3001\u667a\u80fd\u3001\u81ea\u7136\u7684\u5bf9\u8bdd\u4f53\u9a8c. </p> </li> <li> <p>\u5f53\u524d\uff0c\u804a\u5929\u673a\u5668\u4eba\u5728\u591a\u4e2a\u9886\u57df\u5f97\u5230\u5e7f\u6cdb\u5e94\u7528\u3002\u9996\u5148\uff0c\u5b83\u4eec\u5e38\u7528\u4e8e\u5728\u7ebf\u5ba2\u670d\u7cfb\u7edf\uff0c\u80fd\u591f\u5feb\u901f\u3001\u51c6\u786e\u5730\u56de\u7b54\u7528\u6237\u7684\u5e38\u89c1\u95ee\u9898\uff0c\u89e3\u51b3\u7591\u95ee\u3002\u5176\u6b21\uff0c\u804a\u5929\u673a\u5668\u4eba\u53ef\u4ee5\u4f5c\u4e3a\u4e2a\u4eba\u52a9\u624b\uff0c\u63d0\u4f9b\u4e2a\u6027\u5316\u7684\u63a8\u8350\u3001\u5efa\u8bae\u548c\u65e5\u7a0b\u5b89\u6392\u7b49\u670d\u52a1\uff0c\u63d0\u5347\u7528\u6237\u4f53\u9a8c\u3002\u6b64\u5916\uff0c\u804a\u5929\u673a\u5668\u4eba\u8fd8\u88ab\u5e94\u7528\u4e8e\u793e\u4ea4\u5a31\u4e50\u3001\u8bed\u8a00\u5b66\u4e60\u3001\u65c5\u6e38\u6307\u5357\u7b49\u9886\u57df\uff0c\u4e3a\u7528\u6237\u63d0\u4f9b\u6709\u8da3\u3001\u4fbf\u6377\u7684\u5bf9\u8bdd\u4f53\u9a8c.</p> </li> <li>\u5e38\u89c1\u7684\u76f8\u5173\u804a\u5929\u673a\u5668\u4eba\u4ea7\u54c1\uff1a</li> </ul> <p>\u5fae\u8f6f\u5c0f\u51b0\uff1a\u5fae\u8f6f\u516c\u53f8\u5f00\u53d1\u3002\u5b83\u5177\u5907\u81ea\u7136\u8bed\u8a00\u5904\u7406\u3001\u60c5\u611f\u5206\u6790\u548c\u5bf9\u8bdd\u751f\u6210\u7b49\u529f\u80fd\uff0c\u80fd\u591f\u4e0e\u7528\u6237\u8fdb\u884c\u667a\u80fd\u5bf9\u8bdd\uff0c\u63d0\u4f9b\u60c5\u611f\u652f\u6301\u548c\u5a31\u4e50\u7b49\u670d\u52a1\u3002</p> <p>\u963f\u91cc\u4e91\u5c0f\u871c\uff1a\u963f\u91cc\u4e91\u516c\u53f8\u63a8\u51fa\uff0c\u63d0\u4f9b\u4e86\u4e30\u5bcc\u7684\u667a\u80fd\u5bf9\u8bdd\u670d\u52a1\u3002\u5b83\u5177\u5907\u81ea\u7136\u8bed\u8a00\u5904\u7406\u548c\u5bf9\u8bdd\u7ba1\u7406\u80fd\u529b\uff0c\u652f\u6301\u591a\u9886\u57df\u7684\u5e94\u7528\u573a\u666f\uff0c\u5982\u5728\u7ebf\u5ba2\u670d\u3001\u667a\u80fd\u52a9\u624b\u548c\u865a\u62df\u5bfc\u8d2d\u7b49\u3002</p> <p>\u767e\u5ea6\u667a\u80fd\u4e91\u5c0f\u5ea6\uff1a\u767e\u5ea6\u667a\u80fd\u4e91\u5f00\u53d1\uff0c\u63d0\u4f9b\u4e86\u591a\u9886\u57df\u7684\u667a\u80fd\u5bf9\u8bdd\u80fd\u529b\u3002\u5c0f\u5ea6\u673a\u5668\u4eba\u53ef\u5e94\u7528\u4e8e\u5bb6\u5ead\u52a9\u7406\u3001\u667a\u80fd\u97f3\u7bb1\u548c\u79fb\u52a8\u5e94\u7528\u7b49\u573a\u666f\uff0c\u901a\u8fc7\u8bed\u97f3\u548c\u6587\u672c\u4ea4\u4e92\u4e0e\u7528\u6237\u8fdb\u884c\u667a\u80fd\u5bf9\u8bdd\uff0c\u63d0\u4f9b\u4fe1\u606f\u67e5\u8be2\u3001\u97f3\u4e50\u64ad\u653e\u548c\u65e5\u7a0b\u5b89\u6392\u7b49\u529f\u80fd\u3002</p> <p>\u672c\u9879\u76ee**\u57fa\u4e8e\u533b\u7597\u9886\u57df\u6570\u636e\u6784\u5efa\u4e86\u667a\u80fd\u533b\u7597\u95ee\u7b54\u7cfb\u7edf**\uff0c\u76ee\u7684\u662f\u4e3a\u4e3a\u7528\u6237\u63d0\u4f9b\u51c6\u786e\u3001\u9ad8\u6548\u3001\u4f18\u8d28\u7684\u533b\u7597\u95ee\u7b54\u670d\u52a1\u3002</p>"},{"location":"02-%E5%9F%BA%E4%BA%8EGPT2%E7%9A%84%E5%8C%BB%E7%96%97%E9%97%AE%E8%AF%8A%E6%9C%BA%E5%99%A8%E4%BA%BA/01-%E5%8C%BB%E7%96%97%E9%97%AE%E8%AF%8A%E6%9C%BA%E5%99%A8%E4%BA%BA%E5%AE%9E%E7%8E%B0.html#12","title":"1.2 \u73af\u5883\u51c6\u5907","text":"<ul> <li>python==3.10</li> <li>transformers==4.40.2</li> <li>torch==2.5.1+cu121</li> </ul>"},{"location":"02-%E5%9F%BA%E4%BA%8EGPT2%E7%9A%84%E5%8C%BB%E7%96%97%E9%97%AE%E8%AF%8A%E6%9C%BA%E5%99%A8%E4%BA%BA/01-%E5%8C%BB%E7%96%97%E9%97%AE%E8%AF%8A%E6%9C%BA%E5%99%A8%E4%BA%BA%E5%AE%9E%E7%8E%B0.html#13","title":"1.3 \u9879\u76ee\u6574\u4f53\u7ed3\u6784","text":"<p>\u6574\u4f53\u4ee3\u7801\u7ed3\u6784\uff1a</p> <p></p>"},{"location":"02-%E5%9F%BA%E4%BA%8EGPT2%E7%9A%84%E5%8C%BB%E7%96%97%E9%97%AE%E8%AF%8A%E6%9C%BA%E5%99%A8%E4%BA%BA/01-%E5%8C%BB%E7%96%97%E9%97%AE%E8%AF%8A%E6%9C%BA%E5%99%A8%E4%BA%BA%E5%AE%9E%E7%8E%B0.html#2","title":"2. \u6570\u636e\u5904\u7406","text":""},{"location":"02-%E5%9F%BA%E4%BA%8EGPT2%E7%9A%84%E5%8C%BB%E7%96%97%E9%97%AE%E8%AF%8A%E6%9C%BA%E5%99%A8%E4%BA%BA/01-%E5%8C%BB%E7%96%97%E9%97%AE%E8%AF%8A%E6%9C%BA%E5%99%A8%E4%BA%BA%E5%AE%9E%E7%8E%B0.html#21","title":"2.1 \u6570\u636e\u4ecb\u7ecd","text":"<ul> <li>\u6570\u636e\u5b58\u653e\u4f4d\u7f6e\uff1allm_tuning/Gpt2_Chatbot/data</li> <li>data\u6587\u4ef6\u5939\u4e2d\u5b58\u6709\u539f\u59cb\u8bad\u7ec3\u8bed\u6599\u4e3atrain.txt\u3002train.txt\u7684\u683c\u5f0f\u5982\u4e0b\uff0c\u6bcf\u6bb5\u95f2\u804a\u4e4b\u95f4\u95f4\u9694\u4e00\u884c\uff0c\u683c\u5f0f\u5982\u4e0b\uff1a</li> </ul> <pre><code>\u5e15\u91d1\u68ee\u53e0\u52a0\u7efc\u5408\u5f81\u7684\u8f85\u52a9\u6cbb\u7597\u6709\u4e9b\u4ec0\u4e48\uff1f\n\u7efc\u5408\u6cbb\u7597\uff1b\u5eb7\u590d\u8bad\u7ec3\uff1b\u751f\u6d3b\u62a4\u7406\u6307\u5bfc\uff1b\u4f4e\u9891\u91cd\u590d\u7ecf\u9885\u78c1\u523a\u6fc0\u6cbb\u7597\n\n\u5375\u5de2\u764c\u8089\u7624\u7684\u5f71\u50cf\u5b66\u68c0\u67e5\u6709\u4e9b\u4ec0\u4e48\uff1f\n\u8d85\u58f0\u6f0f\u8bca\uff1b\u58f0\u50cf\u56fe\uff1bMR\u68c0\u67e5\uff1b\u80bf\u7269\u8d85\u58f0\uff1b\u672f\u524d\u8d85\u58f0\uff1bCT\u68c0\u67e5\n</code></pre>"},{"location":"02-%E5%9F%BA%E4%BA%8EGPT2%E7%9A%84%E5%8C%BB%E7%96%97%E9%97%AE%E8%AF%8A%E6%9C%BA%E5%99%A8%E4%BA%BA/01-%E5%8C%BB%E7%96%97%E9%97%AE%E8%AF%8A%E6%9C%BA%E5%99%A8%E4%BA%BA%E5%AE%9E%E7%8E%B0.html#22","title":"2.2 \u6570\u636e\u5904\u7406","text":"<ul> <li>\u76ee\u7684\uff1a\u5c06\u4e2d\u6587\u6587\u672c\u6570\u636e\u5904\u7406\u6210\u6a21\u578b\u80fd\u591f\u8bc6\u522b\u7684\u5f20\u91cf\u5f62\u5f0f\uff0c\u5e76\u5c06\u4e0a\u8ff0\u6587\u672c\u8fdb\u884c\u5f20\u91cf\u7684\u8f6c\u6362</li> <li>\u5b9e\u73b0\u8fc7\u7a0b\uff1a<ul> <li>\u8fd0\u884cpreprocess.py\uff0c\u5bf9data/train.txt\u5bf9\u8bdd\u8bed\u6599\u8fdb\u884ctokenize\uff0c\u7136\u540e\u8fdb\u884c\u5e8f\u5217\u5316\u4fdd\u5b58\u5230data/train.pkl\u3002train.pkl\u4e2d\u5e8f\u5217\u5316\u7684\u5bf9\u8c61\u7684\u7c7b\u578b\u4e3aList[List],\u8bb0\u5f55\u5bf9\u8bdd\u5217\u8868\u4e2d,\u6bcf\u4e2a\u5bf9\u8bdd\u5305\u542b\u7684token\u3002</li> </ul> </li> </ul>"},{"location":"02-%E5%9F%BA%E4%BA%8EGPT2%E7%9A%84%E5%8C%BB%E7%96%97%E9%97%AE%E8%AF%8A%E6%9C%BA%E5%99%A8%E4%BA%BA/01-%E5%8C%BB%E7%96%97%E9%97%AE%E8%AF%8A%E6%9C%BA%E5%99%A8%E4%BA%BA%E5%AE%9E%E7%8E%B0.html#221","title":"2.2.1 \u914d\u7f6e\u6587\u4ef6","text":"<ul> <li>\u4ee3\u7801\u8def\u5f84\uff1allm_tuning/Gpt2_Chatbot/parameter_config.py</li> </ul> <pre><code>import torch\nimport os\n\nbase_dir = os.path.dirname(os.path.abspath(__file__))\n# print(f'base_dir--&gt;{base_dir}')\n\nclass ParameterConfig():\n    def __init__(self):\n        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n        # self.device = torch.device('mps' if torch.cuda.is_available() else 'cpu')\n        # \u8bcd\u5178\u8def\u5f84\uff1a\u5728vocab\u6587\u4ef6\u5939\u91cc\u9762\n        self.vocab_path = os.path.join(base_dir, 'vocab/vocab.txt')\n        # \u8bad\u7ec3\u6587\u4ef6\u8def\u5f84\n        self.train_path = os.path.join(base_dir, 'data/medical_train.pkl')\n        # \u9a8c\u8bc1\u6570\u636e\u6587\u4ef6\u8def\u5f84\n        self.valid_path = os.path.join(base_dir, 'data/medical_valid.pkl')\n        # \u6a21\u578b\u914d\u7f6e\u6587\u4ef6\n        self.config_json = os.path.join(base_dir, 'config/config.json')\n        # \u6a21\u578b\u4fdd\u5b58\u8def\u5f84\n        self.save_model_path = os.path.join(base_dir, 'save_model')\n        # \u5982\u679c\u4f60\u6709\u9884\u8bad\u7ec3\u6a21\u578b\u5c31\u5199\u4e0a\u8def\u5f84\uff08\u6211\u4eec\u672c\u6b21\u6ca1\u6709\u76f4\u63a5\u8fd0\u7528GPT2\u9884\u8bad\u7ec3\u597d\u7684\u6a21\u578b\uff0c\u800c\u662f\u4ec5\u53ea\u7528\u4e86\u8be5\u6a21\u578b\u7684\u6846\u67b6\uff09\n        self.pretrained_model = ''\n        # \u4fdd\u5b58\u5bf9\u8bdd\u8bed\u6599\n        self.save_samples_path = os.path.join(base_dir, 'sample')\n        # \u5ffd\u7565\u4e00\u4e9b\u5b57\u7b26\uff1a\u53e5\u5b50\u9700\u8981\u957f\u5ea6\u8865\u9f50\uff0c\u9488\u5bf9\u8865\u7684\u90e8\u5206\uff0c\u6ca1\u6709\u610f\u4e49\uff0c\u6240\u4ee5\u4e00\u822c\u4e0d\u8fdb\u884c\u68af\u5ea6\u66f4\u65b0\n        self.ignore_index = -100\n        # \u5386\u53f2\u5bf9\u8bdd\u53e5\u5b50\u7684\u957f\u5ea6\n        self.max_history_len = 3  # \"dialogue history\u7684\u6700\u5927\u957f\u5ea6\"\n        # \u6bcf\u4e00\u4e2a\u5b8c\u6574\u5bf9\u8bdd\u7684\u53e5\u5b50\u6700\u5927\u957f\u5ea6\n        self.max_len = 300  # '\u6bcf\u4e2autterance\u7684\u6700\u5927\u957f\u5ea6,\u8d85\u8fc7\u6307\u5b9a\u957f\u5ea6\u5219\u8fdb\u884c\u622a\u65ad,\u9ed8\u8ba425'\n        self.repetition_penalty = 5.0  # \"\u91cd\u590d\u60e9\u7f5a\u53c2\u6570\uff0c\u82e5\u751f\u6210\u7684\u5bf9\u8bdd\u91cd\u590d\u6027\u8f83\u9ad8\uff0c\u53ef\u9002\u5f53\u63d0\u9ad8\u8be5\u53c2\u6570\"\n        self.topk = 2  # '\u4fdd\u7559\u6982\u7387\u6700\u9ad8\u7684topk\u4e2atoken\u3002\u9ed8\u8ba44'\n        self.topp = 0.7  # '\u4fdd\u7559\u7d2f\u79ef\u6982\u7387top\u4e2atoken\u3002\u9ed8\u8ba40.7'\n        self.batch_size = 8  # \u4e00\u4e2a\u6279\u6b21\u51e0\u4e2a\u6837\u672c\n        self.epochs = 4  # \u8bad\u7ec3\u51e0\u8f6e\n        self.loss_step = 100  # \u591a\u5c11\u6b65\u6c47\u62a5\u4e00\u6b21loss\n        self.lr = 2.6e-5\n        # eps\uff0c\u4e3a\u4e86\u589e\u52a0\u6570\u503c\u8ba1\u7b97\u7684\u7a33\u5b9a\u6027\u800c\u52a0\u5230\u5206\u6bcd\u91cc\u7684\u9879\uff0c\u5176\u4e3a\u4e86\u9632\u6b62\u5728\u5b9e\u73b0\u4e2d\u9664\u4ee5\u96f6\n        self.eps = 1.0e-09\n        self.max_grad_norm = 2.0\n        self.gradient_accumulation_steps = 4  # \u68af\u5ea6\u7d2f\u79ef\u7684\u6b65\u6570\n        # \u4f7f\u7528Warmup\u9884\u70ed\u5b66\u4e60\u7387\u7684\u65b9\u5f0f,\u5373\u5148\u7528\u6700\u521d\u7684\u5c0f\u5b66\u4e60\u7387\u8bad\u7ec3\uff0c\u7136\u540e\u6bcf\u4e2astep\u589e\u5927\u4e00\u70b9\u70b9\uff0c\u76f4\u5230\u8fbe\u5230\u6700\u521d\u8bbe\u7f6e\u7684\u6bd4\u8f83\u5927\u7684\u5b66\u4e60\u7387\u65f6\uff08\u6ce8\uff1a\u6b64\u65f6\u9884\u70ed\u5b66\u4e60\u7387\u5b8c\u6210\uff09\uff0c\u91c7\u7528\u6700\u521d\u8bbe\u7f6e\u7684\u5b66\u4e60\u7387\u8fdb\u884c\u8bad\u7ec3\uff08\u6ce8\uff1a\u9884\u70ed\u5b66\u4e60\u7387\u5b8c\u6210\u540e\u7684\u8bad\u7ec3\u8fc7\u7a0b\uff0c\u5b66\u4e60\u7387\u662f\u8870\u51cf\u7684\uff09\uff0c\u6709\u52a9\u4e8e\u4f7f\u6a21\u578b\u6536\u655b\u901f\u5ea6\u53d8\u5feb\uff0c\u6548\u679c\u66f4\u4f73\u3002\u9ed8\u8ba4.warmup_steps = 4000\n        self.warmup_steps = 100\n\n\nif __name__ == '__main__':\n    pc = ParameterConfig()\n    print(pc.train_path)\n    print(pc.device)\n</code></pre>"},{"location":"02-%E5%9F%BA%E4%BA%8EGPT2%E7%9A%84%E5%8C%BB%E7%96%97%E9%97%AE%E8%AF%8A%E6%9C%BA%E5%99%A8%E4%BA%BA/01-%E5%8C%BB%E7%96%97%E9%97%AE%E8%AF%8A%E6%9C%BA%E5%99%A8%E4%BA%BA%E5%AE%9E%E7%8E%B0.html#221_1","title":"2.2.1 \u6570\u636e\u5f20\u91cf\u8f6c\u6362","text":"<ul> <li>\u4ee3\u7801\u8def\u5f84\uff1allm_tuning/Gpt2_Chatbot/data_preprocess/preprocess.py</li> </ul> <pre><code>from transformers import BertTokenizerFast, BertTokenizer\nimport pickle  # \u4fdd\u5b58pkl\u6587\u4ef6\u7684\u547d\u4ee4\nfrom tqdm import tqdm  # \u52a0\u8f7d\u8fdb\u5ea6\u6761\n\n\ndef data_preprocess(train_txt_path, train_pkl_path):\n    \"\"\"\n    \u5bf9\u539f\u59cb\u8bed\u6599\u8fdb\u884ctokenizer\uff0c\u5c06\u6bcf\u6bb5\u5bf9\u8bdd\u5904\u7406\u6210\u5982\u4e0b\u5f62\u5f0f\uff1a\"[CLS]sentence1[SEP]sentence2[SEP]sentence3[SEP]\"\n    \"\"\"\n    # BertTokenizerFast\u76f8\u6bd4BertTokenizer\uff0c\u901f\u5ea6\u66f4\u5feb\uff0c\u5e76\u4e14\u63d0\u4f9b\u4e86 \u5b57\u8282\u7ea7\u522b\u7cbe\u786e\u5bf9\u9f50 \u7684 offset mapping\uff08\u53ef\u4ee5\u77e5\u9053 token \u5bf9\u5e94\u539f\u59cb\u6587\u672c\u7684\u5b57\u7b26\u4f4d\u7f6e\n    # tokenizer = BertTokenizerFast.from_pretrained(r'D:\\workspace\\python\\llm_tuning\\bert-base-chinese')\n    tokenizer = BertTokenizerFast('../vocab/vocab.txt',\n                                  sep_token=\"[SEP]\",\n                                  pad_token=\"[PAD]\",\n                                  cls_token=\"[CLS]\")\n    # print(f'tokenizer.vocab_size--&gt;{tokenizer.vocab_size}')\n\n    sep_id = tokenizer.sep_token_id  # \u83b7\u53d6\u5206\u9694\u7b26[SEP]\u7684token ID\n    cls_id = tokenizer.cls_token_id  # \u83b7\u53d6\u8d77\u59cb\u7b26[CLS]\u7684token ID\n    # print(f'sep_id--&gt;{sep_id}')\n    # print(f'cls_id--&gt;{cls_id}')\n\n    # \u8bfb\u53d6\u8bad\u7ec3\u6570\u636e\u96c6\n    with open(train_txt_path, \"r\", encoding=\"utf-8\") as f:\n        data = f.read()\n    # print(f'data--&gt;{data}')\n    # \u6839\u636e\u6362\u884c\u7b26\u533a\u5206\u4e0d\u540c\u7684\u5bf9\u8bdd\u6bb5\u843d\uff0c\u9700\u8981\u533a\u5206Windows\u548cLinux\\mac\u73af\u5883\u4e0b\u7684\u6362\u884c\u7b26\n    if \"\\r\\n\" in data:\n        train_data = data.split(\"\\r\\n\\r\\n\")\n    else:\n        train_data = data.split(\"\\n\\n\")\n    # print(f'len(train_data)--&gt;{len(train_data)}')  # \u6253\u5370\u5bf9\u8bdd\u6bb5\u843d\u6570\u91cf\n    # print(f'train_data[:2]--&gt;{train_data[:2]}')\n\n    # \u4fdd\u5b58\u6240\u6709\u7684\u5bf9\u8bdd\u6570\u636e,\u6bcf\u6761\u6570\u636e\u7684\u683c\u5f0f\u4e3a\uff1a\"[CLS]seq1[SEP]seq2[SEP]seq3[SEP]\"\n    dialogue_len = []  # \u8bb0\u5f55\u6240\u6709\u5bf9\u8bddtokenize\u5206\u8bcd\u4e4b\u540e\u7684\u957f\u5ea6\uff0c\u7528\u4e8e\u7edf\u8ba1\u4e2d\u4f4d\u6570\u4e0e\u5747\u503c\n    dialogue_list = []  # \u8bb0\u5f55\u6240\u6709\u5bf9\u8bdd\n\n    # \u904d\u5386\u8bad\u7ec3\u6570\u636e\uff0c\u5176\u4e2denumerate\u7528\u4e8e\u83b7\u53d6\u6bcf\u4e2a\u5bf9\u8bdd\u7684\u7d22\u5f15\u548c\u5185\u5bb9\uff0ctqdm\u7528\u4e8e\u663e\u793a\u8fdb\u5ea6\u6761\n    for index, dialogue in enumerate(tqdm(train_data)):\n        # \u6839\u636e\u4e0d\u540c\u7684\u6362\u884c\u7b26\u5206\u5272\u5bf9\u8bdd\uff0c\u4ee5\u5904\u7406\u6570\u636e\u96c6\u4e2d\u7684\u4e0d\u4e00\u81f4\u6027\n        if \"\\r\\n\" in dialogue:\n            sequences = dialogue.split(\"\\r\\n\")\n        else:\n            sequences = dialogue.split(\"\\n\")\n        # print(f'sequences--&gt;{sequences}')\n\n        # \u521d\u59cb\u5316input_ids\u5217\u8868\uff0c\u7528\u4e8e\u5b58\u50a8\u6240\u6709\u5bf9\u8bdd\u7684tokenized\u7248\u672c, \u6bcf\u4e2adialogue\u4ee5[CLS]seq1[sep]seq2[sep]\n        input_ids = [cls_id]\n        for sequence in sequences:\n            # \u5c06\u6bcf\u4e2a\u5bf9\u8bdd\u53e5\u5b50\u8fdb\u884ctokenize\uff0c\u5e76\u5c06\u7ed3\u679c\u62fc\u63a5\u5230input_ids\u5217\u8868\u4e2d\n            input_ids += tokenizer.encode(sequence, add_special_tokens=False)\n            # \u6bcf\u4e2aseq\u4e4b\u540e\u6dfb\u52a0[SEP]\uff0c\u8868\u793aseqs\u4f1a\u8bdd\u7ed3\u675f\n            input_ids.append(sep_id)\n        # print(f'input_ids--&gt;{input_ids}')\n\n        # \u5c06\u5bf9\u8bdd\u7684tokenize\u540e\u7684\u957f\u5ea6\u6dfb\u52a0\u5230\u5bf9\u8bdd\u957f\u5ea6\u5217\u8868\u4e2d\n        dialogue_len.append(len(input_ids))\n        # \u5c06tokenize\u540e\u7684\u5bf9\u8bdd\u6dfb\u52a0\u5230\u5bf9\u8bdd\u5217\u8868\u4e2d\n        dialogue_list.append(input_ids)\n        # break\n    # print(f'dialogue_list--&gt;{dialogue_list}')\n    print(f'dialogue_len--&gt;{sum(dialogue_len)/len(dialogue_len)}')\n\n    # \u4fdd\u5b58\u5bf9\u8bdd\u5217\u8868\u6570\u636e\u5230\u8bad\u7ec3\u6570\u636e\u7684pickle\u6587\u4ef6\u4e2d\n    with open(train_pkl_path, \"wb\") as f:\n        pickle.dump(dialogue_list, f)\n\n\nif __name__ == '__main__':\n    train_txt_path = '../data/medical_train.txt'\n    train_pkl_path = '../data/medical_train.pkl'\n    data_preprocess(train_txt_path, train_pkl_path)\n\n    valid_txt_path = '../data/medical_valid.txt'\n    valid_pkl_path = '../data/medical_valid.pkl'\n    data_preprocess(valid_txt_path, valid_pkl_path)\n</code></pre>"},{"location":"02-%E5%9F%BA%E4%BA%8EGPT2%E7%9A%84%E5%8C%BB%E7%96%97%E9%97%AE%E8%AF%8A%E6%9C%BA%E5%99%A8%E4%BA%BA/01-%E5%8C%BB%E7%96%97%E9%97%AE%E8%AF%8A%E6%9C%BA%E5%99%A8%E4%BA%BA%E5%AE%9E%E7%8E%B0.html#222-dataloader","title":"2.2.2 \u83b7\u53d6dataloader","text":""},{"location":"02-%E5%9F%BA%E4%BA%8EGPT2%E7%9A%84%E5%8C%BB%E7%96%97%E9%97%AE%E8%AF%8A%E6%9C%BA%E5%99%A8%E4%BA%BA/01-%E5%8C%BB%E7%96%97%E9%97%AE%E8%AF%8A%E6%9C%BA%E5%99%A8%E4%BA%BA%E5%AE%9E%E7%8E%B0.html#1dataset","title":"\uff081\uff09\u5c01\u88c5Dataset\u5bf9\u8c61","text":"<ul> <li>\u4ee3\u7801\u8def\u5f84\uff1allm_tuning/Gpt2_Chatbot/data_preprocess/dataset.py</li> </ul> <pre><code>from torch.utils.data import Dataset  # \u5bfc\u5165Dataset\u6a21\u5757\uff0c\u7528\u4e8e\u5b9a\u4e49\u81ea\u5b9a\u4e49\u6570\u636e\u96c6\nimport torch  # \u5bfc\u5165torch\u6a21\u5757\uff0c\u7528\u4e8e\u5904\u7406\u5f20\u91cf\u548c\u6784\u5efa\u795e\u7ecf\u7f51\u7edc\nimport pickle\n\n\n# \u81ea\u5b9a\u4e49\u6570\u636e\u96c6\u7c7b\uff0c\u7ee7\u627f\u81eaDataset\u7c7b\nclass MyDataset(Dataset):\n    def __init__(self, input_list, max_len):\n        \"\"\"\n        \u521d\u59cb\u5316\u51fd\u6570\uff0c\u7528\u4e8e\u8bbe\u7f6e\u6570\u636e\u96c6\u7684\u5c5e\u6027\n        :param input_list: \u8f93\u5165\u5217\u8868\uff0c\u5305\u542b\u6240\u6709\u5bf9\u8bdd\u7684tokenize\u540e\u7684\u8f93\u5165\u5e8f\u5217\n        :param max_len: \u6700\u5927\u5e8f\u5217\u957f\u5ea6\uff0c\u7528\u4e8e\u5bf9\u8f93\u5165\u8fdb\u884c\u622a\u65ad\u6216\u586b\u5145\n        \"\"\"\n        super().__init__()\n        self.input_list = input_list  # \u5c06\u8f93\u5165\u5217\u8868\u8d4b\u503c\u7ed9\u6570\u636e\u96c6\u7684input_list\u5c5e\u6027\n        self.max_len = max_len  # \u5c06\u6700\u5927\u5e8f\u5217\u957f\u5ea6\u8d4b\u503c\u7ed9\u6570\u636e\u96c6\u7684max_len\u5c5e\u6027\n\n    def __len__(self):\n        return len(self.input_list)  # \u8fd4\u56de\u6570\u636e\u96c6\u7684\u957f\u5ea6\n\n    def __getitem__(self, index):\n        input_ids = self.input_list[index]  # \u83b7\u53d6\u7ed9\u5b9a\u7d22\u5f15\u5904\u7684\u8f93\u5165\u5e8f\u5217\n        # print(f'input_ids--&gt;{input_ids}')\n\n        # \u6839\u636e\u6700\u5927\u5e8f\u5217\u957f\u5ea6\u5bf9\u8f93\u5165\u8fdb\u884c\u622a\u65ad\u6216\u586b\u5145\uff08\u586b\u5145\u903b\u8f91\u5728dataloader\u6587\u4ef6\u4e2d\u5b9e\u73b0\uff09\n        input_ids = input_ids[:self.max_len]  # \u622a\u65ad\n        input_ids = torch.tensor(input_ids, dtype=torch.long)  # \u5c06\u8f93\u5165\u5e8f\u5217\u8f6c\u6362\u4e3a\u5f20\u91cflong\u7c7b\u578b\n        return input_ids  # \u8fd4\u56de\u6837\u672c\u7684\u8f93\u5165\u5e8f\u5217\u5f20\u91cf\n\n\nif __name__ == '__main__':\n    with open('../data/medical_train.pkl', \"rb\") as f:\n        train_input_list = pickle.load(f)  # \u4ece\u6587\u4ef6\u4e2d\u52a0\u8f7d\u8f93\u5165\u5217\n    print(f'train_input_list--&gt;{len(train_input_list)}')\n    print(f'train_input_list--&gt;{type(train_input_list)}')\n    print(f'train_input_list--&gt;{train_input_list[0]}')\n\n    mydataset = MyDataset(input_list=train_input_list, max_len=300)\n    print(f'mydataset--&gt;{len(mydataset)}')\n    result = mydataset[0]\n    print(result)\n</code></pre>"},{"location":"02-%E5%9F%BA%E4%BA%8EGPT2%E7%9A%84%E5%8C%BB%E7%96%97%E9%97%AE%E8%AF%8A%E6%9C%BA%E5%99%A8%E4%BA%BA/01-%E5%8C%BB%E7%96%97%E9%97%AE%E8%AF%8A%E6%9C%BA%E5%99%A8%E4%BA%BA%E5%AE%9E%E7%8E%B0.html#2dataloader","title":"\uff082\uff09\u5c01\u88c5DataLoader\u5bf9\u8c61","text":"<ul> <li>\u4ee3\u7801\u8def\u5f84\uff1a/home/user/ProjectStudy/Gpt2_Chatbot/data_preprocess/dataloader.py</li> </ul> <pre><code>import torch.nn.utils.rnn as rnn_utils  # \u5bfc\u5165rnn_utils\u6a21\u5757\uff0c\u7528\u4e8e\u5904\u7406\u53ef\u53d8\u957f\u5ea6\u5e8f\u5217\u7684\u586b\u5145\u548c\u6392\u5e8f\nfrom torch.utils.data import Dataset, DataLoader  # \u5bfc\u5165Dataset\u548cDataLoader\u6a21\u5757\uff0c\u7528\u4e8e\u52a0\u8f7d\u548c\u5904\u7406\u6570\u636e\u96c6\nimport pickle  # \u5bfc\u5165pickle\u6a21\u5757\uff0c\u7528\u4e8e\u5e8f\u5217\u5316\u548c\u53cd\u5e8f\u5217\u5316Python\u5bf9\u8c61\n\nfrom Gpt2_Chatbot.data_preprocess.dataset import MyDataset\nfrom Gpt2_Chatbot.parameter_config import ParameterConfig\n\nparams = ParameterConfig()\n\n\ndef load_dataset(train_path, valid_path):\n    \"\"\"\n    \u52a0\u8f7d\u8bad\u7ec3\u96c6\u548c\u9a8c\u8bc1\u96c6\n    :param train_path: \u8bad\u7ec3\u6570\u636e\u96c6\u8def\u5f84\n    :return: \u8bad\u7ec3\u6570\u636e\u96c6\u548c\u9a8c\u8bc1\u6570\u636e\u96c6\n    \"\"\"\n    with open(train_path, \"rb\") as f:\n        train_input_list = pickle.load(f)  # \u4ece\u6587\u4ef6\u4e2d\u52a0\u8f7d\u8f93\u5165\u5217\u8868\n\n    with open(valid_path, \"rb\") as f:\n        valid_input_list = pickle.load(f)  # \u4ece\u6587\u4ef6\u4e2d\u52a0\u8f7d\u8f93\u5165\u5217\u8868\n    # \u5212\u5206\u8bad\u7ec3\u96c6\u4e0e\u9a8c\u8bc1\u96c6\n    # print(len(train_input_list))  # \u6253\u5370\u8f93\u5165\u5217\u8868\u7684\u957f\u5ea6\n    # print(train_input_list[0])\n\n    train_dataset = MyDataset(train_input_list, params.max_len)  # \u521b\u5efa\u8bad\u7ec3\u6570\u636e\u96c6\u5bf9\u8c61\n    val_dataset = MyDataset(valid_input_list, params.max_len)  # \u521b\u5efa\u9a8c\u8bc1\u6570\u636e\u96c6\u5bf9\u8c61\n    return train_dataset, val_dataset  # \u8fd4\u56de\u8bad\u7ec3\u6570\u636e\u96c6\u548c\u9a8c\u8bc1\u6570\u636e\u96c6\n\n\ndef collate_fn(batch):\n    \"\"\"\n    \u81ea\u5b9a\u4e49\u7684collate_fn\u51fd\u6570\uff0c\u7528\u4e8e\u5c06\u6570\u636e\u96c6\u4e2d\u7684\u6837\u672c\u8fdb\u884c\u6279\u5904\u7406\n    :param batch: \u6837\u672c\u5217\u8868\n    :return: \u7ecf\u8fc7\u586b\u5145\u7684\u8f93\u5165\u5e8f\u5217\u5f20\u91cf\u548c\u6807\u7b7e\u5e8f\u5217\u5f20\u91cf\n    \"\"\"\n    # \u5bf9\u8f93\u5165\u5e8f\u5217\u8fdb\u884c\u586b\u5145\uff0c\u4f7f\u5176\u957f\u5ea6\u4e00\u81f4\n    # rnn_utils.pad_sequence\uff1a\u5c06\u6839\u636e\u4e00\u4e2abatch\u4e2d\uff0c\u6700\u5927\u53e5\u5b50\u957f\u5ea6\uff0c\u8fdb\u884c\u8865\u9f50\n    # \u53c2\u6570batch_first=True\u8868\u793a\u8fd4\u56de\u7684\u5f20\u91cf\u5f62\u72b6\u4e3a(batch_size, max_seq_length)\n    input_ids = rnn_utils.pad_sequence(batch, batch_first=True, padding_value=0)\n\n    # \u5bf9\u6807\u7b7e\u5e8f\u5217\u8fdb\u884c\u586b\u5145, \u4f7f\u5176\u957f\u5ea6\u4e00\u81f4, \u7528-100\u8fdb\u884c\u586b\u5145(\u8ba1\u7b97\u635f\u5931\u503c\u4f1a\u5ffd\u7565-100)\n    # \u540e\u7eed\u8ba1\u7b97\u635f\u5931\u65f6, \u9884\u6d4b\u7684\u6807\u7b7e\u548c\u771f\u5b9e\u7684\u6807\u7b7e\u9700\u8981\u8fdb\u884c\u9519\u4e00\u4f4d(input_ids\u548clabels\u4e00\u6837)\n    labels = rnn_utils.pad_sequence(batch, batch_first=True, padding_value=-100)\n    # print(f'labels--&gt;{labels}')\n    return input_ids, labels  # \u8fd4\u56de\u7ecf\u8fc7\u586b\u5145\u7684\u8f93\u5165\u5e8f\u5217\u5f20\u91cf\u548c\u6807\u7b7e\u5e8f\u5217\u5f20\u91cf\n\n\ndef get_dataloader(train_path, valid_path):\n    \"\"\"\n    \u83b7\u53d6\u8bad\u7ec3\u6570\u636e\u96c6\u548c\u9a8c\u8bc1\u6570\u636e\u96c6\u7684DataLoader\u5bf9\u8c61\n    :param train_path: \u8bad\u7ec3\u6570\u636e\u96c6\u8def\u5f84\n    :param valid_path: \u9a8c\u8bc1\u6570\u636e\u96c6\u8def\u5f84\n    :return: \u8bad\u7ec3\u6570\u636e\u96c6\u7684DataLoader\u5bf9\u8c61\u548c\u9a8c\u8bc1\u6570\u636e\u96c6\u7684DataLoader\u5bf9\u8c61\n    \"\"\"\n    train_dataset, val_dataset = load_dataset(train_path, valid_path)  # \u52a0\u8f7d\u8bad\u7ec3\u6570\u636e\u96c6\u548c\u9a8c\u8bc1\u6570\u636e\u96c6\n    # print(f'train_dataset--&gt;{len(train_dataset)}')\n    # print(f'val_dataset--&gt;{len(val_dataset)}')\n\n    # \u521b\u5efa\u8bad\u7ec3\u6570\u636e\u96c6\u7684DataLoader\u5bf9\u8c61\n    train_dataloader = DataLoader(train_dataset,\n                                  batch_size=params.batch_size,\n                                  shuffle=False,\n                                  collate_fn=collate_fn,\n                                  drop_last=True)\n\n    # \u521b\u5efa\u9a8c\u8bc1\u6570\u636e\u96c6\u7684DataLoader\u5bf9\u8c61\n    validate_dataloader = DataLoader(val_dataset,\n                                     batch_size=params.batch_size,\n                                     shuffle=True,\n                                     collate_fn=collate_fn,\n                                     drop_last=True)\n\n    # \u8fd4\u56de\u8bad\u7ec3\u6570\u636e\u96c6\u7684DataLoader\u5bf9\u8c61\u548c\u9a8c\u8bc1\u6570\u636e\u96c6\u7684DataLoader\u5bf9\u8c61\n    return train_dataloader, validate_dataloader\n\n\nif __name__ == '__main__':\n    train_dataloader, validate_dataloader = get_dataloader(params.train_path, params.valid_path)\n    for input_ids, labels in train_dataloader:\n        print(f'input_ids--&gt;{input_ids.shape}')\n        print(f'labels--&gt;{labels.shape}')\n        break\n</code></pre>"},{"location":"02-%E5%9F%BA%E4%BA%8EGPT2%E7%9A%84%E5%8C%BB%E7%96%97%E9%97%AE%E8%AF%8A%E6%9C%BA%E5%99%A8%E4%BA%BA/01-%E5%8C%BB%E7%96%97%E9%97%AE%E8%AF%8A%E6%9C%BA%E5%99%A8%E4%BA%BA%E5%AE%9E%E7%8E%B0.html#3","title":"3. \u6a21\u578b\u642d\u5efa","text":""},{"location":"02-%E5%9F%BA%E4%BA%8EGPT2%E7%9A%84%E5%8C%BB%E7%96%97%E9%97%AE%E8%AF%8A%E6%9C%BA%E5%99%A8%E4%BA%BA/01-%E5%8C%BB%E7%96%97%E9%97%AE%E8%AF%8A%E6%9C%BA%E5%99%A8%E4%BA%BA%E5%AE%9E%E7%8E%B0.html#31","title":"3.1 \u6a21\u578b\u67b6\u6784\u4ecb\u7ecd","text":"<ul> <li> <p>\u6a21\u578b\u67b6\u6784\u89e3\u6790\uff1a</p> <ul> <li>\u8f93\u5165\u5c42\uff1a\u8bcd\u5d4c\u5165\u5c42\uff1aWordEmbedding +\u4f4d\u7f6e\u5d4c\u5165\u5c42\uff1aPositionEmbedding</li> <li>\u4e2d\u95f4\u5c42\uff1aTransformer\u7684Decoder\u6a21\u5757---12\u5c42</li> <li>\u8f93\u51fa\u5c42\uff1a\u7ebf\u6027\u5168\u8fde\u63a5\u5c42</li> </ul> </li> <li> <p>\u6a21\u578b\u4e3b\u8981\u53c2\u6570\u7b80\u4ecb(\u8be6\u89c1\u6a21\u578b\u7684config.json\u6587\u4ef6):</p> <ul> <li>n_embd: 768</li> <li>n_head: 12</li> <li>n_layer: 12</li> <li>n_positions: 1024</li> <li>vocab_size: 13317</li> </ul> </li> </ul>"},{"location":"02-%E5%9F%BA%E4%BA%8EGPT2%E7%9A%84%E5%8C%BB%E7%96%97%E9%97%AE%E8%AF%8A%E6%9C%BA%E5%99%A8%E4%BA%BA/01-%E5%8C%BB%E7%96%97%E9%97%AE%E8%AF%8A%E6%9C%BA%E5%99%A8%E4%BA%BA%E5%AE%9E%E7%8E%B0.html#32-gpt2","title":"3.2 GPT2\u6a21\u578b\u51c6\u5907","text":"<ul> <li>\u672c\u6b21\u9879\u76ee\u4f7f\u7528GPT2\u7684\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u56e0\u6b64\u4e0d\u9700\u8981\u989d\u5916\u642d\u5efaModel\u7c7b\uff0c\u4e0b\u9762\u4ee3\u7801\u662f\u5982\u4f55\u76f4\u63a5\u52a0\u8f7d\u4f7f\u7528GPT2\u9884\u8bad\u7ec3\u6a21\u578b</li> <li>\u4ee3\u7801\u793a\u4f8b:</li> </ul> <pre><code>from transformers import GPT2LMHeadModel, GPT2Config\n# \u521b\u5efa\u6a21\u578b\nif params.pretrained_model:  \n    # \u52a0\u8f7d\u9884\u8bad\u7ec3\u6a21\u578b\n    model = GPT2LMHeadModel.from_pretrained(params.pretrained_model)\nelse:  \n    # \u521d\u59cb\u5316\u6a21\u578b\n    model_config = GPT2Config.from_json_file(params.config_json)\n    model = GPT2LMHeadModel(config=model_config)\n</code></pre> <ul> <li>\u5982\u679c\u4f7f\u7528\u7b2c\u4e8c\u79cd\u65b9\u5f0f\uff0c\u9700\u8981\u914d\u7f6e\u6a21\u578b\u7684\u53c2\u6570</li> </ul> <p>\u4f4d\u7f6e\uff1allm_tuning/Gpt2_Chatbot/config/config.json</p> <pre><code>{\n  \"activation_function\": \"gelu_new\",\n  \"architectures\": [\n    \"GPT2LMHeadModel\"\n  ],\n  \"attn_pdrop\": 0.1,\n  \"bos_token_id\": 50256,\n  \"embd_pdrop\": 0.1,\n  \"eos_token_id\": 50256,\n  \"gradient_checkpointing\": false,\n  \"initializer_range\": 0.02,\n  \"layer_norm_epsilon\": 1e-05,\n  \"model_type\": \"gpt2\",\n  \"n_ctx\": 1024,\n  \"n_embd\": 768,\n  \"n_head\": 12,\n  \"n_inner\": null,\n  \"n_layer\": 12,\n  \"n_positions\": 1024,\n  \"output_past\": true,\n  \"resid_pdrop\": 0.1,\n  \"summary_activation\": null,\n  \"summary_first_dropout\": 0.1,\n  \"summary_proj_to_labels\": true,\n  \"summary_type\": \"cls_index\",\n  \"summary_use_proj\": true,\n  \"task_specific_params\": {\n    \"text-generation\": {\n      \"do_sample\": true,\n      \"max_length\": 400\n    }\n  },\n  \"tokenizer_class\": \"BertTokenizer\",\n  \"transformers_version\": \"4.2.0\",\n  \"use_cache\": true,\n  \"vocab_size\": 13317\n}\n</code></pre>"},{"location":"02-%E5%9F%BA%E4%BA%8EGPT2%E7%9A%84%E5%8C%BB%E7%96%97%E9%97%AE%E8%AF%8A%E6%9C%BA%E5%99%A8%E4%BA%BA/01-%E5%8C%BB%E7%96%97%E9%97%AE%E8%AF%8A%E6%9C%BA%E5%99%A8%E4%BA%BA%E5%AE%9E%E7%8E%B0.html#4","title":"4. \u6a21\u578b\u8bad\u7ec3\u548c\u9a8c\u8bc1","text":"<ul> <li>\u4e3b\u8981\u4ee3\u7801</li> </ul> <ul> <li> <p>\u4ee3\u7801\u4f4d\u7f6e</p> </li> <li> <p>\u8bad\u7ec3\u4e3b\u51fd\u6570\uff1allm_tuning/Gpt2_Chatbot/train.py</p> </li> <li> <p>\u8f85\u52a9\u5de5\u5177\u7c7b\uff1allm_tuning/Gpt2_Chatbot/functions_tools.py</p> </li> </ul> <ul> <li>trian.py\u4ee3\u7801\u89e3\u6790</li> </ul> <pre><code>import torch\nimport os\nfrom datetime import datetime\n\nfrom torch.optim import AdamW\nfrom tqdm import tqdm\nfrom transformers import GPT2LMHeadModel, GPT2Config, BertTokenizerFast\nfrom transformers import get_linear_schedule_with_warmup\n\nfrom Gpt2_Chatbot.data_preprocess.dataloader import get_dataloader\nfrom Gpt2_Chatbot.functions_tools import calculate_acc\nfrom Gpt2_Chatbot.parameter_config import ParameterConfig\n\n\ndef train_epoch(model, train_dataloader, optimizer, scheduler, epoch, args):\n    '''\n    :param model: GPT2\u6a21\u578b\n    :param train_dataloader: \u8bad\u7ec3\u6570\u636e\u96c6\n    :param optimizer: \u4f18\u5316\u5668\uff1a\u66f4\u65b0\u53c2\u6570\n    :param scheduler: \u5b66\u4e60\u7387\u9884\u70ed\n    :param epoch: \u5f53\u524d\u7684\u8f6e\u6b21\n    :param args: \u6a21\u578b\u914d\u7f6e\u6587\u4ef6\u7684\u53c2\u6570\u5bf9\u8c61\n    :return: \u6bcf\u6b21\u8fed\u4ee3\u7684\u5e73\u5747\u635f\u5931\u503c\n    '''\n    print(\"start training...\")\n    # \u6307\u660e\u6a21\u578b\u8bad\u7ec3\n    model.train()\n    device = args.device\n    # \u5bf9\u4e8eignore_index\u7684label token\u4e0d\u8ba1\u7b97\u68af\u5ea6\n    ignore_index = args.ignore_index\n    epoch_start_time = datetime.now()\n    total_loss = 0  # \u8bb0\u5f55\u4e0b\u6574\u4e2aepoch\u7684loss\u7684\u603b\u548c\n\n    # epoch_correct_num: \u6bcf\u4e2aepoch\u4e2d,output\u9884\u6d4b\u6b63\u786e\u7684word\u7684\u6570\u91cf\n    # epoch_total_num: \u6bcf\u4e2aepoch\u4e2d,output\u9884\u6d4b\u7684word\u7684\u603b\u6570\u91cf\n    epoch_correct_num, epoch_total_num = 0, 0\n\n    for batch_idx, (input_ids, labels) in enumerate(tqdm(train_dataloader)):\n        input_ids = input_ids.to(device)\n        labels = labels.to(device)\n        # print(f'input_ids--&gt;{input_ids.shape}')\n        # print(f'labels--&gt;{labels.shape}')\n        # \u5982\u679c\u5bf9\u6a21\u578b\u8f93\u5165\u4e0d\u4ec5\u5305\u542binput\u8fd8\u5305\u542b\u6807\u7b7e\uff0c\u90a3\u4e48\u5f97\u5230\u7ed3\u679c\u76f4\u63a5\u5c31\u6709loss\u503c\u3002\u5176\u4e2d\u6a21\u578b\u5185\u90e8\u4f1a\u628a logits/labels \u505a\u4f4d\u79fb\u5bf9\u9f50\uff08labels \u901a\u5e38\u53ef\u4ee5\u7b49\u4e8e input_ids\uff0c \u6a21\u578b\u5185\u90e8\u4f1a\u628a logits[..., :-1, :] \u4e0e labels[..., 1:] \u5bf9\u9f50\u8ba1\u7b97\u635f\u5931\uff09\uff0c\u5e76\u5bf9\u88ab\u6807\u4e3a -100 \u7684 label \u5ffd\u7565\u4e0d\u8ba1\u3002\n        # \u5982\u679c\u5bf9\u6a21\u578b\u7684\u8f93\u5165\u53ea\u6709input\uff0c\u90a3\u4e48\u6a21\u578b\u7684\u7ed3\u679c\u4e0d\u4f1a\u542b\u6709loss\u503c\uff0c\u6b64\u65f6\uff0c\u53ef\u4ee5\u81ea\u5b9a\u4e49\u51fd\u6570\u6765\u8ba1\u7b97\u635f\u5931\n        outputs = model(input_ids, labels=labels)\n        # print(f'outputs--&gt;{outputs}')\n        # print(f'outputs--&gt;{outputs.keys()}')\n        # print(f'outputs.logits--&gt;{outputs.logits.shape}')\n        # print(f'outputs.loss--&gt;{outputs.loss}')\n        logits = outputs.logits\n        loss = outputs.loss\n\n        # \u7d2f\u52a0\u5f53\u524d\u635f\u5931\u5230\u603b\u635f\u5931\u4e2d\n        total_loss += loss.item()\n\n        # \u7edf\u8ba1\u8be5batch\u7684\u9884\u6d4btoken\u7684\u6b63\u786e\u6570\u4e0e\u603b\u6570\n        batch_correct_num, batch_total_num = calculate_acc(logits,\n                                                           labels,\n                                                           ignore_index=ignore_index)\n        # print(f'batch_correct_num--&gt;{batch_correct_num}')\n        # print(f'batch_total_num--&gt;{batch_total_num}')\n\n        # \u8ba1\u7b97\u8be5batch\u7684accuracy\n        batch_acc = batch_correct_num / batch_total_num\n        # \u7edf\u8ba1\u8be5epoch\u7684\u9884\u6d4btoken\u7684\u6b63\u786e\u6570\u4e0e\u603b\u6570\n        epoch_correct_num += batch_correct_num\n        epoch_total_num += batch_total_num\n\n        '''\n        self.gradient_accumulation_steps = 4 \u7d2f\u79ef\u7684\u6b65\u6570\n        \u5982\u679c\u8bbe\u7f6e\u4e86\u68af\u5ea6\u7d2f\u79ef\u6b65\u6570\u4e14\u5927\u4e8e1\uff0c\u5219\u9700\u8981\u5bf9\u635f\u5931\u8fdb\u884c\u76f8\u5e94\u7684\u8c03\u6574\n        \u8fd9\u662f\u56e0\u4e3a\u5728\u7d2f\u79ef\u6b65\u6570\u5185\uff0c\u635f\u5931\u4f1a\u88ab\u7d2f\u79ef\u8ba1\u7b97\uff0c\u800c\u4e0d\u662f\u5728\u6bcf\u4e2a\u8bad\u7ec3\u6b65\u9aa4\u540e\u7acb\u5373\u66f4\u65b0\u6743\u91cd\n        \u901a\u8fc7\u5c06\u635f\u5931\u9664\u4ee5\u68af\u5ea6\u7d2f\u79ef\u6b65\u6570\uff0c\u53ef\u4ee5\u5f97\u5230\u6bcf\u6b21\u7d2f\u79ef\u540e\u5b9e\u9645\u5e94\u8be5\u5e94\u7528\u7684\u5e73\u5747\u635f\u5931        \n        '''\n        if args.gradient_accumulation_steps &gt; 1:\n            loss = loss / args.gradient_accumulation_steps\n\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=args.max_grad_norm)\n\n        # \u5728\u5230\u8fbe\u68af\u5ea6\u7d2f\u8ba1\u7684\u6b65\u6570\u4e4b\u540e\uff0c\u6267\u884c\u53c2\u6570\u66f4\u65b0\n        if (batch_idx + 1) % args.gradient_accumulation_steps == 0:\n            # \u66f4\u65b0\u53c2\u6570\n            optimizer.step()\n            # \u66f4\u65b0\u5b66\u4e60\u7387\n            scheduler.step()\n            # \u6e05\u7a7a\u68af\u5ea6\u4fe1\u606f\n            optimizer.zero_grad()\n\n        # \u5f53batch_idx\u52a01\u80fd\u88abargs.loss_step\u6574\u9664\u65f6\uff0c\u8f93\u51fa\u5f53\u524d\u6279\u6b21\u7684\u8bad\u7ec3\u4fe1\u606f\n        if (batch_idx + 1) % args.loss_step == 0:\n            print(\n                \"batch {} of epoch {}, loss {:.4f}, batch_acc {:.4f}, lr {}\".format(\n                    batch_idx + 1, epoch + 1, loss.item() * args.gradient_accumulation_steps,\n                    batch_acc, scheduler.get_last_lr()[0]))\n            # break\n\n    # \u8bb0\u5f55\u5f53\u524depoch\u7684\u5e73\u5747loss\u4e0eaccuracy\n    epoch_mean_loss = total_loss / len(train_dataloader)\n    epoch_mean_acc = epoch_correct_num / epoch_total_num\n    print(\"epoch {}: loss {:.4f}, predict_acc {:.4f}\".format(epoch + 1, epoch_mean_loss, epoch_mean_acc))\n\n    # \u5728\u6bcf\u4e2aepoch\u7ed3\u675f\u65f6\uff0c\u6839\u636e\u6761\u4ef6\u4fdd\u5b58\u6a21\u578b\n    if (epoch + 1) % 2 == 0 or epoch == args.epochs:\n        # \u6253\u5370\u4fdd\u5b58\u6a21\u578b\u7684\u4fe1\u606f\n        print('\u6b63\u5728\u4fdd\u5b58\u7b2c {} \u8f6e\u6b21\u6a21\u578b'.format(epoch + 1))\n        # \u6784\u9020\u6a21\u578b\u4fdd\u5b58\u8def\u5f84\n        model_path = os.path.join(args.save_model_path, 'bj_epoch{}'.format(epoch + 1))\n        # \u5982\u679c\u6a21\u578b\u4fdd\u5b58\u8def\u5f84\u4e0d\u5b58\u5728\uff0c\u5219\u521b\u5efa\u8be5\u76ee\u5f55\n        if not os.path.exists(model_path):\n            os.mkdir(model_path)\n        # \u4fdd\u5b58\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u65b9\u5f0f\n        model.save_pretrained(model_path)\n        # \u6253\u5370\u5b8c\u6210\u4fe1\u606f\n        print('\u7b2c {} \u8f6e\u6b21\u6a21\u578b\u4fdd\u5b58\u5b8c\u6210\u3002'.format(epoch + 1))\n        # \u8bb0\u5f55epoch\u5b8c\u6210\u7684\u65f6\u95f4\n        epoch_finish_time = datetime.now()\n        # \u6253\u5370\u5b8c\u6210\u8be5\u8f6e\u6b21\u6240\u9700\u7684\u65f6\u95f4\n        print('\u5b8c\u6210\u672c\u8f6e\u6b21\u6240\u82b1\u65f6\u95f4\u4e3a: {:.4f}'.format(epoch_finish_time - epoch_start_time))\n\n    return epoch_mean_loss\n\n\ndef validate_epoch(model, validate_dataloader, epoch, args):\n    '''\n    \u9a8c\u8bc1\u6a21\u578b\u5728\u4e00\u4e2aepoch\u4e0a\u7684\u8868\u73b0\u3002\n    :param model: \u8981\u9a8c\u8bc1\u7684\u6a21\u578b\n    :param validate_dataloader: \u9a8c\u8bc1\u6570\u636e\u52a0\u8f7d\u5668\n    :param epoch: \u5f53\u524d\u9a8c\u8bc1\u7684epoch\u6570\n    :param args: \u5305\u542b\u8bbe\u5907\u4fe1\u606f\u7b49\u7684\u53c2\u6570\u5bf9\u8c61\n    :return: \u5f53\u524depoch\u7684\u5e73\u5747\u635f\u5931\n    '''\n    print(\"start validating...\")\n    model.eval()  # \u5c06\u6a21\u578b\u8bbe\u7f6e\u4e3a\u8bc4\u4f30\u6a21\u5f0f\n    device = args.device  # \u83b7\u53d6\u8bbe\u5907\u4fe1\u606f\n    epoch_start_time = datetime.now()  # \u8bb0\u5f55epoch\u5f00\u59cb\u65f6\u95f4\n    total_loss = 0  # \u521d\u59cb\u5316\u603b\u635f\u5931\n\n    # \u6355\u83b7cuda out of memory exception\n    with torch.no_grad():  # \u7981\u6b62\u8ba1\u7b97\u68af\u5ea6\u4ee5\u8282\u7701\u5185\u5b58\n        for batch_idx, (input_ids, labels) in enumerate(tqdm(validate_dataloader)):\n            input_ids = input_ids.to(device)  # \u5c06\u8f93\u5165\u6570\u636e\u79fb\u52a8\u5230\u6307\u5b9a\u8bbe\u5907\n            labels = labels.to(device)  # \u5c06\u6807\u7b7e\u79fb\u52a8\u5230\u6307\u5b9a\u8bbe\u5907\n            outputs = model(input_ids, labels=labels)  # \u6a21\u578b\u524d\u5411\u4f20\u64ad\n\n            # logits = outputs.logits\n            loss = outputs.loss  # \u83b7\u53d6\u635f\u5931\n            total_loss += loss.item()  # \u7d2f\u52a0\u635f\u5931\n\n    # \u8bb0\u5f55\u5f53\u524depoch\u7684\u5e73\u5747loss\n    epoch_mean_loss = total_loss / len(validate_dataloader)  # \u8ba1\u7b97\u5e73\u5747\u635f\u5931\n    print(\"\u7b2c {} \u8f6e\u7684\u6a21\u578b\u5728\u9a8c\u8bc1\u96c6\u4e0a\u7684\u5e73\u5747\u635f\u5931\u4e3a\uff1a{:.4f}\".format(epoch + 1, epoch_mean_loss))\n    epoch_finish_time = datetime.now()  # \u8bb0\u5f55epoch\u7ed3\u675f\u65f6\u95f4\n    print('\u5b8c\u6210\u672c\u8f6e\u6b21\u9a8c\u8bc1\u6240\u82b1\u65f6\u95f4\u4e3a: {:.4f}'.format(epoch_finish_time - epoch_start_time))\n    return epoch_mean_loss  # \u8fd4\u56de\u5e73\u5747\u635f\u5931\n\n\ndef train(model, train_dataloader, validate_dataloader, args):\n    # eps\uff0c\u4e3a\u4e86\u589e\u52a0\u6570\u503c\u8ba1\u7b97\u7684\u7a33\u5b9a\u6027\u800c\u52a0\u5230\u5206\u6bcd\u91cc\u7684\u9879\uff0c\u5176\u4e3a\u4e86\u9632\u6b62\u5728\u5b9e\u73b0\u4e2d\u9664\u4ee5\u96f6\n    optimizer = AdamW(model.parameters(), lr=args.lr, eps=args.eps)\n    # \u8fd9\u91cc\u4f7f\u7528\u5b66\u4e60\u7387\u9884\u70ed\u5904\u7406\u4f18\u5316\n    # print(f'\u6bcf\u4e2a\u8f6e\u6b21\u4e2d\u7684\u6279\u6b21\u6570--&gt;{len(train_dataloader)}')\n    # t_total\u4e3a\u4f7f\u7528\u68af\u5ea6\u7d2f\u79ef\u65f6\uff0c\u6a21\u578b\u8bad\u7ec3\u5b8c\u6bd5\u4e00\u5171 \u66f4\u65b0\u53c2\u6570\u7684\u6b21\u6570\n    t_total = len(train_dataloader) // args.gradient_accumulation_steps * args.epochs\n    scheduler = get_linear_schedule_with_warmup(optimizer,\n                                                num_warmup_steps=args.warmup_steps,\n                                                num_training_steps=t_total)\n\n    # \u7528\u4e8e\u8bb0\u5f55\u6bcf\u4e2aepoch\u8bad\u7ec3\u548c\u9a8c\u8bc1\u7684loss\n    train_losses, validate_losses = [], []\n    # \u8bb0\u5f55\u9a8c\u8bc1\u96c6\u7684\u6700\u5c0floss\n    best_val_loss = 10000\n    # \u5f00\u59cb\u8bad\u7ec3\n    for epoch in range(args.epochs):\n        # ========== train ========== #\n        train_loss = train_epoch(\n            model=model, train_dataloader=train_dataloader,\n            optimizer=optimizer, scheduler=scheduler,\n            epoch=epoch, args=args)\n        train_losses.append(train_loss)\n        # ========== validate ========== #\n        validate_loss = validate_epoch(\n            model=model, validate_dataloader=validate_dataloader,\n            epoch=epoch, args=args)\n        validate_losses.append(validate_loss)\n\n        # \u4fdd\u5b58\u5f53\u524d\u635f\u5931\u6700\u4f4e\u7684\u6a21\u578b\n        if validate_loss &lt; best_val_loss:\n            best_val_loss = validate_loss\n            print('\u4fdd\u5b58\u5f53\u524d\u6700\u597d\u7684\u6a21\u578b\uff0c\u8f6e\u6b21\u4e3a epoch {}'.format(epoch + 1))\n            model_path = os.path.join(args.save_model_path, 'min_loss_model_bj'.format(epoch + 1))\n            if not os.path.exists(model_path):\n                os.mkdir(model_path)\n            model.save_pretrained(model_path)\n\n\ndef run():\n    # \u521d\u59cb\u5316\u914d\u7f6e\u53c2\u6570\n    params = ParameterConfig()\n\n    # \u8bbe\u7f6e\u4f7f\u7528\u54ea\u4e9b\u663e\u5361\u8fdb\u884c\u8bad\u7ec3:\u9ed8\u8ba4\u4e3a0\n    # \u5982\u679c\u4f60\u7684\u7535\u8111\u6709\u5927\u4e8e1\u5f20\u7684\u663e\u5361\uff0c\u53ef\u4ee5\u9009\u62e9\u4f7f\u7528\n    # nvidia-smi:\u67e5\u8be2\u5f53\u524d\u663e\u5361\u7684\u72b6\u6001\uff1a4090(16G\u663e\u5b58), L40(48G\u663e\u5b58), L20(24G\u663e\u5b58), A100(80G), H100(80G), T4(16G), V100(16G)\n    # os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\u6570\u5b570\u4ee3\u8868\u4f60\u7684\u7b2c\u4e00\u5f20\u663e\u5361\n    # os.environ[\"CUDA_VISIBLE_DEVICES\"] = '1'\u6570\u5b571\u4ee3\u8868\u4f60\u7684\u7b2c\u4e8c\u5f20\u663e\u5361\n    # os.environ[\"CUDA_VISIBLE_DEVICES\"] ='0, 1'\u4ee3\u8868\u540c\u65f6\u5229\u75280\u548c1\u4e24\u5f20\u663e\u5361\n    os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'  # \u6307\u5b9a\u7b2c\u4e00\u5f20\u663e\u5361\n\n    # \u521d\u59cb\u5316tokenizer\n    tokenizer = BertTokenizerFast(params.vocab_path,\n                                  sep_token=\"[SEP]\",\n                                  pad_token=\"[PAD]\",\n                                  cls_token=\"[CLS]\")\n\n    # \u521b\u5efa\u6a21\u578b\u7684\u8f93\u51fa\u76ee\u5f55\n    # \u5982\u679c\u6ca1\u6709\u521b\u5efa\u4f1a\u81ea\u52a8\u7684\u521b\u5efa\u8f93\u51fa\u76ee\u5f55\n    if not os.path.exists(params.save_model_path):\n        os.mkdir(params.save_model_path)\n\n    # \u6839\u636e\u53c2\u6570\u51b3\u5b9a\u6a21\u578b\u7684\u521b\u5efa\u65b9\u5f0f\n    if params.pretrained_model:  # \u5982\u679c\u63d0\u4f9b\u4e86\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u8def\u5f84\n        # \u52a0\u8f7d\u9884\u8bad\u7ec3\u6a21\u578b\n        model = GPT2LMHeadModel.from_pretrained(params.pretrained_model)\n    else:  # \u5982\u679c\u6ca1\u6709\u63d0\u4f9b\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u8def\u5f84\n        # \u4eceJSON\u6587\u4ef6\u4e2d\u52a0\u8f7d\u6a21\u578b\u914d\u7f6e\n        model_config = GPT2Config.from_json_file(params.config_json)\n        # print(model_config) \u7528\u4e8e\u8c03\u8bd5\uff0c\u67e5\u770b\u6a21\u578b\u914d\u7f6e\n        # \u6839\u636e\u914d\u7f6e\u521d\u59cb\u5316\u6a21\u578b\n        model = GPT2LMHeadModel(config=model_config)\n    # print(f'model--&gt;{model}') \u7528\u4e8e\u8c03\u8bd5\uff0c\u67e5\u770b\u6a21\u578b\u7ed3\u6784\n    # \u5c06\u6a21\u578b\u79fb\u52a8\u5230\u6307\u5b9a\u7684\u8bbe\u5907\u4e0a\uff08\u5982GPU\uff09\n    model = model.to(params.device)\n    # print(f'model.config.vocab_size--&gt;{model.config.vocab_size}')\n    # print(f'tokenizer.vocab_size--&gt;{tokenizer.vocab_size}')\n    # \u786e\u8ba4\u6a21\u578b\u7684\u8bcd\u6c47\u8868\u5927\u5c0f\u4e0e\u5206\u8bcd\u5668\u7684\u8bcd\u6c47\u8868\u5927\u5c0f\u4e00\u81f4\n    assert model.config.vocab_size == tokenizer.vocab_size\n\n    # \u521d\u59cb\u5316\u6a21\u578b\u53c2\u6570\u6570\u91cf\u4e3a0\n    num_parameters = 0\n    # \u83b7\u53d6\u6a21\u578b\u7684\u6240\u6709\u53c2\u6570\n    parameters = model.parameters()\n    # \u904d\u5386\u6bcf\u4e00\u9879\u53c2\u6570\n    for parameter in parameters:\n        # \u5c06\u6bcf\u4e00\u9879\u53c2\u6570\u7684\u5143\u7d20\u6570\u91cf\u7d2f\u52a0\u5230\u603b\u53c2\u6570\u91cf\u4e2d\n        num_parameters += parameter.numel()\n    # \u6253\u5370\u6a21\u578b\u7684\u603b\u53c2\u6570\u91cf\n    print(f'\u6a21\u578b\u53c2\u6570\u603b\u91cf---\u300b{num_parameters}')\n\n    # \u52a0\u8f7d\u8bad\u7ec3\u96c6\u548c\u9a8c\u8bc1\u96c6\n    train_dataloader, validate_dataloader = get_dataloader(params.train_path, params.valid_path)\n    print(f'train_dataloader--&gt;{len(train_dataloader)}')\n    print(f'validate_dataloader--&gt;{len(validate_dataloader)}')\n    train(model, train_dataloader, validate_dataloader, params)\n\n\nif __name__ == '__main__':\n    run()\n</code></pre> <ul> <li>functions_tools.py\u4ee3\u7801\u89e3\u6790</li> </ul> <pre><code>import torch\nimport torch.nn.functional as F\n\ndef calculate_acc(logit, labels, ignore_index=-100):\n    '''\n    \u8ba1\u7b97\u6a21\u578b\u9884\u6d4b\u7684\u51c6\u786etoken\u548c\u603btoken\u3002\n    :param logit: \u6a21\u578b\u7684\u9884\u6d4b\u8f93\u51fa\n    :param labels: \u6570\u636e\u7684\u771f\u5b9e\u6807\u7b7e\n    :param ignore_index: \u6307\u5b9a\u4e00\u4e2a\u7d22\u5f15\uff0c\u8be5\u7d22\u5f15\u5bf9\u5e94\u7684\u6807\u7b7e\u5c06\u88ab\u5ffd\u7565\uff0c\u4e0d\u4f1a\u53c2\u4e0e\u51c6\u786e\u7387\u7684\u8ba1\u7b97\u3002\u9ed8\u8ba4\u503c\u4e3a-100\u3002\n    :return: n_correct: \u9884\u6d4b\u6b63\u786e\u7684token\u4e2a\u6570\u3002\n    n_word: \u975e\u586b\u5145\u7684\u603btoken\u4e2a\u6570\u3002\n    '''\n    # 1.\u5904\u7406\u8f93\u5165\u6570\u636e\uff0c\u53bb\u6389\u9884\u6d4b\u7ed3\u679c\u7684\u6700\u540e\u4e00\u4e2a\u5b57\u7b26\uff0c\u53bb\u6389\u6807\u7b7e\u7684\u7b2c\u4e00\u4e2a\u5b57\u7b26\n    # print(f'logit---&gt;\u539f\u59cb\u503c\u7684\u5f62\u72b6{logit.shape}')\n    # print(f'labels---&gt;\u539f\u59cb\u503c\u7684\u5f62\u72b6{labels.shape}')\n    # print(f'logit.size--&gt;{logit.size(-1)}')\n    # print(f'logit[:, :-1, :]--&gt;{logit[:, :-1, :].shape}')\n    # :-1-&gt;\u53bb\u6389\u9884\u6d4b\u7684\u7ed3\u675f\u7b26\n    logit = logit[:, :-1, :].contiguous().view(-1, logit.size(-1))\n    # print(f'logit\u6539\u53d8\u5b8c\u5f62\u72b6\u7684---&gt;{logit.shape}')\n    # print(f'labels[:, 1:]---&gt;{labels[:, 1:].shape}')\n    # 1:-&gt;\u53bb\u6389\u6807\u7b7e\u7684\u5f00\u59cb\u7b26, \u5b9e\u73b0\u9519\u4e00\u4f4d, logit\u548clabels\u5c31\u80fd\u5bf9\u5e94\n    labels = labels[:, 1:].contiguous().view(-1)\n    # print(f'labels\u6539\u53d8\u5b8c\u5f62\u72b6\u7684---&gt;{labels.shape}')\n\n    # 2.\u53d6\u51fa\u6700\u5927\u6982\u7387\u503c\u4ee5\u53ca\u5bf9\u5e94\u7d22\u5f15\n    max_index = logit.argmax(dim=-1)\n    # print(f'logit\u53d6\u51fa\u6a21\u578b\u9884\u6d4b\u6700\u5927\u7d22\u5f15\u503c--&gt;{max_index}')\n    # print(f'max_index.shape--&gt;{max_index.shape}')\n\n    # 3.\u8ba1\u7b97\u9884\u6d4b\u6b63\u786e\u7684token\u4e2a\u6570\u548c\u603b\u7684token\u4e2a\u6570\n    '''\n    \u5728 PyTorch \u4e2d\uff0clabels.ne(ignore_index) \u8868\u793a\u5c06\u6807\u7b7e\u5f20\u91cf labels \u4e2d\u7684\u503c\u4e0d\u7b49\u4e8e ignore_index \u7684\u4f4d\u7f6e\u6807\u8bb0\u4e3a True\uff0c\u7b49\u4e8e ignore_index \u7684\u4f4d\u7f6e\u6807\u8bb0\u4e3a False\u3002\n    \u8fd9\u4e2a\u64cd\u4f5c\uff0c\u4ee5\u8fc7\u6ee4\u6389 ignore_index \u5bf9\u635f\u5931\u7684\u5f71\u54cd\n    '''\n    # \u8fdb\u884c\u975e\u8fd0\u7b97\uff0c\u8fd4\u56de\u4e00\u4e2atensor\uff0c\u82e5labels\u7684\u7b2ci\u4e2a\u4f4d\u7f6e\u4e3apad_id\uff0c\u5219\u7f6e\u4e3a0\uff0c\u5426\u5219\u4e3a1\n    non_pad_mask = labels.ne(ignore_index)\n    # print(f'non_pad_mask--&gt;{non_pad_mask}')\n    '''\n    \u5728 PyTorch \u4e2d\uff0clogit.eq(labels) \u8868\u793a\u5c06\u6a21\u578b\u7684\u9884\u6d4b\u8f93\u51fa\u503c max_index \u4e2d\u7b49\u4e8e\u6807\u7b7e\u5f20\u91cf labels \u7684\u4f4d\u7f6e\u6807\u8bb0\u4e3a True\uff0c\n    \u4e0d\u7b49\u4e8e\u6807\u7b7e\u5f20\u91cf labels \u7684\u4f4d\u7f6e\u6807\u8bb0\u4e3a False\u3002\u4ee5\u6807\u8bb0\u51fa\u9884\u6d4b\u8f93\u51fa\u503c\u548c\u6807\u7b7e\u503c\u76f8\u7b49\u7684\u4f4d\u7f6e\u3002\n    masked_select(non_pad_mask) \u8868\u793a\u5c06\u5f20\u91cf\u4e2d\u975e\u586b\u5145\u6807\u8bb0\u7684\u4f4d\u7f6e\u9009\u51fa\u6765\u3002\n    '''\n    # print(f'max_index.eq(labels)---&gt;{max_index.eq(labels).shape}')\n    # print(f'max_index.eq(labels)---&gt;{ max_index.eq(labels)}')\n    # \u8ba1\u7b97\u9884\u6d4b\u6b63\u786e\u7684\u5355\u8bcd\u6570\u91cf\n    n_correct = max_index.eq(labels).masked_select(non_pad_mask).sum().item()\n    # print(f'n_correct--&gt;{n_correct}')\n\n    # \u8ba1\u7b97\u975e\u586b\u5145\u5355\u8bcd\u7684\u603b\u6570\n    n_word = non_pad_mask.sum().item()\n    # print(f'non_pad_mask.sum()--&gt;{non_pad_mask.sum()}')\n\n    return n_correct, n_word\n</code></pre>"},{"location":"02-%E5%9F%BA%E4%BA%8EGPT2%E7%9A%84%E5%8C%BB%E7%96%97%E9%97%AE%E8%AF%8A%E6%9C%BA%E5%99%A8%E4%BA%BA/01-%E5%8C%BB%E7%96%97%E9%97%AE%E8%AF%8A%E6%9C%BA%E5%99%A8%E4%BA%BA%E5%AE%9E%E7%8E%B0.html#5","title":"5. \u6a21\u578b\u9884\u6d4b\uff08\u4eba\u673a\u4ea4\u4e92\uff09","text":"<ul> <li>\u4f7f\u7528\u8bad\u7ec3\u597d\u7684\u6a21\u578b\uff0c\u8fdb\u884c\u4eba\u673a\u4ea4\u4e92\uff0c\u8f93\u5165Ctrl+Z\u7ed3\u675f\u5bf9\u8bdd\u4e4b\u540e\uff0c\u804a\u5929\u8bb0\u5f55\u5c06\u4fdd\u5b58\u5230sample\u76ee\u5f55\u4e0b\u7684sample.txt\u6587\u4ef6\u4e2d\u3002</li> </ul> <p>\u4ee3\u7801\u4f4d\u7f6e\uff1allm_tuning/Gpt2_Chatbot/interact.py</p> <pre><code>import os\nfrom datetime import datetime\n\nimport torch\nfrom transformers import GPT2LMHeadModel, BertTokenizerFast\nimport torch.nn.functional as F\n\nfrom Gpt2_Chatbot.parameter_config import ParameterConfig\n\n\ndef top_k_top_p_filtering(logits, top_k=0, top_p=0.0, filter_value=-float('Inf')):\n    '''\n    \u4f7f\u7528top-k\u548c/\u6216nucleus\uff08top-p\uff09\u7b5b\u9009\u6765\u8fc7\u6ee4logits\u7684\u5206\u5e03\n    :param logits: \u6700\u540e\u4e00\u4e2atoken\u7684logits\u7684\u5206\u5e03\uff0c\u5f62\u72b6\u4e3a\uff08\u8bcd\u6c47\u5927\u5c0f\uff09\n    :param top_k: top_k &gt; 0: \u4fdd\u7559\u6982\u7387\u6700\u9ad8\u7684top k\u4e2atoken\uff08top-k\u7b5b\u9009\uff09\n    :param top_p: top_p &gt; 0.0: \u4fdd\u7559\u7d2f\u79ef\u6982\u7387\u5927\u4e8e\u7b49\u4e8etop_p\u7684top token\uff08nucleus\u7b5b\u9009\uff09\n    :param filter_value: \u6781\u5c0f\u503c\n    :return: logits: \u8fc7\u6ee4\u540e\u7684logits\u5206\u5e03\uff0c\u5176\u4e2d\u4f4e\u6982\u7387\u6807\u8bb0\u88ab\u8bbe\u7f6e\u4e3afilter_value\u3002\n    '''\n    # \u786e\u4fddlogits\u7684\u7ef4\u5ea6\u4e3a1\uff0c\u8fd9\u91cc\u53ea\u5904\u7406\u6279\u91cf\u5927\u5c0f\u4e3a1\u7684\u60c5\u51b5\u3002\n    assert logits.dim() == 1\n    # \u5bf9top_k\u503c\u8fdb\u884c\u5b89\u5168\u6027\u68c0\u67e5\uff0c\u9632\u6b62\u5b83\u8d85\u8fc7logits\u6700\u540e\u4e00\u4e2a\u7ef4\u5ea6\u7684\u5927\u5c0f\uff0c\u907f\u514d\u8fd0\u884c\u65f6\u9519\u8bef\u3002\n    top_k = min(top_k, logits.size(-1))\n\n    if top_k &gt; 0:\n        # \u79fb\u9664\u6982\u7387\u5c0f\u4e8etop_k\u6807\u8bb0\n        # torch.topk()\u8fd4\u56de\u6700\u540e\u4e00\u7ef4\u4e2d\u6700\u5927\u7684top_k\u4e2a\u5143\u7d20\uff0c\u8fd4\u56de\u503c\u4e3a\u4e8c\u7ef4(values, indices)\n        # print(f'torch.topk(logits, top_k)---&gt;{torch.topk(logits, top_k)}')\n        # print(f'torch.topk(logits, top_k)[0]--&gt;{torch.topk(logits, top_k)[0]}')\n        # print(f'torch.topk(logits, top_k)[0][-1]--&gt;{torch.topk(logits, top_k)[0][-1]}')\n        # \u5224\u65adlogits\u7684\u503c\uff0c\u5982\u679c\u5c0f\u4e8etop_k\u6807\u8bb0\uff0c\u5219\u8bbe\u7f6e\u4e3afilter_value\n        indices_to_remove = logits &lt; torch.topk(logits, top_k)[0][-1]\n        # print(f'indices_to_remove---&gt;{indices_to_remove}')\n        # print(f'logits---&gt;{logits}')\n        logits[indices_to_remove] = filter_value  # \u5bf9\u4e8etopk\u4e4b\u5916\u7684\u5176\u4ed6\u5143\u7d20\u7684logits\u503c\u8bbe\u4e3a\u8d1f\u65e0\u7a77\n        # print(f'logits---&gt;{logits}')\n\n    if top_p &gt; 0.0:\n        sorted_logits, sorted_indices = torch.sort(logits, descending=True)  # \u5bf9logits\u8fdb\u884c\u9012\u51cf\u6392\u5e8f\n        # print(f'sorted_logits--&gt;{sorted_logits}')\n        # print(f'sorted_indices--&gt;{sorted_indices}')\n\n        # F.softmax(sorted_logits, dim=-1)\uff1a\u5c06\u6392\u5e8f\u540e\u7684 logits \u8f6c\u4e3a\u6982\u7387\u5206\u5e03\n        # torch.cumsum(..., dim=-1)\uff1a\u8ba1\u7b97\u7d2f\u79ef\u6982\u7387\n        cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n\n        # \u5bf9\u5e94\u4f4d\u7f6e\u4e3a True \u7684 token \u662f \u7d2f\u79ef\u6982\u7387\u8d85\u8fc7 top_p \u9608\u503c\u7684 token\uff0c\u901a\u5e38\u9700\u8981\u88ab\u79fb\u9664\n        sorted_indices_to_remove = cumulative_probs &gt; top_p\n        # \u5c06\u7d22\u5f15\u5411\u53f3\u79fb\u52a8\uff0c\u4ee5\u786e\u4fdd\u5373\u4f7f\u7b2c\u4e00\u4e2atoken\u8d85\u8fc7\u9608\u503c\u4e5f\u80fd\u4fdd\u7559\n        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()  # \u5c06\u7d22\u5f15\u5411\u53f3\u79fb\u52a8\u4e00\u4f4d\n        sorted_indices_to_remove[..., 0] = 0  # \u5c06\u7b2c\u4e00\u4e2atoken\u8bbe\u4e3aFalse\uff0c\u786e\u4fdd\u7b2c\u4e00\u4e2a token \u88ab\u4fdd\u7559\n        # print(f'sorted_indices_to_remove---&gt;{sorted_indices_to_remove}')\n\n        # \u5c06\u9700\u8981\u79fb\u9664\u7684token\u8bbe\u7f6e\u4e3afilter_value\u3002\n        indices_to_remove = sorted_indices[sorted_indices_to_remove]\n        logits[indices_to_remove] = filter_value\n        # print(f'logits---&gt;{logits}')\n    return logits\n\n\ndef main():\n    # \u521d\u59cb\u5316\u53c2\u6570\u914d\u7f6e\u5bf9\u8c61\n    pconf = ParameterConfig()\n\n    # \u6839\u636eCUDA\u7684\u53ef\u7528\u6027\u9009\u62e9\u4f7f\u7528GPU\u8fd8\u662fCPU\n    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n    # \u6253\u5370\u5f53\u524d\u4f7f\u7528\u7684\u8bbe\u5907\n    print('using device:{}'.format(device))\n    # \u8bbe\u7f6e\u73af\u5883\u53d8\u91cf\u4ee5\u4f7f\u7528\u7b2c\u4e00\u4e2aGPU\u8bbe\u5907\n    os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\n\n    # \u521d\u59cb\u5316BERT tokenizer\uff0c\u6307\u5b9a\u8bcd\u6c47\u8868\u8def\u5f84\u548c\u7279\u6b8a\u4ee4\u724c\n    tokenizer = BertTokenizerFast(vocab_file=pconf.vocab_path,\n                                  sep_token=\"[SEP]\",\n                                  pad_token=\"[PAD]\",\n                                  cls_token=\"[CLS]\")\n\n    # \u4ece\u9884\u8bad\u7ec3\u8def\u5f84\u52a0\u8f7dGPT-2\u6a21\u578b\n    model_path = os.path.join(pconf.save_model_path, 'min_loss_model_bj')\n    model = GPT2LMHeadModel.from_pretrained(model_path).to(device)\n    # \u5c06\u6a21\u578b\u8bbe\u7f6e\u4e3a\u8bc4\u4f30\u6a21\u5f0f\n    model.eval()\n\n    # \u5982\u679c\u914d\u7f6e\u4e86\u4fdd\u5b58\u804a\u5929\u8bb0\u5f55\u7684\u6587\u4ef6\u8def\u5f84\uff0c\u5219\u521b\u5efa\u6587\u4ef6\u5939\u548c\u6587\u4ef6\u4ee5\u4fdd\u5b58\u804a\u5929\u8bb0\u5f55\n    if pconf.save_samples_path:\n        # \u68c0\u67e5\u8def\u5f84\u662f\u5426\u5b58\u5728\uff0c\u5982\u679c\u4e0d\u5b58\u5728\u5219\u521b\u5efa\n        if not os.path.exists(pconf.save_samples_path):\n            os.makedirs(pconf.save_samples_path)\n        # \u6253\u5f00\u6216\u521b\u5efa\u4e00\u4e2a\u540d\u4e3a'samples.txt'\u7684\u6587\u4ef6\uff0c\u7528\u4e8e\u8ffd\u52a0\u804a\u5929\u8bb0\u5f55\n        file_path = os.path.join(pconf.save_samples_path, 'samples.txt')\n        samples_file = open(file_path, 'a', encoding='utf8')\n        # \u5728\u6587\u4ef6\u4e2d\u5199\u5165\u5f53\u524d\u804a\u5929\u8bb0\u5f55\u7684\u65f6\u95f4\u6233\n        samples_file.write(\"\u804a\u5929\u8bb0\u5f55{}:\\n\".format(datetime.now()))\n\n    # \u521d\u59cb\u5316\u4e00\u4e2a\u7a7a\u5217\u8868\u7528\u4e8e\u5b58\u50a8\u804a\u5929\u8bb0\u5f55\uff0c\u804a\u5929\u8bb0\u5f55\u4ee5token id\u7684\u5f62\u5f0f\u5b58\u50a8\n    history = []\n\n    # \u6253\u5370\u6b22\u8fce\u8bed\uff0c\u4ecb\u7ecd\u804a\u5929\u673a\u5668\u4eba\u7684\u8eab\u4efd\n    print('\u4f60\u597d\uff0c\u6211\u662f\u60a8\u7684\u5065\u5eb7\u52a9\u624b')\n\n    while True:\n        try:\n            # \u63d0\u793a\u7528\u6237\u8f93\u5165\u6587\u672c\u5e76\u5b58\u50a8\n            text = input(\"user:\")\n            # print(f'text---&gt;{text}')\n            # \u5982\u679c\u914d\u7f6e\u4e86\u4fdd\u5b58\u6837\u672c\u7684\u8def\u5f84\uff0c\u5219\u5c06\u7528\u6237\u8f93\u5165\u7684\u6587\u672c\u5199\u5165\u6837\u672c\u6587\u4ef6\n            if pconf.save_samples_path:\n                samples_file.write(\"user:{}\\n\".format(text))\n\n            # \u4f7f\u7528tokenizer\u5c06\u7528\u6237\u8f93\u5165\u7684\u6587\u672c\u8f6c\u6362\u4e3a\u6587\u672cID\u5e8f\u5217\uff0c\u4e0d\u6dfb\u52a0\u7279\u6b8a\u4ee4\u724c\n            text_ids = tokenizer.encode(\"\u7528\u6237\u95ee\u9898\uff1a\" + text, add_special_tokens=False)\n            print(f'text_ids--&gt;{text_ids}')\n            # \u5c06\u6587\u672cID\u5e8f\u5217\u6dfb\u52a0\u5230\u5386\u53f2\u5bf9\u8bdd\u5217\u8868\u4e2d\n            history.append(text_ids)\n            print(f'history--&gt;{history}')\n\n            # \u521d\u59cb\u5316\u8f93\u5165ID\u5e8f\u5217\uff0c\u6bcf\u4e2a\u8f93\u5165\u4ee5[CLS]\u4ee4\u724c\u5f00\u59cb\n            input_ids = [tokenizer.cls_token_id]\n            print(f'input_ids--&gt;{input_ids}')\n            print('*' * 80)\n\n            # pconf.max_history_len\u76ee\u7684\uff1a\u5c06\u6700\u8fd1\u7684\u5386\u53f2\u6d88\u606f\u8bb0\u5f55\u9001\u5230\u6a21\u578b\n            # \u904d\u5386\u5386\u53f2\u8bb0\u5f55\uff0chistory_id\u7528\u4e8e\u8bb0\u5f55\u7d22\u5f15\uff0chistory_utr\u7528\u4e8e\u8bb0\u5f55\u6bcf\u4e2a\u5386\u53f2\u5bf9\u8bdd\u7684token ID\u5217\u8868\n            # eg\uff1ahistory =  [[872, 1962], [872, 1962], [872, 342, 123], [334, 55,234]]--&gt;history[-3:]\n            for history_id, history_utr in enumerate(history[-pconf.max_history_len:]):\n                print(f'history_utr---&gt;{history_utr}')\n                # \u5c06\u5386\u53f2\u5bf9\u8bdd\u7684token ID\u5217\u8868\u6dfb\u52a0\u5230input_ids\u4e2d\n                input_ids.extend(history_utr)\n                # \u5728\u6bcf\u4e2a\u5386\u53f2\u5bf9\u8bdd\u7684token ID\u5217\u8868\u4e4b\u540e\u6dfb\u52a0\u5206\u9694\u7b26token ID\n                input_ids.append(tokenizer.sep_token_id)\n                print(f'input_ids---&gt;{input_ids}')\n            print(f'\u5b58\u50a8\u4e86\u5386\u53f2\u5bf9\u8bdd\u7684\u8f93\u5165\u4fe1\u606f--&gt;{input_ids}')\n\n            input_ids = torch.tensor(input_ids, dtype=torch.long, device=device)\n            # \u5c06input_ids\u6269\u5c55\u4e3a\u4e8c\u7ef4\u5f20\u91cf\uff0c\u5f62\u72b6\u4e3a\uff081\uff0cinput_ids\u957f\u5ea6\uff09\n            input_ids = input_ids.unsqueeze(0)\n            print(f'\u7b26\u5408\u6a21\u578b\u7684\u8f93\u5165--&gt;{input_ids.shape}')\n\n            # \u521d\u59cb\u5316\u4e00\u4e2a\u7a7a\u5217\u8868response\uff0c\u7528\u4e8e\u5b58\u50a8\u751f\u6210\u7684\u54cd\u5e94\n            response = []  # \u6839\u636econtext\uff0c\u751f\u6210\u7684response\n            # \u5faa\u73af\u751f\u6210\u54cd\u5e94\uff0c\u6b21\u6570\u7531pconf.max_len\u51b3\u5b9a\n            for _ in range(pconf.max_len):\n                # \u4f7f\u7528\u6a21\u578b\u751f\u6210\u8f93\u51fa\n                outputs = model(input_ids=input_ids)\n                # \u83b7\u53d6\u6a21\u578b\u8f93\u51fa\u7684logits\uff0c\u5373\u672a\u7ecf\u8fc7softmax\u7684\u9884\u6d4b\u503c\n                logits = outputs.logits\n                print(f'logits---&gt;{logits.shape}')\n                # \u4ecelogits\u4e2d\u63d0\u53d6\u4e0b\u4e00\u4e2atoken\u7684\u6982\u7387\u503c\n                next_token_logits = logits[0, -1, :]\n                print(f'next_token_logits----&gt;{next_token_logits.shape}')\n\n                # \u5bf9\u4e8e\u5df2\u751f\u6210\u7684\u7ed3\u679cgenerated\u4e2d\u7684\u6bcf\u4e2atoken\u6dfb\u52a0\u4e00\u4e2a\u91cd\u590d\u60e9\u7f5a\u9879, \u964d\u4f4e\u5176\u751f\u6210\u6982\u7387(\u89e3\u51b3\u590d\u8bfb\u673a\u95ee\u9898)\n                # print(f'set(response)--&gt;{set(response)}')\n                # \u904d\u5386\u54cd\u5e94\u4e2d\u7684\u6bcf\u4e2a\u6807\u8bc6\u7b26\uff0c\u4ee5\u5e94\u7528\u91cd\u590d\u60e9\u7f5a\n                for id in set(response):\n                    # print(f'id---&gt;{id}')\n                    # \u6839\u636e\u91cd\u590d\u60e9\u7f5a\u914d\u7f6e\uff0c\u8c03\u6574\u6807\u8bc6\u7b26\u7684\u4e0b\u4e00\u6b21\u51fa\u73b0\u7684logits\u503c\n                    if next_token_logits[id] &lt; 0:\n                        next_token_logits[id] *= pconf.repetition_penalty\n                    else:\n                        next_token_logits[id] /= pconf.repetition_penalty\n\n                # \u5bf9\u4e8e[UNK]\u7684\u6982\u7387\u8bbe\u4e3a\u65e0\u7a77\u5c0f\uff0c\u4e5f\u5c31\u662f\u8bf4\u6a21\u578b\u7684\u9884\u6d4b\u7ed3\u679c\u4e0d\u53ef\u80fd\u662f[UNK]\u8fd9\u4e2atoken\n                next_token_logits[tokenizer.convert_tokens_to_ids('[UNK]')] = -float('Inf')\n\n                # \u8fd9\u91cc\u4f7f\u7528\u4e86 top_k_top_p_filtering \u51fd\u6570\uff0c\u8be5\u51fd\u6570\u7684\u4f5c\u7528\u662f\u6839\u636e\u7ed9\u5b9a\u7684 top_k \u53c2\u6570\u548c top_p \u53c2\u6570\uff0c\u5c06\u4e0d\u7b26\u5408\u7684token\u6982\u7387\u88ab\u8bbe\u7f6e\u4e3a\u65e0\u7a77\u5c0f\uff0c\u4ece\u800c\u9009\u62e9\u51fa\u7b26\u5408 k \u4e2a\u5143\u7d20\u7684\u4e0b\u6807\u548c\u5bf9\u5e94\u7684\u503c\n                # \u8fd9\u79cd\u65b9\u6cd5\u53ef\u4ee5\u5728\u4fdd\u8bc1\u751f\u6210\u6587\u672c\u7684\u8d28\u91cf\u7684\u540c\u65f6\uff0c\u5177\u6709\u4e00\u5b9a\u7684\u968f\u673a\u6027\n                filtered_logits = top_k_top_p_filtering(next_token_logits, top_k=pconf.topk, top_p=pconf.topp)\n\n                # torch.multinomial\u8868\u793a\u4ece\u5019\u9009\u96c6\u5408\u4e2d\u65e0\u653e\u56de\u5730\u8fdb\u884c\u62bd\u53d6num_samples\u4e2a\u5143\u7d20\uff0c\u6743\u91cd\u8d8a\u9ad8\uff0c\u62bd\u5230\u7684\u51e0\u7387\u8d8a\u9ad8\uff0c\u8fd4\u56de\u5143\u7d20\u7684\u4e0b\u6807\n                next_token = torch.multinomial(F.softmax(filtered_logits, dim=-1), num_samples=1)\n                # print(f'next_token--&gt;{next_token}')\n\n                # \u9047\u5230[SEP]\u5219\u8868\u660eresponse\u751f\u6210\u7ed3\u675f\n                if next_token.item() == tokenizer.sep_token_id:\n                    break\n                # \u5c06\u9884\u6d4b\u7684\u4e0b\u4e00\u4e2atoken\u7684ID\u6dfb\u52a0\u5230\u54cd\u5e94\u5e8f\u5217\u4e2d\n                response.append(next_token.item())\n                print(f'response--&gt;{response}')\n\n\n                # \u5728\u8f93\u5165ID\u5e8f\u5217\u4e2d\u6dfb\u52a0\u4e0b\u4e00\u4e2atoken\u7684ID\uff0c\u4ee5\u4fbf\u5c06\u5176\u4f5c\u4e3a\u4e0b\u4e00\u4e2a\u9884\u6d4b\u6b65\u9aa4\u7684\u8f93\u5165\n                input_ids = torch.cat((input_ids, next_token.unsqueeze(0)), dim=1)\n\n            model_ids = tokenizer.encode(\"\u6a21\u578b\u56de\u7b54\uff1a\", add_special_tokens=False)\n            # print(f'model_ids + response---&gt;{model_ids + response}')\n            # \u5c06\u673a\u5668\u4eba\u7684\u54cd\u5e94\u6dfb\u52a0\u5230\u5bf9\u8bdd\u5386\u53f2\u4e2d\n            history.append(model_ids + response)\n\n            # \u5c06\u54cd\u5e94\u4eceID\u5e8f\u5217\u8f6c\u6362\u4e3a\u6587\u672c\n            text = tokenizer.convert_ids_to_tokens(response)\n            # \u6253\u5370\u673a\u5668\u4eba\u7684\u56de\u590d\n            print(\"chatbot:\" + \"\".join(text))\n        except KeyboardInterrupt:\n            break\n\n\nif __name__ == '__main__':\n    main()\n</code></pre>"},{"location":"02-%E5%9F%BA%E4%BA%8EGPT2%E7%9A%84%E5%8C%BB%E7%96%97%E9%97%AE%E8%AF%8A%E6%9C%BA%E5%99%A8%E4%BA%BA/01-%E5%8C%BB%E7%96%97%E9%97%AE%E8%AF%8A%E6%9C%BA%E5%99%A8%E4%BA%BA%E5%AE%9E%E7%8E%B0.html#6-flaskweb","title":"6. \u57fa\u4e8eFlask\u6846\u67b6web\u5f00\u53d1(\u4e86\u89e3)","text":"<ul> <li>\u5bf9interact.py\u8fdb\u884c\u8c03\u6574, \u53bb\u9664while\u65e0\u9650\u5faa\u73af\uff0c\u7531\u524d\u7aef\u4fdd\u5b58history\uff0c\u53ea\u9700\u8981\u5bf9\u4f20\u5165\u7684\u53e5\u5b50\u8fdb\u884c\u9884\u6d4b\u5373\u53ef\u3002</li> </ul> <p>\u4ee3\u7801\u4f4d\u7f6e\uff1allm_tuning/Gpt2_Chatbot/flask_predict.py</p> <pre><code>import os\nimport torch\nfrom transformers import GPT2LMHeadModel, BertTokenizerFast\nimport torch.nn.functional as F\n\nfrom Gpt2_Chatbot.interact import top_k_top_p_filtering\nfrom Gpt2_Chatbot.parameter_config import ParameterConfig\n\n# \u521d\u59cb\u5316\u53c2\u6570\u914d\u7f6e\u5bf9\u8c61\npconf = ParameterConfig()\n# \u6839\u636eCUDA\u7684\u53ef\u7528\u6027\u9009\u62e9\u4f7f\u7528GPU\u8fd8\u662fCPU\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\n# \u6253\u5370\u5f53\u524d\u4f7f\u7528\u7684\u8bbe\u5907\n# print('using device:{}'.format(device))\n# \u8bbe\u7f6e\u73af\u5883\u53d8\u91cf\u4ee5\u4f7f\u7528\u7b2c\u4e00\u4e2aGPU\u8bbe\u5907\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\n\n# \u521d\u59cb\u5316BERT tokenizer\uff0c\u6307\u5b9a\u8bcd\u6c47\u8868\u8def\u5f84\u548c\u7279\u6b8a\u4ee4\u724c\ntokenizer = BertTokenizerFast(vocab_file=pconf.vocab_path,\n                              sep_token=\"[SEP]\",\n                              pad_token=\"[PAD]\",\n                              cls_token=\"[CLS]\")\n\n# \u4ece\u9884\u8bad\u7ec3\u8def\u5f84\u52a0\u8f7dGPT-2\u6a21\u578b\nmodel_path = os.path.join(pconf.save_model_path, 'min_loss_model_bj')\nmodel = GPT2LMHeadModel.from_pretrained(model_path).to(device)\n# \u5c06\u6a21\u578b\u8bbe\u7f6e\u4e3a\u8bc4\u4f30\u6a21\u5f0f\nmodel.eval()\n\n\n# \u6a21\u578b\u9884\u6d4b\u51fd\u6570\uff0c\u8f93\u5165\u4e3a\u7528\u6237\u8f93\u5165\u7684\u6587\u672c\uff0c\u8fd4\u56de\u9884\u6d4b\u7ed3\u679c\ndef model_predict(text, history):\n    # \u4f7f\u7528tokenizer\u5c06\u7528\u6237\u8f93\u5165\u7684\u6587\u672c\u8f6c\u6362\u4e3a\u6587\u672cID\u5e8f\u5217\uff0c\u4e0d\u6dfb\u52a0\u7279\u6b8a\u4ee4\u724c\n    text_ids = tokenizer.encode(\"\u7528\u6237\u95ee\u9898\uff1a\" + text, add_special_tokens=False)\n    # print(f'text_ids--&gt;{text_ids}')\n    # \u5c06\u6587\u672cID\u5e8f\u5217\u6dfb\u52a0\u5230\u5386\u53f2\u5bf9\u8bdd\u5217\u8868\u4e2d\n    history.append(text_ids)\n    # print(f'history--&gt;{history}')\n\n    # \u521d\u59cb\u5316\u8f93\u5165ID\u5e8f\u5217\uff0c\u6bcf\u4e2a\u8f93\u5165\u4ee5[CLS]\u4ee4\u724c\u5f00\u59cb\n    input_ids = [tokenizer.cls_token_id]\n    # print(f'input_ids--&gt;{input_ids}')\n\n    # pconf.max_history_len\u76ee\u7684\uff1a\u5c06\u6700\u8fd1\u7684\u5386\u53f2\u6d88\u606f\u8bb0\u5f55\u9001\u5230\u6a21\u578b\n    # \u904d\u5386\u5386\u53f2\u8bb0\u5f55\uff0chistory_id\u7528\u4e8e\u8bb0\u5f55\u7d22\u5f15\uff0chistory_utr\u7528\u4e8e\u8bb0\u5f55\u6bcf\u4e2a\u5386\u53f2\u5bf9\u8bdd\u7684token ID\u5217\u8868\n    # eg\uff1ahistory =  [[872, 1962], [872, 1962], [872, 342, 123], [334, 55,234]]--&gt;history[-3:]\n    for history_id, history_utr in enumerate(history[-pconf.max_history_len:]):\n        # print(f'history_utr---&gt;{history_utr}')\n        # \u5c06\u5386\u53f2\u5bf9\u8bdd\u7684token ID\u5217\u8868\u6dfb\u52a0\u5230input_ids\u4e2d\n        input_ids.extend(history_utr)\n        # \u5728\u6bcf\u4e2a\u5386\u53f2\u5bf9\u8bdd\u7684token ID\u5217\u8868\u4e4b\u540e\u6dfb\u52a0\u5206\u9694\u7b26token ID\n        input_ids.append(tokenizer.sep_token_id)\n        # print(f'input_ids---&gt;{input_ids}')\n    # print(f'\u5b58\u50a8\u4e86\u5386\u53f2\u5bf9\u8bdd\u7684\u8f93\u5165\u4fe1\u606f--&gt;{input_ids}')\n\n    input_ids = torch.tensor(input_ids, dtype=torch.long, device=device)\n    # \u5c06input_ids\u6269\u5c55\u4e3a\u4e8c\u7ef4\u5f20\u91cf\uff0c\u5f62\u72b6\u4e3a\uff081\uff0cinput_ids\u957f\u5ea6\uff09\n    input_ids = input_ids.unsqueeze(0)\n    # print(f'\u7b26\u5408\u6a21\u578b\u7684\u8f93\u5165--&gt;{input_ids.shape}')\n\n    # \u521d\u59cb\u5316\u4e00\u4e2a\u7a7a\u5217\u8868response\uff0c\u7528\u4e8e\u5b58\u50a8\u751f\u6210\u7684\u54cd\u5e94\n    response = []  # \u6839\u636econtext\uff0c\u751f\u6210\u7684response\n    # \u5faa\u73af\u751f\u6210\u54cd\u5e94\uff0c\u6b21\u6570\u7531pconf.max_len\u51b3\u5b9a\n    for _ in range(pconf.max_len):\n        # \u4f7f\u7528\u6a21\u578b\u751f\u6210\u8f93\u51fa\n        outputs = model(input_ids=input_ids)\n        # \u83b7\u53d6\u6a21\u578b\u8f93\u51fa\u7684logits\uff0c\u5373\u672a\u7ecf\u8fc7softmax\u7684\u9884\u6d4b\u503c\n        logits = outputs.logits\n        # print(f'logits---&gt;{logits.shape}')\n        # \u4ecelogits\u4e2d\u63d0\u53d6\u4e0b\u4e00\u4e2atoken\u7684\u6982\u7387\u503c\n        next_token_logits = logits[0, -1, :]\n        # print(f'next_token_logits----&gt;{next_token_logits.shape}')\n\n        # \u5bf9\u4e8e\u5df2\u751f\u6210\u7684\u7ed3\u679cgenerated\u4e2d\u7684\u6bcf\u4e2atoken\u6dfb\u52a0\u4e00\u4e2a\u91cd\u590d\u60e9\u7f5a\u9879, \u964d\u4f4e\u5176\u751f\u6210\u6982\u7387(\u89e3\u51b3\u590d\u8bfb\u673a\u95ee\u9898)\n        # print(f'set(response)--&gt;{set(response)}')\n        # \u904d\u5386\u54cd\u5e94\u4e2d\u7684\u6bcf\u4e2a\u6807\u8bc6\u7b26\uff0c\u4ee5\u5e94\u7528\u91cd\u590d\u60e9\u7f5a\n        for id in set(response):\n            # print(f'id---&gt;{id}')\n            # \u6839\u636e\u91cd\u590d\u60e9\u7f5a\u914d\u7f6e\uff0c\u8c03\u6574\u6807\u8bc6\u7b26\u7684\u4e0b\u4e00\u6b21\u51fa\u73b0\u7684logits\u503c\n            if next_token_logits[id] &lt; 0:\n                next_token_logits[id] *= pconf.repetition_penalty\n            else:\n                next_token_logits[id] /= pconf.repetition_penalty\n\n        # \u5bf9\u4e8e[UNK]\u7684\u6982\u7387\u8bbe\u4e3a\u65e0\u7a77\u5c0f\uff0c\u4e5f\u5c31\u662f\u8bf4\u6a21\u578b\u7684\u9884\u6d4b\u7ed3\u679c\u4e0d\u53ef\u80fd\u662f[UNK]\u8fd9\u4e2atoken\n        next_token_logits[tokenizer.convert_tokens_to_ids('[UNK]')] = -float('Inf')\n\n        # \u8fd9\u91cc\u4f7f\u7528\u4e86 top_k_top_p_filtering \u51fd\u6570\uff0c\u8be5\u51fd\u6570\u7684\u4f5c\u7528\u662f\u6839\u636e\u7ed9\u5b9a\u7684 top_k \u53c2\u6570\u548c top_p \u53c2\u6570\uff0c\u5c06\u4e0d\u7b26\u5408\u7684token\u6982\u7387\u88ab\u8bbe\u7f6e\u4e3a\u65e0\u7a77\u5c0f\uff0c\u4ece\u800c\u9009\u62e9\u51fa\u7b26\u5408 k \u4e2a\u5143\u7d20\u7684\u4e0b\u6807\u548c\u5bf9\u5e94\u7684\u503c\n        # \u8fd9\u79cd\u65b9\u6cd5\u53ef\u4ee5\u5728\u4fdd\u8bc1\u751f\u6210\u6587\u672c\u7684\u8d28\u91cf\u7684\u540c\u65f6\uff0c\u5177\u6709\u4e00\u5b9a\u7684\u968f\u673a\u6027\n        filtered_logits = top_k_top_p_filtering(next_token_logits, top_k=pconf.topk, top_p=pconf.topp)\n\n        # torch.multinomial\u8868\u793a\u4ece\u5019\u9009\u96c6\u5408\u4e2d\u65e0\u653e\u56de\u5730\u8fdb\u884c\u62bd\u53d6num_samples\u4e2a\u5143\u7d20\uff0c\u6743\u91cd\u8d8a\u9ad8\uff0c\u62bd\u5230\u7684\u51e0\u7387\u8d8a\u9ad8\uff0c\u8fd4\u56de\u5143\u7d20\u7684\u4e0b\u6807\n        next_token = torch.multinomial(F.softmax(filtered_logits, dim=-1), num_samples=1)\n        # print(f'next_token--&gt;{next_token}')\n\n        # \u9047\u5230[SEP]\u5219\u8868\u660eresponse\u751f\u6210\u7ed3\u675f\n        if next_token.item() == tokenizer.sep_token_id:\n            break\n        # \u5c06\u9884\u6d4b\u7684\u4e0b\u4e00\u4e2atoken\u7684ID\u6dfb\u52a0\u5230\u54cd\u5e94\u5e8f\u5217\u4e2d\n        response.append(next_token.item())\n        # print(f'response--&gt;{response}')\n\n        # \u5728\u8f93\u5165ID\u5e8f\u5217\u4e2d\u6dfb\u52a0\u4e0b\u4e00\u4e2atoken\u7684ID\uff0c\u4ee5\u4fbf\u5c06\u5176\u4f5c\u4e3a\u4e0b\u4e00\u4e2a\u9884\u6d4b\u6b65\u9aa4\u7684\u8f93\u5165\n        input_ids = torch.cat((input_ids, next_token.unsqueeze(0)), dim=1)\n\n    model_ids = tokenizer.encode(\"\u6a21\u578b\u56de\u7b54\uff1a\", add_special_tokens=False)\n    # print(f'model_ids + response---&gt;{model_ids + response}')\n    # \u5c06\u673a\u5668\u4eba\u7684\u54cd\u5e94\u6dfb\u52a0\u5230\u5bf9\u8bdd\u5386\u53f2\u4e2d\n    history.append(model_ids + response)\n\n    # \u5c06\u54cd\u5e94\u4eceID\u5e8f\u5217\u8f6c\u6362\u4e3a\u6587\u672c\n    text = tokenizer.convert_ids_to_tokens(response)\n    # \u6253\u5370\u673a\u5668\u4eba\u7684\u56de\u590d\n    response = \"\".join(text)\n    return response, history\n\n\nif __name__ == '__main__':\n    history = []\n    response, history = model_predict('\u4f60\u597d', history)\n    print(f'response---&gt;{response}')\n    print(f'history---&gt;{history}')\n    response, history = model_predict('\u5375\u5de2\u764c\u8089\u7624\u7684\u5f71\u50cf\u5b66\u68c0\u67e5\u6709\u4e9b\u4ec0\u4e48', history)\n    print(f'response---&gt;{response}')\n    print(f'history---&gt;{history}')\n</code></pre> <ul> <li>\u57fa\u4e8eFlask\u6846\u67b6\u7684web\u540e\u7aef\u63a5\u53e3</li> </ul> <p>\u8fd9\u90e8\u5206\u53ef\u4ee5\u7528\u5927\u6a21\u578b\u751f\u6210\uff0c\u5199\u597d\u63d0\u793a\u8bcd\u5373\u53ef\u3002</p> <pre><code>\u4f7f\u7528\u5927\u6a21\u578b\u751f\u6210web\u524d\u540e\u7aef\u4ee3\u7801, \u63cf\u8ff0\u5982\u4e0b:\n\u73b0\u5728\u4f60\u662f\u4e00\u4e2a\u4ee3\u7801\u4e13\u5bb6, \u76ee\u524d\u5df2\u7ecf\u8bad\u7ec3\u597d\u4e86\u4e00\u4e2a\u6a21\u578b, \u5e76\u4e14\u5df2\u7ecf\u5c06\u8be5\u6a21\u578b\u8fdb\u884c\u4e86\u5c01\u88c5, \u51fd\u6570\u540d\u4e3amodel_predict(), \u8be5\u51fd\u6570\u4f20\u5165\u53c2\u6570\u4e3atext, history\uff0c\u5176\u4e2dtext\u4e3a\u7528\u6237\u95ee\u9898\uff0chistory\u4e3a\u4e00\u4e2a\u5217\u8868\uff0c\u7528\u6765\u4fdd\u5b58\u4e0a\u4e0b\u6587\u4fe1\u606f\uff1b\u8fd4\u56de\u503c\u4e3aresponse, history\uff0c\u5176\u4e2dresponse\u4e3a\u9884\u6d4b\u7ed3\u679c\uff0chistory\u4e3a\u5217\u8868\u3002\n\n\u73b0\u5728\u9700\u8981\u4f60\u57fa\u4e8eFlask\u6846\u67b6, \u5bf9\u8be5\u51fd\u6570\u8fdb\u884cAPI\u63a5\u53e3\u5c01\u88c5\u5236\u4f5c, \u5e76\u4e14\u5e0c\u671b\u80fd\u591f\u5236\u4f5c\u4e00\u4e2a\u7b80\u5355\u7684web\u754c\u9762, \u754c\u9762\u7684\u4e3b\u8981\u529f\u80fd\u5448\u73b0\u5982\u4e0b:\n1. \u7528\u6237\u8f93\u5165\u95ee\u9898\u548chistory\u5217\u8868, \u7136\u540e\u8fd4\u56de\u9884\u6d4b\u7ed3\u679c\u548chistory\u3002\u8fd9\u4e2ahistory\u4e0d\u9700\u8981\u524d\u7aef\u5f80\u91cc\u8fb9\u6dfb\u52a0\u5185\u5bb9\uff0c\u53ea\u9700\u8981\u4fdd\u5b58\u8fd9\u4e2a\u53d8\u91cf\u5373\u53ef\u3002\u5728\u524d\u7aef\u9875\u9762\u5237\u65b0\u65f6\uff0c\u8fd9\u4e2a\u53d8\u91cf\u6e05\u7a7a\u4e3a[]\u3002\n2. \u5c06\u7528\u6237\u548c\u6a21\u578b\u7684\u804a\u5929\u4fe1\u606f\u4fdd\u7559\u5728\u9875\u9762\u4e0a\u5c55\u793a\n3. \u9875\u9762\u6807\u9898\u540d\u79f0\u53eb\u505a:\u9ed1\u9a6c\u533b\u7597\u95ee\u8bca\u673a\u5668\u4eba\n4. \u9875\u9762\u6807\u9898\u548c\u8f93\u5165\u5bf9\u8bdd\u6846\u5e03\u5c40\u8981\u5bf9\u79f0, \u5e76\u4e14\u7528\u4e0d\u540c\u7684\u989c\u8272\u6e32\u67d3\n\u8bf7\u7ed9\u51fa\u8be6\u7ec6\u7684app.py\u548cindex.html\u7684\u4ee3\u7801\n</code></pre> <p>\u4ee3\u7801\u4f4d\u7f6e\uff1allm_tuning/Gpt2_Chatbot/app.py</p> <pre><code>from flask import Flask, request, jsonify, render_template\nfrom Gpt2_Chatbot.flask_predict import model_predict\n\napp = Flask(__name__)\n\n# API \u63a5\u53e3\n@app.route('/api/predict', methods=['POST'])\ndef predict():\n    data = request.json or {}\n    question = data.get('question', '')\n    print(f'Question--&gt;{question}')\n    # \u524d\u7aef\u4f20\u8fc7\u6765\u4e00\u4e2a history \u5217\u8868\uff08\u4f8b\u5982\uff1a[[872, 1962], [872, 1962, 8024, 6821...]]\uff09\n    history = data.get('history', [])\n    print(f'History--&gt;{history}')\n\n    # \u8c03\u7528\u6a21\u578b\u9884\u6d4b\uff0c\u5047\u8bbe model_predict \u8fd4\u56de (answer, new_history)\n    answer, new_history = model_predict(question, history=history)\n\n    # \u628a answer \u548c\u66f4\u65b0\u540e\u7684 history \u4e00\u5e76\u8fd4\u56de\uff08\u524d\u7aef\u4fdd\u5b58\u5e76\u66f4\u65b0history\uff0c\u4f01\u4e1a\u4e2d\u4e5f\u53ef\u4ee5\u5c06history\u5176\u4fdd\u5b58\u5230\u6570\u636e\u5e93\u6216\u6587\u4ef6\u4e2d\uff09\n    return jsonify({'question': question, 'answer': answer, 'history': new_history})\n\n# Web \u754c\u9762\n@app.route('/')\ndef index():\n    return render_template('index.html')\n\n\nif __name__ == '__main__':\n    app.run(debug=True)\n</code></pre> <ul> <li>web\u524d\u7aef\u4ee3\u7801</li> </ul> <p>\u4ee3\u7801\u4f4d\u7f6e\uff1allm_tuning/Gpt2_Chatbot/templates/index.html</p> <pre><code>&lt;!DOCTYPE html&gt;\n&lt;html lang=\"zh-CN\"&gt;\n&lt;head&gt;\n    &lt;meta charset=\"UTF-8\"&gt;\n    &lt;meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\"&gt;\n    &lt;title&gt;\u9ed1\u9a6c\u533b\u7597\u95ee\u8bca\u673a\u5668\u4eba&lt;/title&gt;\n    &lt;style&gt;\n        body {\n            font-family: 'Arial', sans-serif;\n            max-width: 800px;\n            margin: 0 auto;\n            padding: 20px;\n            background-color: #f5f5f5;\n        }\n        .header {\n            text-align: center;\n            margin-bottom: 30px;\n            padding: 20px;\n            background: linear-gradient(135deg, #4b6cb7, #182848);\n            color: white;\n            border-radius: 10px;\n            box-shadow: 0 4px 8px rgba(0, 0, 0, 0.1);\n        }\n        .chat-container {\n            background-color: white;\n            border-radius: 10px;\n            box-shadow: 0 4px 8px rgba(0, 0, 0, 0.1);\n            padding: 20px;\n            margin-bottom: 20px;\n            height: 400px;\n            overflow-y: auto;\n        }\n        .message {\n            margin-bottom: 15px;\n            padding: 10px 15px;\n            border-radius: 18px;\n            max-width: 70%;\n            word-wrap: break-word;\n        }\n        .user-message {\n            background-color: #e3f2fd;\n            margin-left: auto;\n            border-bottom-right-radius: 5px;\n        }\n        .bot-message {\n            background-color: #f1f1f1;\n            margin-right: auto;\n            border-bottom-left-radius: 5px;\n        }\n        .input-container {\n            display: flex;\n            gap: 10px;\n        }\n        #user-input {\n            flex: 1;\n            padding: 12px;\n            border: 1px solid #ddd;\n            border-radius: 20px;\n            font-size: 16px;\n        }\n        #send-button {\n            padding: 12px 20px;\n            background-color: #4b6cb7;\n            color: white;\n            border: none;\n            border-radius: 20px;\n            cursor: pointer;\n            font-size: 16px;\n            transition: background-color 0.3s;\n        }\n        #send-button:hover {\n            background-color: #3a56a1;\n        }\n        .timestamp {\n            font-size: 12px;\n            color: #777;\n            margin-top: 5px;\n            text-align: right;\n        }\n    &lt;/style&gt;\n&lt;/head&gt;\n&lt;body&gt;\n    &lt;div class=\"header\"&gt;\n        &lt;h1&gt;\u9ed1\u9a6c\u533b\u7597\u95ee\u8bca\u673a\u5668\u4eba&lt;/h1&gt;\n        &lt;p&gt;\u60a8\u7684\u667a\u80fd\u5065\u5eb7\u987e\u95ee&lt;/p&gt;\n    &lt;/div&gt;\n\n    &lt;div class=\"chat-container\" id=\"chat-box\"&gt;\n        &lt;!-- \u521d\u59cb\u7cfb\u7edf\u6d88\u606f --&gt;\n        &lt;div class=\"message bot-message\"&gt;\n            \u60a8\u597d\uff01\u6211\u662f\u9ed1\u9a6c\u5c0f\u5065\u5eb7\u52a9\u624b\uff0c\u8bf7\u95ee\u6709\u4ec0\u4e48\u5065\u5eb7\u95ee\u9898\u53ef\u4ee5\u5e2e\u60a8\u89e3\u7b54\uff1f\n            &lt;div class=\"timestamp\"&gt;\u7cfb\u7edf\u6d88\u606f&lt;/div&gt;\n        &lt;/div&gt;\n    &lt;/div&gt;\n\n    &lt;div class=\"input-container\"&gt;\n        &lt;input type=\"text\" id=\"user-input\" placeholder=\"\u8bf7\u8f93\u5165\u60a8\u7684\u5065\u5eb7\u95ee\u9898...\" autofocus&gt;\n        &lt;button id=\"send-button\"&gt;\u53d1\u9001&lt;/button&gt;\n    &lt;/div&gt;\n\n&lt;script&gt;\n    // \u6bcf\u6b21\u5237\u65b0\u9875\u9762 history \u90fd\u4ece\u7a7a\u5f00\u59cb\n    let history = [];\n\n    document.getElementById('send-button').addEventListener('click', sendMessage);\n    document.getElementById('user-input').addEventListener('keypress', function(e) {\n        if (e.key === 'Enter') sendMessage();\n    });\n\n    function sendMessage() {\n        const userInput = document.getElementById('user-input');\n        const question = userInput.value.trim();\n        if (question === '') return;\n\n        // \u663e\u793a\u7528\u6237\u6d88\u606f\n        addMessage(question, 'user');\n        userInput.value = '';\n\n        // \u628a\u5f53\u524d history \u53d1\u7ed9\u540e\u7aef\n        fetch('/api/predict', {\n            method: 'POST',\n            headers: { 'Content-Type': 'application/json' },\n            body: JSON.stringify({ question: question, history: history })\n        })\n        .then(response =&gt; response.json())\n        .then(data =&gt; {\n            // \u663e\u793a\u673a\u5668\u4eba\u56de\u7b54\n            addMessage(data.answer, 'bot');\n\n            // \u66f4\u65b0 history\n            if (Array.isArray(data.history)) {\n                history = data.history;\n            } else if (Array.isArray(data.new_history)) {\n                history = data.new_history;\n            } else {\n                history.push([question, data.answer]);\n            }\n        })\n        .catch(error =&gt; {\n            console.error('Error:', error);\n            addMessage('\u62b1\u6b49\uff0c\u670d\u52a1\u6682\u65f6\u4e0d\u53ef\u7528\uff0c\u8bf7\u7a0d\u540e\u518d\u8bd5\u3002', 'bot');\n        });\n    }\n\n    function addMessage(text, sender) {\n        const chatBox = document.getElementById('chat-box');\n        const messageDiv = document.createElement('div');\n        messageDiv.className = `message ${sender}-message`;\n\n        const now = new Date();\n        const timeString = now.toLocaleTimeString([], {hour: '2-digit', minute:'2-digit'});\n\n        messageDiv.innerHTML = `\n            ${text}\n            &lt;div class=\"timestamp\"&gt;${sender === 'user' ? '\u60a8' : '\u5c0f\u5065\u5eb7\u52a9\u624b'} \u00b7 ${timeString}&lt;/div&gt;\n        `;\n\n        chatBox.appendChild(messageDiv);\n        chatBox.scrollTop = chatBox.scrollHeight;\n    }\n&lt;/script&gt;\n\n&lt;/body&gt;\n&lt;/html&gt;\n</code></pre> <ul> <li>\u8fd0\u884capp.py\u6587\u4ef6, \u6548\u679c\u5982\u4e0b:</li> </ul> <p></p>"},{"location":"03-%E6%96%B0%E9%9B%B6%E5%94%AE%E8%A1%8C%E4%B8%9A%E8%AF%84%E4%BB%B7%E5%86%B3%E7%AD%96%E7%B3%BB%E7%BB%9F/01-%E9%A1%B9%E7%9B%AE%E8%83%8C%E6%99%AF%E4%BB%8B%E7%BB%8D.html","title":"\u9879\u76ee\u80cc\u666f\u4ecb\u7ecd","text":""},{"location":"03-%E6%96%B0%E9%9B%B6%E5%94%AE%E8%A1%8C%E4%B8%9A%E8%AF%84%E4%BB%B7%E5%86%B3%E7%AD%96%E7%B3%BB%E7%BB%9F/01-%E9%A1%B9%E7%9B%AE%E8%83%8C%E6%99%AF%E4%BB%8B%E7%BB%8D.html#_2","title":"\u5b66\u4e60\u76ee\u6807","text":"<ul> <li>\u4e86\u89e3\u9879\u76ee\u80cc\u666f</li> <li>\u638c\u63e1\u8bc4\u8bba\u6587\u672c\u5206\u7c7b\u7684\u4e3b\u8981\u89e3\u51b3\u65b9\u6cd5</li> </ul>"},{"location":"03-%E6%96%B0%E9%9B%B6%E5%94%AE%E8%A1%8C%E4%B8%9A%E8%AF%84%E4%BB%B7%E5%86%B3%E7%AD%96%E7%B3%BB%E7%BB%9F/01-%E9%A1%B9%E7%9B%AE%E8%83%8C%E6%99%AF%E4%BB%8B%E7%BB%8D.html#1","title":"1 \u9879\u76ee\u80cc\u666f","text":"<ul> <li>\u968f\u7740\u79d1\u6280\u7684\u8fc5\u901f\u53d1\u5c55\u548c\u667a\u80fd\u8bbe\u5907\u7684\u666e\u53ca\uff0cAI\u6280\u672f\u5728\u65b0\u96f6\u552e\u884c\u4e1a\u4e2d\u5f97\u5230\u4e86\u5e7f\u6cdb\u5e94\u7528\u3002\u5176\u4e2d**\u667a\u80fd\u63a8\u8350\u7cfb\u7edf**\u662fAI\u6280\u5728\u65b0\u96f6\u552e\u4e2d\u6700\u4e3a\u5e38\u89c1\u4e14\u6709\u6548\u7684\u5e94\u7528\u4e4b\u4e00\u3002\u901a\u8fc7\u5206\u6790\u7528\u6237\u7684\u8d2d\u4e70\u5386\u53f2\u3001\u6d4f\u89c8\u884c\u4e3a\u4ee5\u53ca\u559c\u597d\u504f\u597d\uff0c\u63a8\u8350\u7cfb\u7edf\u53ef\u4ee5\u6839\u636e\u4e2a\u4eba\u7279\u5f81\u7ed9\u7528\u6237\u8fdb\u884c\u4e2a\u6027\u5316\u5546\u54c1\u63a8\u8350\u3002\u8fd9\u79cd\u4e2a\u6027\u5316\u63a8\u8350\u4e0d\u4ec5\u53ef\u4ee5\u63d0\u9ad8\u7528\u6237\u8d2d\u4e70\u610f\u613f\uff0c\u51cf\u5c11\u4fe1\u606f\u8fc7\u8f7d\uff0c\u8fd8\u53ef\u4ee5\u5e26\u6765\u66f4\u9ad8\u7684\u7528\u6237\u6ee1\u610f\u5ea6\u548c\u9500\u91cf\u3002</li> <li>\u5728\u667a\u80fd\u63a8\u8350\u7cfb\u7edf\u4e2d\uff0c\u6587\u672c\u5206\u7c7b\u7684\u5e94\u7528\u5c5e\u4e8e\u91cd\u8981\u7684\u5e94\u7528\u73af\u8282\u3002\u6bd4\u5982\uff1a\u67d0\u7535\u5546\u7f51\u7ad9\u90fd\u5141\u8bb8\u7528\u6237\u4e3a\u5546\u54c1\u586b\u5199\u8bc4\u8bba\uff0c\u8fd9\u4e9b\u6587\u672c\u8bc4\u8bba\u80fd\u591f\u4f53\u73b0\u51fa\u7528\u6237\u7684\u504f\u597d\u4ee5\u53ca\u5546\u54c1\u7279\u5f81\u4fe1\u606f\uff0c\u662f\u4e00\u79cd\u8bed\u4e49\u4fe1\u606f\u4e30\u5bcc\u7684\u9690\u5f0f\u7279\u5f81\u3002 \u76f8\u6bd4\u4e8e\u5355\u7eaf\u7684\u5229\u7528\u663e\u5f0f\u8bc4\u5206\u7279\u5f81\uff0c\u6587\u672c\u4fe1\u606f\u4e00\u65b9\u9762\u53ef\u4ee5\u5f25\u8865\u8bc4\u5206\u7a00\u758f\u6027\u7684\u95ee\u9898\uff0c\u53e6\u4e00\u65b9\u9762\u5728\u63a8\u8350\u7cfb\u7edf\u7684\u53ef\u89e3\u91ca\u65b9\u9762\u4e5f\u80fd\u591f\u505a\u7684\u66f4\u597d\u3002</li> <li>\u56e0\u6b64\uff0c\u672c\u6b21\u9879\u76ee\u6211\u4eec\u5c06**\u4ee5\"\u7535\u5546\u5e73\u53f0\u7528\u6237\u8bc4\u8bba\"\u4e3a\u80cc\u666f\uff0c\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5b9e\u73b0\u8bc4\u8bba\u6587\u672c\u7684\u51c6\u786e\u5206\u7c7b**\uff0c\u8fd9\u6837\u505a\u7684\u76ee\u7684\u662f\u901a\u8fc7\u7528\u6237\u5bf9\u4e0d\u540c\u5546\u54c1\u6216\u670d\u52a1\u7684\u8bc4\u4ef7\uff0c\u5e73\u53f0\u80fd\u591f\u5feb\u901f\u56de\u5e94\u7528\u6237\u9700\u6c42\uff0c\u6539\u8fdb\u4ea7\u54c1\u548c\u670d\u52a1\u3002\u540c\u65f6\uff0c\u81ea\u52a8\u5206\u7c7b\u4e5f\u4e3a\u4e2a\u6027\u5316\u63a8\u8350\u5960\u5b9a\u57fa\u7840\uff0c\u5e2e\u52a9\u7528\u6237\u66f4\u8f7b\u677e\u5730\u627e\u5230\u7b26\u5408\u5176\u504f\u597d\u7684\u5546\u54c1\u3002</li> </ul>"},{"location":"03-%E6%96%B0%E9%9B%B6%E5%94%AE%E8%A1%8C%E4%B8%9A%E8%AF%84%E4%BB%B7%E5%86%B3%E7%AD%96%E7%B3%BB%E7%BB%9F/01-%E9%A1%B9%E7%9B%AE%E8%83%8C%E6%99%AF%E4%BB%8B%E7%BB%8D.html#2","title":"2 \u8bc4\u8bba\u6587\u672c\u5206\u7c7b\u5b9e\u73b0\u65b9\u6cd5","text":""},{"location":"03-%E6%96%B0%E9%9B%B6%E5%94%AE%E8%A1%8C%E4%B8%9A%E8%AF%84%E4%BB%B7%E5%86%B3%E7%AD%96%E7%B3%BB%E7%BB%9F/01-%E9%A1%B9%E7%9B%AE%E8%83%8C%E6%99%AF%E4%BB%8B%E7%BB%8D.html#21","title":"2.1 \u4f20\u7edf\u7684\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5","text":"<ul> <li>\u76ee\u524d\u5b9e\u73b0\u6587\u672c\u5206\u7c7b\u7684\u65b9\u6cd5\u5f88\u591a\uff0c\u5982\u7ecf\u5178\u7684\u5e94\u7528\u4e8e\u6587\u672c\u7684\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08Text-CNN\uff09\u3001\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\uff08Text-RNN)\u3001\u57fa\u4e8eBERT\u7b49\u9884\u8bad\u7ec3\u6a21\u578b\u7684fine-tuning\u7b49\uff0c\u4f46\u662f\u8fd9\u4e9b\u65b9\u6cd5\u591a\u4e3a\u5efa\u7acb\u5728\u5177\u6709\u5927\u91cf\u7684\u6807\u6ce8\u6570\u636e\u4e0b\u7684\u6709\u76d1\u7763\u5b66\u4e60\u3002\u5728\u5f88\u591a\u5b9e\u9645\u573a\u666f\u4e2d\uff0c\u7531\u4e8e\u9886\u57df\u7279\u6b8a\u6027\u548c\u6807\u6ce8\u6210\u672c\u9ad8\uff0c\u5bfc\u81f4\u6807\u6ce8\u8bad\u7ec3\u6570\u636e\u7f3a\u4e4f\uff0c\u6a21\u578b\u65e0\u6cd5\u6709\u6548\u5730\u5b66\u4e60\u53c2\u6570\uff0c\u4ece\u800c\u6613\u51fa\u73b0\u8fc7\u62df\u5408\u73b0\u8c61\u3002\u56e0\u6b64\uff0c\u5982\u4f55**\u901a\u8fc7\u5c0f\u6837\u672c\u6570\u636e\u8bad\u7ec3\u5f97\u5230\u4e00\u4e2a\u6027\u80fd\u8f83\u597d\u7684\u5206\u7c7b\u6a21\u578b**\u662f\u76ee\u524d\u7684\u7814\u7a76\u70ed\u70b9\u3002</li> </ul>"},{"location":"03-%E6%96%B0%E9%9B%B6%E5%94%AE%E8%A1%8C%E4%B8%9A%E8%AF%84%E4%BB%B7%E5%86%B3%E7%AD%96%E7%B3%BB%E7%BB%9F/01-%E9%A1%B9%E7%9B%AE%E8%83%8C%E6%99%AF%E4%BB%8B%E7%BB%8D.html#22","title":"2.2 \u6a21\u578b\u5fae\u8c03\u65b9\u6cd5","text":"<ul> <li>\u57fa\u4e8e\u524d\u9762\u7ae0\u8282\u7684\u4ecb\u7ecd\uff0c\u6211\u4eec\u53ef\u4ee5\u501f\u52a9Prompt-Tuning\u7684\u6280\u672f\uff0c\u6765\u5b9e\u73b0\u6a21\u578b\u90e8\u5206\u53c2\u6570\u7684\u5fae\u8c03\uff08\u5f53\u7136\u5982\u679c\u6a21\u578b\u53c2\u6570\u8f83\u5c0f\u6bd4\u5982BERT,\u4e5f\u53ef\u4ee5\u5168\u91cf\u53c2\u6570\u5fae\u8c03\uff09\uff0c\u76f8\u6bd4\u4f20\u7edf\u6280\u672f\u65b9\u6cd5\uff0cPrompt-Tuning\u65b9\u6cd5\u53ef\u4ee5\u5b9e\u73b0\u5728\u8f83\u5c11\u6837\u672c\u7684\u8bad\u7ec3\u4e0a\uff0c\u5c31\u53ef\u4ee5\u8fbe\u5230\u8f83\u597d\u7684\u7ed3\u679c\u3002</li> <li>\u5728\u672c\u6b21\u9879\u76ee\u4e2d\uff0c\u6211\u4eec\u5c06\u5206\u522b\u57fa\u4e8e**BERT+PET\uff08\u786c\u6a21\u7248\uff09\u4ee5\u53caBERT+P-Tuning\uff08\u8f6f\u6a21\u7248\uff09**\u4e24\u79cd\u65b9\u5f0f\u5b9e\u73b0\u7528\u6237\u8bc4\u8bba\u6587\u672c\u7684\u5206\u7c7b\u3002\u91cd\u70b9\u662f\u7406\u89e3prompt\u7684\u6784\u9020\u65b9\u6cd5\uff0c\u4ee5\u53capromt-tuning\u65b9\u6cd5\u7684\u5b9e\u73b0\u539f\u7406\u3002</li> </ul>"},{"location":"03-%E6%96%B0%E9%9B%B6%E5%94%AE%E8%A1%8C%E4%B8%9A%E8%AF%84%E4%BB%B7%E5%86%B3%E7%AD%96%E7%B3%BB%E7%BB%9F/01-%E9%A1%B9%E7%9B%AE%E8%83%8C%E6%99%AF%E4%BB%8B%E7%BB%8D.html#_3","title":"\u5c0f\u7ed3\u603b\u7ed3","text":"<p>\u672c\u7ae0\u8282\u4e3b\u8981\u4ecb\u7ecd\u4e86\u9879\u76ee\u5f00\u53d1\u7684\u80cc\u666f\u53ca\u610f\u4e49\uff0c\u8bb2\u89e3\u4e86\u8bc4\u8bba\u6587\u672c\u5206\u7c7b\u7684\u5b9e\u73b0\u65b9\u6cd5\u3002</p>"},{"location":"03-%E6%96%B0%E9%9B%B6%E5%94%AE%E8%A1%8C%E4%B8%9A%E8%AF%84%E4%BB%B7%E5%86%B3%E7%AD%96%E7%B3%BB%E7%BB%9F/02-BERT%2BPET%E6%96%B9%E5%BC%8F%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E4%BB%8B%E7%BB%8D.html","title":"\u57fa\u4e8eBERT+PET\u65b9\u5f0f\u6587\u672c\u5206\u7c7b\u4ecb\u7ecd","text":""},{"location":"03-%E6%96%B0%E9%9B%B6%E5%94%AE%E8%A1%8C%E4%B8%9A%E8%AF%84%E4%BB%B7%E5%86%B3%E7%AD%96%E7%B3%BB%E7%BB%9F/02-BERT%2BPET%E6%96%B9%E5%BC%8F%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E4%BB%8B%E7%BB%8D.html#_1","title":"\u5b66\u4e60\u76ee\u6807","text":"<ul> <li>\u7406\u89e3PET\u65b9\u5f0f\u7684\u601d\u60f3</li> <li>\u5b89\u88c5\u9879\u76ee\u5fc5\u5907\u7684\u5de5\u5177\u5305</li> <li>\u4e86\u89e3\u57fa\u4e8eBERT+PET\u65b9\u5f0f\u5b9e\u73b0\u6587\u672c\u5206\u7c7b\u7684\u6574\u4f53\u9879\u76ee\u67b6\u6784</li> </ul>"},{"location":"03-%E6%96%B0%E9%9B%B6%E5%94%AE%E8%A1%8C%E4%B8%9A%E8%AF%84%E4%BB%B7%E5%86%B3%E7%AD%96%E7%B3%BB%E7%BB%9F/02-BERT%2BPET%E6%96%B9%E5%BC%8F%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E4%BB%8B%E7%BB%8D.html#1","title":"1 \u9879\u76ee\u4ecb\u7ecd","text":"<ul> <li>\u672c\u7ae0\u6211\u4eec\u5c06\u4ee5\"\u7535\u5546\u5e73\u53f0\u7528\u6237\u8bc4\u8bba\"\u4e3a\u80cc\u666f\uff0c\u57fa\u4e8eBERT+PET\uff08\u786c\u6a21\u7248\uff09\u65b9\u6cd5\u5b9e\u73b0\u8bc4\u8bba\u6587\u672c\u7684\u51c6\u786e\u5206\u7c7b</li> </ul>"},{"location":"03-%E6%96%B0%E9%9B%B6%E5%94%AE%E8%A1%8C%E4%B8%9A%E8%AF%84%E4%BB%B7%E5%86%B3%E7%AD%96%E7%B3%BB%E7%BB%9F/02-BERT%2BPET%E6%96%B9%E5%BC%8F%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E4%BB%8B%E7%BB%8D.html#2-pet","title":"2 PET\u56de\u987e","text":"<ul> <li>PET\uff08PatternExploiting Training\uff09\u7684\u6838\u5fc3\u601d\u60f3\u662f\uff1a\u6839\u636e\u5148\u9a8c\u77e5\u8bc6\u4eba\u5de5\u5b9a\u4e49\u6a21\u7248\uff0c\u5c06\u76ee\u6807\u5206\u7c7b\u4efb\u52a1\u8f6c\u6362\u4e3a\u4e0eMLM\u4e00\u81f4\u7684\u5b8c\u5f62\u586b\u7a7a\uff0c\u7136\u540e\u518d\u53bb\u5fae\u8c03MLM\u4efb\u52a1\u53c2\u6570\u3002</li> </ul> <p>\u56fe\u4e2d\u793a\u4f8b1: \u60c5\u611f\u5206\u7c7b\u4efb\u52a1\uff08\u597d\u8bc4\u8fd8\u662f\u5dee\u8bc4\uff09\uff0c\u539f\u59cb\u6587\u672c:\u8fd9\u5bb6\u5e97\u771f\u4e0d\u9519,\u503c\u5f97\u63a8\u8350\u3002PET\u6a21\u677f: [MASK]\u6ee1\u610f\u3002Label:\u4e0d/\u5f88\u3002\u6807\u7b7e\u8bcd\u6620\u5c04\uff08Label Word Verbalizer\uff09\uff1a\u4f8b\u5982\u5982\u679c<code>[MASK]</code>\u9884\u6d4b\u7684\u8bcd\u662f\u201c\u4e0d\u201d\uff0c\u5219\u8ba4\u4e3a\u662f\u5dee\u8bc4\u7c7b\uff0c\u5982\u679c\u662f\u201c\u5f88\u201d\uff0c\u5219\u8ba4\u4e3a\u662f\u597d\u8bc4\u7c7b\u3002</p> <p>\u56fe\u4e2d\u793a\u4f8b2:\u65b0\u95fb\u5206\u7c7b\u4efb\u52a1\uff08\u591a\u5206\u7c7b\uff09\uff0c\u539f\u59cb\u6587\u672c\uff1a\u4e2d\u56fd\u5973\u6392\u518d\u593a\u51a0\uff01PET\u6a21\u7248\uff1a\u4e0b\u9762\u662f[MASK] [MASK]\u65b0\u95fb\uff0cLabel\uff1a\u4f53\u80b2/\u8d22\u7ecf/\u65f6\u653f/\u519b\u4e8b</p> <ul> <li>PET \u65b9\u6cd5\u7684\u6838\u5fc3\u6b65\u9aa4</li> </ul> <p>PET\u65b9\u6cd5\u6574\u4f53\u8fc7\u7a0b\u53ef\u4ee5\u6982\u62ec\u4e3a\uff1a\u9996\u5148\uff0c\u5c06\u4e0b\u6e38\u4efb\u52a1\u901a\u8fc7\u4eba\u5de5\u6a21\u677f\uff08pattern\uff09\u8f6c\u5316\u4e3a\u8bed\u8a00\u6a21\u578b\u7684\u586b\u7a7a\u4efb\u52a1\uff0c\u5e76\u901a\u8fc7verbalizer\u628a\u9884\u6d4b\u7684\u8bcd\u6620\u5c04\u5230\u4efb\u52a1\u6807\u7b7e\uff0c\u4ece\u800c\u7528\u5c11\u91cf\u6807\u6ce8\u6837\u672c\u8bad\u7ec3\u591a\u4e2a\u201c\u5b50\u6a21\u578b\u201d\uff1b\u63a5\u7740\uff0c\u8fd9\u4e9b\u5b50\u6a21\u578b\u5728\u5927\u91cf\u672a\u6807\u6ce8\u6570\u636e\u4e0a\u751f\u6210\u4f2a\u6807\u7b7e\uff0c\u5f62\u6210\u8f6f\u6807\u6ce8\u6570\u636e\uff1b\u6700\u540e\uff0c\u901a\u8fc7\u77e5\u8bc6\u84b8\u998f\uff0c\u8bad\u7ec3\u4e00\u4e2a\u5355\u4e00\u7684\u5b66\u751f\u6a21\u578b\u6765\u5b66\u4e60\u591a\u4e2a\u5b50\u6a21\u578b\u7684\u9884\u6d4b\u5206\u5e03\uff0c\u4ece\u800c\u517c\u987e\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002\u5728\u8fd9\u91cc\uff0c\u6211\u4eec\u53ea\u9700\u8981\u5b8c\u6210\u5206\u7c7b\u4efb\u52a1\uff0c\u6240\u4ee5\u53ea\u9700\u8981\u5b9e\u73b0\u7b2c\u4e00\u6b65\u5373\u53ef\u3002</p> <p>\u5177\u4f53\u6b65\u9aa4\u5982\u4e0b\uff1a</p> <p>1\uff09\u5b9a\u4e49\u4efb\u52a1\u6a21\u5f0f (Task Patterns)\uff1a</p> <ul> <li>\u9996\u5148\uff0c\u4f60\u9700\u8981\u5c06\u4e0b\u6e38\u4efb\u52a1\u7684\u8f93\u5165\u548c\u8f93\u51fa\uff0c\u8f6c\u6362\u4e3a\u4e00\u79cd \u5305\u542b\u7a7a\u767d\uff08[MASK] \u6216\u5176\u4ed6\u7279\u6b8a\u6807\u8bb0\uff09\u7684\u81ea\u7136\u8bed\u8a00\u53e5\u5b50\u6a21\u677f \u3002\u8fd9\u4e9b\u6a21\u677f\u88ab\u79f0\u4e3a\u201c\u6a21\u5f0f\u201d\u3002</li> <li>\u793a\u4f8b (\u60c5\u611f\u5206\u7c7b)\uff1a<ul> <li>\u539f\u59cb\u8f93\u5165\uff1a<code>\u8fd9\u90e8\u7535\u5f71\u592a\u68d2\u4e86\uff01</code> \u6807\u7b7e\uff1a<code>\u79ef\u6781</code></li> <li>POFT \u6a21\u5f0f\uff1a<code>\u8fd9\u90e8\u7535\u5f71\u592a\u68d2\u4e86\uff01\u8fd9\u662f\u4e00\u90e8____\u7684\u7535\u5f71\u3002</code> \uff08\u5176\u4e2d <code>____</code> \u662f\u5f85\u586b\u5145\u7684\u7a7a\u767d\uff09</li> </ul> </li> <li>\u793a\u4f8b (\u95ee\u9898\u56de\u7b54 - \u62bd\u53d6\u5f0f)\uff1a<ul> <li>\u539f\u59cb\u8f93\u5165\uff1a<code>\u4e0a\u4e0b\u6587\uff1a\u5317\u4eac\u662f\u4e2d\u56fd\u7684\u9996\u90fd\u3002\u95ee\u9898\uff1a\u4e2d\u56fd\u7684\u9996\u90fd\u662f\u54ea\u91cc\uff1f</code> \u7b54\u6848\uff1a<code>\u5317\u4eac</code></li> <li>POFT \u6a21\u5f0f\uff1a<code>\u6839\u636e\u4e0a\u4e0b\u6587\uff1a\u5317\u4eac\u662f\u4e2d\u56fd\u7684\u9996\u90fd\u3002\u4e2d\u56fd\u7684\u9996\u90fd\u662f\u54ea\u91cc\uff1f\u7b54\u6848\u662f____\u3002</code></li> </ul> </li> </ul> <p>2\uff09 \u5b9a\u4e49\u6807\u7b7e\u6620\u5c04 (Verbalizer)\uff1a</p> <ul> <li>\u5bf9\u4e8e\u4efb\u52a1\u7684\u6bcf\u4e2a\u6807\u7b7e\uff08\u6216\u7b54\u6848\uff09\uff0c\u4f60\u9700\u8981\u5c06\u5176\u6620\u5c04\u5230 PLM(\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b) \u8bcd\u6c47\u8868\u4e2d\u7684\u4e00\u4e2a\u6216\u591a\u4e2a \u5177\u4f53\u8bcd\u6c47 \u3002</li> <li>\u793a\u4f8b (\u60c5\u611f\u5206\u7c7b)\uff1a<ul> <li><code>\u79ef\u6781</code> \u2192 <code>\u597d</code>, <code>\u68d2</code>, <code>\u4f18\u79c0</code></li> <li><code>\u6d88\u6781</code> \u2192 <code>\u5dee</code>, <code>\u70c2</code>, <code>\u7cdf\u7cd5</code></li> </ul> </li> <li>\u793a\u4f8b (\u95ee\u9898\u56de\u7b54)\uff1a \u7b54\u6848\u672c\u8eab\u5c31\u662f\u6a21\u578b\u9700\u8981\u751f\u6210\u7684\u8bcd\u8bed\u3002</li> </ul> <p>3\uff09 \u6784\u9020\u8bad\u7ec3\u6837\u672c\uff1a</p> <ul> <li>\u5c06\u4f60\u7684 \u6240\u6709\u6709\u6807\u7b7e\u7684\u8bad\u7ec3\u6570\u636e \uff0c\u6839\u636e\u5b9a\u4e49\u7684\u6a21\u5f0f\u548c\u6807\u7b7e\u6620\u5c04\u8fdb\u884c\u8f6c\u6362\u3002</li> <li>\u5bf9\u4e8e\u6bcf\u4e2a\u6837\u672c\uff0c\u8f93\u5165\u53d8\u6210\u6a21\u5f0f\u5316\u7684\u53e5\u5b50\uff0c\u800c\u6a21\u578b\u7684\u8bad\u7ec3\u76ee\u6807\u662f\u5728\u7a7a\u767d\u5904\u751f\u6210\u6b63\u786e\u7684 Verbalizer \u8bcd\u6c47\uff08\u6216\u7b54\u6848\u8bcd\u6c47\uff09\u3002</li> </ul> <p>4\uff09 \u5168\u91cf\u5fae\u8c03 PLM\uff1a</p> <ul> <li>\u5728\u8fd9\u4e9b \u6a21\u5f0f\u5316 \u3001 \u8f6c\u6362\u540e\u7684\u8bad\u7ec3\u6570\u636e \u4e0a\uff0c\u5bf9 \u6574\u4e2a\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u5168\u91cf\u5fae\u8c03 \u3002</li> <li>\u5fae\u8c03\u7684\u76ee\u6807\u51fd\u6570\u901a\u5e38\u662f \u4ea4\u53c9\u71b5\u635f\u5931 \uff0c\u65e8\u5728\u6700\u5927\u5316\u6a21\u578b\u5728\u7a7a\u767d\u5904\u9884\u6d4b\u6b63\u786e Verbalizer \u8bcd\u6c47\u7684\u6982\u7387\u3002\u8fd9\u5b9e\u9645\u4e0a\u662f\u56de\u5f52\u5230 PLM \u9884\u8bad\u7ec3\u65f6\u7684 \u8bed\u8a00\u6a21\u578b\u76ee\u6807 \uff08\u5982\u63a9\u7801\u8bed\u8a00\u6a21\u578b\u6216\u6587\u672c\u751f\u6210\uff09\u3002<ul> <li>\u533a\u522b\u4e8e Prompt Engineering\uff1a PFT \u5728\u8fd9\u91cc \u66f4\u65b0\u6a21\u578b\u7684\u6240\u6709\u53c2\u6570 \uff0c\u800c\u4e0d\u4ec5\u4ec5\u662f Prompt \u5411\u91cf\u3002</li> <li>\u533a\u522b\u4e8e\u4f20\u7edf Fine-tuning\uff1a \u4f20\u7edf Fine-tuning \u53ef\u80fd\u662f\u5728 PLM \u4e0a\u6dfb\u52a0\u4e00\u4e2a\u4e13\u95e8\u7684\u5206\u7c7b\u5934\u6216\u62bd\u53d6\u5c42\u8fdb\u884c\u5fae\u8c03\u3002\u800c POFT \u5219\u662f\u8ba9 PLM \u901a\u8fc7 \u9884\u6d4b\u8bcd\u6c47 \u6765\u5b8c\u6210\u4efb\u52a1\uff0c\u66f4\u63a5\u8fd1\u5176\u9884\u8bad\u7ec3\u7684\u65b9\u5f0f\u3002</li> </ul> </li> </ul> <p>5\uff09 \u63a8\u7406\u9636\u6bb5\uff1a</p> <ul> <li>\u5bf9\u4e8e\u65b0\u7684\u8f93\u5165\uff0c\u540c\u6837\u901a\u8fc7\u6a21\u5f0f\u8fdb\u884c\u8f6c\u6362\u3002</li> <li>\u5c06\u8f6c\u6362\u540e\u7684\u8f93\u5165\u9001\u5165\u5fae\u8c03\u540e\u7684 PLM\u3002</li> <li>\u6a21\u578b\u4f1a\u5728\u7a7a\u767d\u5904\u751f\u6210\u6700\u53ef\u80fd\u7684\u8bcd\u6c47\u3002\u901a\u8fc7 Verbalizer\uff0c\u5c06\u8fd9\u4e9b\u9884\u6d4b\u7684\u8bcd\u6c47\u53cd\u5411\u6620\u5c04\u56de\u4efb\u52a1\u7684\u539f\u59cb\u6807\u7b7e\u6216\u7b54\u6848\u3002<ul> <li>\u4f8b\u5982\uff0c\u5982\u679c\u6a21\u578b\u9884\u6d4b <code>\u597d</code> \u7684\u6982\u7387\u6700\u9ad8\uff0c\u5c31\u5c06\u5176\u6620\u5c04\u4e3a <code>\u79ef\u6781</code>\u3002</li> </ul> </li> </ul>"},{"location":"03-%E6%96%B0%E9%9B%B6%E5%94%AE%E8%A1%8C%E4%B8%9A%E8%AF%84%E4%BB%B7%E5%86%B3%E7%AD%96%E7%B3%BB%E7%BB%9F/02-BERT%2BPET%E6%96%B9%E5%BC%8F%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E4%BB%8B%E7%BB%8D.html#3","title":"3  \u73af\u5883\u51c6\u5907","text":"<p>\u672c\u9879\u76ee\u57fa\u4e8e torch+ transformers \u5b9e\u73b0\uff0c\u8fd0\u884c\u524d\u8bf7\u5b89\u88c5\u76f8\u5173\u4f9d\u8d56\u5305\uff1a</p> <ul> <li>python==3.10</li> <li>transformers==4.40.2</li> <li>torch==2.5.1+cu121</li> <li>datasets==3.6.0</li> <li>scikit-learn==1.7.0</li> </ul>"},{"location":"03-%E6%96%B0%E9%9B%B6%E5%94%AE%E8%A1%8C%E4%B8%9A%E8%AF%84%E4%BB%B7%E5%86%B3%E7%AD%96%E7%B3%BB%E7%BB%9F/02-BERT%2BPET%E6%96%B9%E5%BC%8F%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E4%BB%8B%E7%BB%8D.html#4","title":"4 \u9879\u76ee\u67b6\u6784","text":"<p>\u9879\u76ee\u67b6\u6784\u6d41\u7a0b\u56fe\uff1a</p> <p>\u9879\u76ee\u6574\u4f53\u4ee3\u7801\u4ecb\u7ecd\uff1a</p> <p></p>"},{"location":"03-%E6%96%B0%E9%9B%B6%E5%94%AE%E8%A1%8C%E4%B8%9A%E8%AF%84%E4%BB%B7%E5%86%B3%E7%AD%96%E7%B3%BB%E7%BB%9F/02-BERT%2BPET%E6%96%B9%E5%BC%8F%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E4%BB%8B%E7%BB%8D.html#_2","title":"\u5c0f\u7ed3\u603b\u7ed3","text":"<p>\u672c\u7ae0\u8282\u4e3b\u8981\u4ecb\u7ecd\u4e86\u9879\u76ee\u5f00\u53d1\u7684\u80cc\u666f\u53ca\u610f\u4e49\uff0c\u660e\u786e\u4e86\u9879\u76ee\u7684\u6574\u4f53\u67b6\u6784\uff0c\u5e76\u5bf9\u9879\u76ee\u4e2d\u6574\u4f53\u4ee3\u7801\u7ed3\u6784\u8fdb\u884c\u4e86\u4ecb\u7ecd\u3002</p>"},{"location":"03-%E6%96%B0%E9%9B%B6%E5%94%AE%E8%A1%8C%E4%B8%9A%E8%AF%84%E4%BB%B7%E5%86%B3%E7%AD%96%E7%B3%BB%E7%BB%9F/03-BERT%2BPET%E6%96%B9%E5%BC%8F%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E4%BB%8B%E7%BB%8D.html","title":"3.3 BERT+PET\u65b9\u5f0f\u6570\u636e\u9884\u5904\u7406","text":""},{"location":"03-%E6%96%B0%E9%9B%B6%E5%94%AE%E8%A1%8C%E4%B8%9A%E8%AF%84%E4%BB%B7%E5%86%B3%E7%AD%96%E7%B3%BB%E7%BB%9F/03-BERT%2BPET%E6%96%B9%E5%BC%8F%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E4%BB%8B%E7%BB%8D.html#bertpet","title":"\u57fa\u4e8eBERT+PET\u65b9\u5f0f\u6570\u636e\u9884\u5904\u7406","text":""},{"location":"03-%E6%96%B0%E9%9B%B6%E5%94%AE%E8%A1%8C%E4%B8%9A%E8%AF%84%E4%BB%B7%E5%86%B3%E7%AD%96%E7%B3%BB%E7%BB%9F/03-BERT%2BPET%E6%96%B9%E5%BC%8F%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E4%BB%8B%E7%BB%8D.html#_1","title":"\u5b66\u4e60\u76ee\u6807","text":"<ul> <li>\u4e86\u89e3\u672c\u9879\u76ee\u6570\u636e\u7c7b\u578b\u548c\u8868\u73b0\u683c\u5f0f</li> <li>\u638c\u63e1\u6570\u636e\u5904\u7406\u7684\u5de5\u5177\u51fd\u6570\u4ee3\u7801\u5b9e\u73b0</li> </ul>"},{"location":"03-%E6%96%B0%E9%9B%B6%E5%94%AE%E8%A1%8C%E4%B8%9A%E8%AF%84%E4%BB%B7%E5%86%B3%E7%AD%96%E7%B3%BB%E7%BB%9F/03-BERT%2BPET%E6%96%B9%E5%BC%8F%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E4%BB%8B%E7%BB%8D.html#bertpet_1","title":"BERT+PET\u65b9\u5f0f\u6570\u636e\u9884\u5904\u7406","text":"<ul> <li>\u672c\u9879\u76ee\u4e2d\u5bf9\u6570\u636e\u90e8\u5206\u7684\u9884\u5904\u7406\u6b65\u9aa4\u5982\u4e0b:</li> <li>\u67e5\u770b\u9879\u76ee\u6570\u636e\u96c6</li> <li>\u7f16\u5199Config\u7c7b\u9879\u76ee\u6587\u4ef6\u914d\u7f6e\u4ee3\u7801</li> <li>\u7f16\u5199\u6570\u636e\u5904\u7406\u76f8\u5173\u4ee3\u7801</li> </ul>"},{"location":"03-%E6%96%B0%E9%9B%B6%E5%94%AE%E8%A1%8C%E4%B8%9A%E8%AF%84%E4%BB%B7%E5%86%B3%E7%AD%96%E7%B3%BB%E7%BB%9F/03-BERT%2BPET%E6%96%B9%E5%BC%8F%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E4%BB%8B%E7%BB%8D.html#1","title":"1 \u67e5\u770b\u9879\u76ee\u6570\u636e\u96c6","text":"<ul> <li> <p>\u6570\u636e\u5b58\u653e\u4f4d\u7f6e\uff1allm_tuning/prompt_tasks/PET/data</p> </li> <li> <p>data\u6587\u4ef6\u5939\u91cc\u9762\u5305\u542b4\u4e2atxt\u6587\u6863\uff0c\u5206\u522b\u4e3a\uff1atrain.txt\u3001dev.txt\u3001prompt.txt\u3001verbalizer.txt</p> </li> </ul>"},{"location":"03-%E6%96%B0%E9%9B%B6%E5%94%AE%E8%A1%8C%E4%B8%9A%E8%AF%84%E4%BB%B7%E5%86%B3%E7%AD%96%E7%B3%BB%E7%BB%9F/03-BERT%2BPET%E6%96%B9%E5%BC%8F%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E4%BB%8B%E7%BB%8D.html#11-traintxt","title":"1.1 train.txt","text":"<ul> <li>train.txt\u4e3a\u8bad\u7ec3\u6570\u636e\u96c6\uff0c\u5176\u90e8\u5206\u6570\u636e\u5c55\u793a\u5982\u4e0b\uff1a</li> </ul> <pre><code>\u6c34\u679c  \u8106\u8106\u7684\uff0c\u751c\u5473\u53ef\u4ee5\uff0c\u53ef\u80fd\u65f6\u95f4\u6709\u70b9\u957f\u4e86\uff0c\u6c34\u5206\u4e0d\u662f\u5f88\u8db3\u3002\n\u5e73\u677f  \u534e\u4e3a\u673a\u5668\u80af\u5b9a\u4e0d\u9519\uff0c\u4f46\u7b2c\u4e00\u6b21\u78b0\u4e0a\u4eac\u4e1c\u6700\u7cdf\u7cd5\u7684\u670d\u52a1\uff0c\u4ee5\u540e\u4e0d\u60f3\u5230\u4eac\u4e1c\u8d2d\u7269\u4e86\u3002\n\u4e66\u7c4d  \u4e3a\u4ec0\u4e48\u4e0d\u8ba4\u771f\u7684\u68c0\u67e5\u4e00\u4e0b\uff0c \u53d1\u8fd9\u4e48\u4e00\u672c\u810f\u810f\u7684\u4e66\u7ed9\u987e\u5ba2\u5462\uff01\n\u8863\u670d  \u624b\u611f\u4e0d\u9519\uff0c\u7528\u6599\u4e5f\u5f88\u597d\uff0c\u4e0d\u77e5\u9053\u6c34\u6d17\u540e\u600e\u6837\uff0c\u76f8\u4fe1\u5927\u54c1\u724c\uff0c\u8d28\u91cf\u8fc7\u5173\uff0c\u4e94\u661f\u597d\u8bc4\uff01\uff01\uff01\n\u6c34\u679c  \u82f9\u679c\u6709\u70b9\u5c0f\uff0c\u4e0d\u8fc7\u597d\u5403\uff0c\u8fd8\u6709\u51e0\u4e2a\u70c2\u7684\u3002\u4f30\u8ba1\u662f\u6545\u610f\u7684\u653e\u7684\u3002\u5dee\u8bc4\u3002\n\u8863\u670d  \u6389\u8272\u6389\u7684\u5389\u5bb3\uff0c\u6d17\u4e00\u6b21\u5c31\u82b1\u4e86\n</code></pre> <p>train.txt\u4e00\u5171\u5305\u542b63\u6761\u6837\u672c\u6570\u636e\uff0c\u6bcf\u4e00\u884c\u7528<code>\\t</code>\u5206\u5f00\uff0c\u524d\u534a\u90e8\u5206\u4e3a\u6807\u7b7e(label)\uff0c\u540e\u534a\u90e8\u5206\u4e3a\u539f\u59cb\u8f93\u5165 (\u7528\u6237\u8bc4\u8bba)\u3002</p> <p>\u5982\u679c\u60f3\u4f7f\u7528\u81ea\u5b9a\u4e49\u6570\u636e\u8bad\u7ec3\uff0c\u53ea\u9700\u8981\u4eff\u7167\u4e0a\u8ff0\u793a\u4f8b\u6570\u636e\u6784\u5efa\u6570\u636e\u96c6\u5373\u53ef\u3002</p>"},{"location":"03-%E6%96%B0%E9%9B%B6%E5%94%AE%E8%A1%8C%E4%B8%9A%E8%AF%84%E4%BB%B7%E5%86%B3%E7%AD%96%E7%B3%BB%E7%BB%9F/03-BERT%2BPET%E6%96%B9%E5%BC%8F%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E4%BB%8B%E7%BB%8D.html#12-devtxt","title":"1.2 dev.txt","text":"<ul> <li>dev.txt\u4e3a\u9a8c\u8bc1\u6570\u636e\u96c6\uff0c\u5176\u90e8\u5206\u6570\u636e\u5c55\u793a\u5982\u4e0b\uff1a</li> </ul> <pre><code>\u4e66\u7c4d  \"\u4e00\u70b9\u90fd\u4e0d\u597d\u7b11,\u5f88\u5931\u671b,\u5185\u5bb9\u4e5f\u4e0d\u662f\u5f88\u5b9e\u7528\"\n\u8863\u670d  \u5b8c\u5168\u662f\u4e00\u6761\u65e7\u88e4\u5b50\u3002\n\u624b\u673a  \u76f8\u673a\u8d28\u91cf\u4e0d\u9519\uff0c\u5982\u679c\u9633\u5149\u5145\u8db3\uff0c\u53ef\u4ee5\u548c\u6570\u7801\u76f8\u673a\u5ab2\u7f8e\uff0e\u754c\u9762\u6bd4\u8f83\u4eba\u6027\u5316\uff0c\u5bb9\u6613\u4f7f\u7528\uff0e\u8f6f\u4ef6\u5b89\u88c5\u7b80\u4fbf\n\u4e66\u7c4d  \u660e\u660e\u8bf4\u6709\u8d27\uff0c\u7ed3\u679c\u9001\u8d27\u53c8\u6ca1\u6709\u4e86\u3002\u5e76\u4e14\u4e5f\u4e0d\u544a\u8bc9\u6211\uff0c\u600e\u4e48\u8bc4\u554a\n\u6d17\u6d74  \u975e\u5e38\u4e0d\u6ee1\u610f\uff0c\u665a\u4e0a\u6d17\u7684\u5934\u53d1\uff0c\u7b2c\u4e8c\u5929\u5934\u75d2\u75d2\u7684\u4e0d\u884c\u4e86\uff0c\u8fd8\u90fd\u662f\u5934\u76ae\u5c51\u3002\n\u6c34\u679c  \u8fd9\u4e2a\u82f9\u679c\u611f\u89c9\u662f\u957f\u719f\u7684\u82f9\u679c\uff0c\u6ca1\u6709\u6253\u8721\uff0c\u4e0d\u9519\uff0c\u53c8\u751c\u53c8\u8106\n</code></pre> <p>dev.txt\u4e00\u5171\u5305\u542b590\u6761\u6837\u672c\u6570\u636e\uff0c\u6bcf\u4e00\u884c\u7528<code>\\t</code>\u5206\u5f00\uff0c\u524d\u534a\u90e8\u5206\u4e3a\u6807\u7b7e(label)\uff0c\u540e\u534a\u90e8\u5206\u4e3a\u539f\u59cb\u8f93\u5165 (\u7528\u6237\u8bc4\u8bba)\u3002</p> <p>\u5982\u679c\u60f3\u4f7f\u7528\u81ea\u5b9a\u4e49\u6570\u636e\u8bad\u7ec3\uff0c\u53ea\u9700\u8981\u4eff\u7167\u4e0a\u8ff0\u793a\u4f8b\u6570\u636e\u6784\u5efa\u6570\u636e\u96c6\u5373\u53ef\u3002</p>"},{"location":"03-%E6%96%B0%E9%9B%B6%E5%94%AE%E8%A1%8C%E4%B8%9A%E8%AF%84%E4%BB%B7%E5%86%B3%E7%AD%96%E7%B3%BB%E7%BB%9F/03-BERT%2BPET%E6%96%B9%E5%BC%8F%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E4%BB%8B%E7%BB%8D.html#13-prompttxt","title":"1.3 prompt.txt","text":"<ul> <li>prompt.txt\u4e3a\u4eba\u5de5\u8bbe\u5b9a\u63d0\u793a\u6a21\u7248\uff0c\u5176\u6570\u636e\u5c55\u793a\u5982\u4e0b\uff1a</li> </ul> <pre><code>\u8fd9\u662f\u4e00\u6761{MASK}\u8bc4\u8bba\uff1a{textA}\n</code></pre> <p>\u5176\u4e2d\uff0c\u7528\u5927\u62ec\u53f7\u62ec\u8d77\u6765\u7684\u90e8\u5206\u4e3a\u300c\u81ea\u5b9a\u4e49\u53c2\u6570\u300d\uff0c\u53ef\u4ee5\u81ea\u5b9a\u4e49\u8bbe\u7f6e\u5927\u62ec\u53f7\u5185\u7684\u503c\u3002</p> <p>\u793a\u4f8b\u4e2d {MASK} \u4ee3\u8868 [MASK] token \u7684\u4f4d\u7f6e\uff0c{textA} \u4ee3\u8868\u8bc4\u8bba\u6570\u636e\u7684\u4f4d\u7f6e\u3002</p> <p>\u4f60\u53ef\u4ee5\u6539\u4e3a\u81ea\u5df1\u60f3\u8981\u7684\u6a21\u677f\uff0c\u4f8b\u5982\u60f3\u65b0\u589e\u4e00\u4e2a {textB} \u53c2\u6570\uff1a</p> <pre><code>{textA}\u548c{textB}\u662f{MASK}\u540c\u7684\u610f\u601d\u3002\n</code></pre>"},{"location":"03-%E6%96%B0%E9%9B%B6%E5%94%AE%E8%A1%8C%E4%B8%9A%E8%AF%84%E4%BB%B7%E5%86%B3%E7%AD%96%E7%B3%BB%E7%BB%9F/03-BERT%2BPET%E6%96%B9%E5%BC%8F%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E4%BB%8B%E7%BB%8D.html#14-verbalizertxt","title":"1.4 verbalizer.txt","text":"<ul> <li> <p>verbalizer.txt \u4e3b\u8981\u7528\u4e8e\u5b9a\u4e49\u300c\u771f\u5b9e\u6807\u7b7e\u300d\u5230\u300c\u6807\u7b7e\u9884\u6d4b\u8bcd\u300d\u4e4b\u95f4\u7684\u6620\u5c04\u3002\u5728\u6709\u4e9b\u60c5\u51b5\u4e0b\uff0c\u5c06\u300c\u771f\u5b9e\u6807\u7b7e\u300d\u4f5c\u4e3a [MASK] \u53bb\u9884\u6d4b\u53ef\u80fd\u4e0d\u5177\u5907\u5f88\u597d\u7684\u8bed\u4e49\u901a\u987a\u6027\uff0c\u56e0\u6b64\uff0c\u6211\u4eec\u4f1a\u5bf9\u300c\u771f\u5b9e\u6807\u7b7e\u300d\u505a\u4e00\u5b9a\u7684\u6620\u5c04\u3002</p> </li> <li> <p>\u4f8b\u5982\uff1a</p> </li> </ul> <pre><code>\"\u4e2d\u56fd\u7206\u51b72-1\u6218\u80dc\u97e9\u56fd\"\u662f\u4e00\u5219[MASK][MASK]\u65b0\u95fb\u3002 \u4f53\u80b2\n</code></pre> <ul> <li> <p>\u8fd9\u53e5\u8bdd\u4e2d\u7684\u6807\u7b7e\u4e3a\u300c\u4f53\u80b2\u300d\uff0c\u4f46\u5982\u679c\u6211\u4eec\u5c06\u6807\u7b7e\u8bbe\u7f6e\u4e3a\u300c\u8db3\u7403\u300d\u4f1a\u66f4\u5bb9\u6613\u9884\u6d4b\u3002</p> </li> <li> <p>\u56e0\u6b64\uff0c\u6211\u4eec\u53ef\u4ee5\u5bf9\u300c\u4f53\u80b2\u300d\u8fd9\u4e2a label \u6784\u5efa\u8bb8\u591a\u4e2a\u5b50\u6807\u7b7e\uff0c\u5728\u63a8\u7406\u65f6\uff0c\u53ea\u8981\u9884\u6d4b\u5230\u5b50\u6807\u7b7e\u6700\u7ec8\u63a8\u7406\u51fa\u771f\u5b9e\u6807\u7b7e\u5373\u53ef\uff0c\u5982\u4e0b\uff1a</p> </li> </ul> <pre><code>\u4f53\u80b2 -&gt; \u8db3\u7403,\u7bee\u7403,\u7f51\u7403,\u68d2\u7403,\u4e52\u4e53,\u4f53\u80b2\n</code></pre> <ul> <li>\u9879\u76ee\u4e2d\u6807\u7b7e\u8bcd\u6620\u5c04\u6570\u636e\u5c55\u793a\u5982\u4e0b\uff1a</li> </ul> <pre><code>\u7535\u8111  \u7535\u8111\n\u6c34\u679c  \u6c34\u679c\n\u5e73\u677f  \u5e73\u677f\n\u8863\u670d  \u8863\u670d\n\u9152\u5e97  \u9152\u5e97\n\u6d17\u6d74  \u6d17\u6d74\n\u4e66\u7c4d  \u4e66\u7c4d\n\u8499\u725b  \u8499\u725b\n\u624b\u673a  \u624b\u673a\n\u7535\u5668  \u7535\u5668\n</code></pre> <p>verbalizer.txt \u4e00\u5171\u5305\u542b10\u4e2a\u7c7b\u522b\uff0c\u4e0a\u8ff0\u6570\u636e\u4e2d\uff0c\u6211\u4eec\u4f7f\u7528\u4e861\u5bf91\u7684verbalizer, \u5982\u679c\u60f3\u5b9a\u4e49\u4e00\u5bf9\u591a\u7684\u6620\u5c04\uff0c\u53ea\u9700\u8981\u5728\u540e\u9762\u7528\",\"\u5206\u5272\u5373\u53ef\uff0c eg: </p> <pre><code>\u6c34\u679c    \u82f9\u679c,\u9999\u8549,\u6a58\u5b50\n</code></pre> <p>\u82e5\u60f3\u4f7f\u7528\u81ea\u5b9a\u4e49\u6570\u636e\u8bad\u7ec3\uff0c\u53ea\u9700\u8981\u4eff\u7167\u793a\u4f8b\u6570\u636e\u6784\u5efa\u6570\u636e\u96c6</p>"},{"location":"03-%E6%96%B0%E9%9B%B6%E5%94%AE%E8%A1%8C%E4%B8%9A%E8%AF%84%E4%BB%B7%E5%86%B3%E7%AD%96%E7%B3%BB%E7%BB%9F/03-BERT%2BPET%E6%96%B9%E5%BC%8F%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E4%BB%8B%E7%BB%8D.html#2-config","title":"2 \u7f16\u5199Config\u7c7b\u9879\u76ee\u6587\u4ef6\u914d\u7f6e\u4ee3\u7801","text":"<ul> <li> <p>\u4ee3\u7801\u8def\u5f84\uff1allm_tuning/prompt_tasks/PET/pet_config.py</p> </li> <li> <p>config\u6587\u4ef6\u76ee\u7684\uff1a\u914d\u7f6e\u9879\u76ee\u5e38\u7528\u53d8\u91cf\uff0c\u4e00\u822c\u8fd9\u4e9b\u53d8\u91cf\u5c5e\u4e8e\u4e0d\u7ecf\u5e38\u6539\u53d8\u7684\uff0c\u6bd4\u5982\uff1a\u8bad\u7ec3\u6587\u4ef6\u8def\u5f84\u3001\u6a21\u578b\u8bad\u7ec3\u6b21\u6570\u3001\u6a21\u578b\u8d85\u53c2\u6570\u7b49\u7b49</p> </li> </ul> <p>\u5177\u4f53\u4ee3\u7801\u5b9e\u73b0\uff1a</p> <pre><code>import torch\nimport os\n\nbase_dir = os.path.dirname(os.path.abspath(__file__))\n# print(f'base_dir--&gt;{base_dir}')\n\nclass ProjectConfig(object):\n    def __init__(self):\n        # \u521d\u59cb\u5316\u8bbe\u5907\u914d\u7f6e\uff0c\u6839\u636e\u7cfb\u7edf\u73af\u5883\u9009\u62e9\u4f7f\u7528GPU\u6216CPU\n        self.device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n        # self.device = \"mps:0\"\n\n        # \u9884\u8bad\u7ec3\u6a21\u578b\u8def\u5f84\u914d\u7f6e\n        self.pre_model = os.path.join(base_dir, '../../bert-base-chinese')\n\n        # \u8bad\u7ec3\u3001\u9a8c\u8bc1\u6570\u636e\u96c6\u8def\u5f84\u914d\u7f6e\n        self.train_path = os.path.join(base_dir, 'data/train.txt')\n        self.dev_path = os.path.join(base_dir, 'data/dev.txt')\n\n        # \u63d0\u793a\u8bcd\u548c\u6807\u7b7e\u6620\u5c04\u6587\u4ef6\u8def\u5f84\u914d\u7f6e\n        self.prompt_file = os.path.join(base_dir, 'data/prompt.txt')\n        self.verbalizer = os.path.join(base_dir, 'data/verbalizer.txt')\n\n        # \u6a21\u578b\u8f93\u5165\u5e8f\u5217\u6700\u5927\u957f\u5ea6\u914d\u7f6e\n        self.max_seq_len = 256\n\n        # \u8bbe\u7f6e\u8bad\u7ec3\u7684\u8d85\u53c2\u6570\n        self.batch_size = 8  # \u6bcf\u4e2a\u6279\u6b21\u7684\u5927\u5c0f\uff0c\u6839\u636e\u663e\u5b58\u548c\u6a21\u578b\u5927\u5c0f\u8c03\u6574\n        self.learning_rate = 5e-5  # \u5b66\u4e60\u7387\uff0c\u5f71\u54cd\u6a21\u578b\u6536\u655b\u901f\u5ea6\u548c\u6548\u679c\n        self.weight_decay = 0  # \u6743\u91cd\u8870\u51cf\uff0c\u7528\u4e8e\u9632\u6b62\u8fc7\u62df\u5408\uff0c\u8fd9\u91cc\u4e0d\u4f7f\u7528\u6743\u91cd\u8870\u51cf\n        self.warmup_ratio = 0.06  # \u5b66\u4e60\u7387\u9884\u70ed\u6bd4\u4f8b\uff0c\u5e2e\u52a9\u6a21\u578b\u521d\u671f\u66f4\u5feb\u5730\u5b66\u4e60\n        self.max_label_len = 2  # \u6700\u5927\u6807\u7b7e\u957f\u5ea6\uff0c\u9650\u5236\u8f93\u51fa\u5e8f\u5217\u7684\u6700\u5927\u957f\u5ea6\n        self.epochs = 20  # \u8bad\u7ec3\u7684\u8f6e\u6570\uff0c\u5373\u6574\u4e2a\u6570\u636e\u96c6\u901a\u8fc7\u6a21\u578b\u7684\u6b21\u6570\n\n        # \u65e5\u5fd7\u548c\u9a8c\u8bc1\u914d\u7f6e\n        self.logging_steps = 2\n        self.valid_steps = 20\n\n        # \u6a21\u578b\u4fdd\u5b58\u8def\u5f84\u914d\u7f6e\n        self.save_dir = os.path.join(base_dir, 'save_model')\n\n\nif __name__ == '__main__':\n    pc = ProjectConfig()\n    print(pc.prompt_file)\n    print(pc.pre_model)\n</code></pre>"},{"location":"03-%E6%96%B0%E9%9B%B6%E5%94%AE%E8%A1%8C%E4%B8%9A%E8%AF%84%E4%BB%B7%E5%86%B3%E7%AD%96%E7%B3%BB%E7%BB%9F/03-BERT%2BPET%E6%96%B9%E5%BC%8F%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E4%BB%8B%E7%BB%8D.html#3","title":"3 \u7f16\u5199\u6570\u636e\u5904\u7406\u76f8\u5173\u4ee3\u7801","text":"<ul> <li> <p>\u4ee3\u7801\u8def\u5f84\uff1allm_tuning/prompt_tasks/PET/data_handle/</p> </li> <li> <p>data_handle\u6587\u4ef6\u5939\u4e2d\u4e00\u5171\u5305\u542b\u4e09\u4e2apy\u811a\u672c\uff1atemplate.py\u3001data_preprocess.py\u3001data_loader.py</p> </li> </ul>"},{"location":"03-%E6%96%B0%E9%9B%B6%E5%94%AE%E8%A1%8C%E4%B8%9A%E8%AF%84%E4%BB%B7%E5%86%B3%E7%AD%96%E7%B3%BB%E7%BB%9F/03-BERT%2BPET%E6%96%B9%E5%BC%8F%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E4%BB%8B%E7%BB%8D.html#31-templatepy","title":"3.1 template.py","text":"<ul> <li> <p>\u76ee\u7684\uff1a\u6784\u5efa\u56fa\u5b9a\u6a21\u7248\u7c7b\uff0ctext2id\u7684\u8f6c\u6362</p> </li> <li> <p>\u5b9a\u4e49HardTemplate\u7c7b\u4ee3\u7801\u5982\u4e0b\uff1a</p> </li> </ul> <pre><code>from rich import print  # \u7ec8\u7aef\u5c42\u6b21\u663e\u793a\nfrom transformers import AutoTokenizer\nimport numpy as np\n\nfrom prompt_tasks.PET.pet_config import ProjectConfig\n\n\n# \u4f7f\u7528\u786c\u6a21\u677f\uff0c\u4eba\u5de5\u5b9a\u4e49\u53e5\u5b50\u548c[MASK]\u4e4b\u95f4\u7684\u4f4d\u7f6e\u5173\u7cfb\u3002\nclass HardTemplate(object):\n    def __init__(self, prompt: str):\n        '''\n        \u521d\u59cb\u5316Prompt\u5bf9\u8c61\u7684\u6784\u9020\u51fd\u6570\n        :param prompt: prompt\u683c\u5f0f\u5b9a\u4e49\u5b57\u7b26\u4e32, \u8868\u793a\u5f85\u5904\u7406\u7684\u63d0\u793a\u6a21\u677f e.g. -&gt; \"\u8fd9\u662f\u4e00\u6761{MASK}\u8bc4\u8bba\uff1a{textA}\u3002\"\n        '''\n        self.prompt = prompt  # \u4fdd\u5b58\u539f\u59cb\u7684\u63d0\u793a\u6a21\u677f\u5b57\u7b26\u4e32\n        self.inputs_list = []  # \u6839\u636e\u6587\u5b57prompt\u62c6\u5206\u4e3a\u5404part\u7684\u5217\u8868\n        self.custom_tokens = {'MASK'}   # \u521d\u59cb\u5316\u81ea\u5b9a\u4e49token\u96c6\u5408\uff0c\u81f3\u5c11\u5305\u542b'MASK' token\n\n        self.prompt_analysis()  # \u89e3\u6790prompt\u6a21\u677f\uff0c\u521d\u59cb\u5316\u65f6\u5373\u5bf9prompt\u8fdb\u884c\u5206\u6790\u5904\u7406\n\n    def prompt_analysis(self):\n        '''\n        \u5c06prompt\u6587\u5b57\u6a21\u677f\u62c6\u89e3\u4e3a\u53ef\u6620\u5c04\u7684\u6570\u636e\u7ed3\u6784\u3002\n        Examples:\n            prompt -&gt; \"\u8fd9\u662f\u4e00\u6761{MASK}\u8bc4\u8bba\uff1a{textA}\u3002\"\n            inputs_list -&gt; ['\u8fd9', '\u662f', '\u4e00', '\u6761', 'MASK', '\u8bc4', '\u8bba', '\uff1a', 'textA', '\u3002']\n            custom_tokens -&gt; {'textA', 'MASK'}\n        :return:\n        '''\n        # print(f'prompt--&gt;{self.prompt}')\n        idx = 0\n        # \u904d\u5386\u63d0\u793a\u6a21\u677f\u5b57\u7b26\u4e32\u4e2d\u7684\u6bcf\u4e2a\u5b57\u7b26\n        while idx &lt; len(self.prompt):\n            str_part = ''\n            # \u5982\u679c\u5f53\u524d\u5b57\u7b26\u4e0d\u662f'{', '}'\uff0c\u5219\u76f4\u63a5\u6dfb\u52a0\u5230\u8f93\u5165\u5217\u8868\u4e2d\n            if self.prompt[idx] not in ['{', '}']:\n                self.inputs_list.append(self.prompt[idx])\n            # \u5982\u679c\u9047\u5230'{'\uff0c\u8868\u793a\u8fdb\u5165\u81ea\u5b9a\u4e49\u5b57\u6bb5\u90e8\u5206\n            if self.prompt[idx] == '{':  # \u8fdb\u5165\u81ea\u5b9a\u4e49\u5b57\u6bb5\n                idx += 1\n                # \u7ee7\u7eed\u904d\u5386\u76f4\u5230\u9047\u5230'}'\uff0c\u5e76\u5c06\u81ea\u5b9a\u4e49\u5b57\u6bb5\u7684\u503c\u62fc\u63a5\u5230str_part\u4e2d\n                while self.prompt[idx] != '}':\n                    str_part += self.prompt[idx]  # \u62fc\u63a5\u8be5\u81ea\u5b9a\u4e49\u5b57\u6bb5\u7684\u503c\n                    idx += 1\n                # print(f'idx--&gt;{idx}')\n            # \u5982\u679c\u9047\u5230'}'\uff0c\u4f46\u6ca1\u6709\u5bf9\u5e94\u7684'{'\uff0c\u629b\u51fa\u5f02\u5e38\u63d0\u793a\u62ec\u53f7\u4e0d\u5339\u914d\n            elif self.prompt[idx] == '}':\n                raise ValueError(\"\u9047\u5230\u4e86\u5355\u72ec\u7684 '}', \u8bf7\u68c0\u67e5\u8f93\u5165\u7684prompt\u3002\")\n            # \u5982\u679cstr_part\u4e0d\u4e3a\u7a7a\uff0c\u8868\u793a\u5df2\u7ecf\u5b8c\u6574\u5730\u83b7\u53d6\u4e86\u4e00\u4e2a\u81ea\u5b9a\u4e49\u5b57\u6bb5\n            if str_part:\n                self.inputs_list.append(str_part)  # \u5c06\u6240\u6709\u81ea\u5b9a\u4e49\u5b57\u6bb5\u6dfb\u52a0\u5230\u8f93\u5165\u5217\u8868\u4e2d\n                self.custom_tokens.add(str_part)  # \u5c06\u6240\u6709\u81ea\u5b9a\u4e49\u5b57\u6bb5\u5b58\u50a8\uff0c\u540e\u7eed\u4f1a\u68c0\u6d4b\u8f93\u5165\u4fe1\u606f\u662f\u5426\u5b8c\u6574\n            # \u79fb\u52a8\u5230\u4e0b\u4e00\u4e2a\u5b57\u7b26\n            idx += 1\n\n        # print(f'self.inputs_list--&gt;{self.inputs_list}')\n        # print(f'self.custom_tokens--&gt;{self.custom_tokens}')\n\n    def __call__(self,\n                 inputs_dict: dict,\n                 tokenizer,\n                 mask_length,\n                 max_seq_len=512):\n        '''\n        \u8f93\u5165\u4e00\u4e2a\u6837\u672c\uff0c\u8f6c\u6362\u4e3a\u7b26\u5408\u6a21\u677f\u7684\u683c\u5f0f\u3002\n        :param inputs_dict: prompt\u4e2d\u7684\u53c2\u6570\u5b57\u5178, e.g. -&gt; { \"textA\": \"\u8fd9\u4e2a\u624b\u673a\u4e5f\u592a\u5361\u4e86\", \"MASK\": \"[MASK]\"}\n        :param tokenizer: \u7528\u4e8eencoding\u6587\u672c\u7684\u5206\u8bcd\u5668\n        :param mask_length: MASK token \u7684\u957f\u5ea6\n        :param max_seq_len: \u6700\u5927\u7684\u53e5\u5b50\u957f\u5ea6\n        :return:\n        dict -&gt; {\n                'text': '[CLS]\u8fd9\u662f\u4e00\u6761[MASK]\u8bc4\u8bba\uff1a\u8fd9\u4e2a\u624b\u673a\u4e5f\u592a\u5361\u4e86\u3002[SEP]',\n                'input_ids': [1, 47, 10, 7, 304, 3, 480, 279, 74, 47, 27, 247, 98, 105, 512, 777, 15, 12043, 2],\n                'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n                'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n                'mask_position': [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n            }\n        '''\n        # \u5b9a\u4e49\u8f93\u51fa\u683c\u5f0f\n        # \u521d\u59cb\u5316\u4e00\u4e2a\u5b57\u5178\u5bf9\u8c61\u4ee5\u5b58\u50a8\u5904\u7406\u540e\u7684\u8f93\u51fa\u6570\u636e\n        # \u8be5\u5b57\u5178\u5305\u542b\u4e86\u6587\u672c\u6570\u636e\u53ca\u5176\u5bf9\u5e94\u7684\u7f16\u7801\u4fe1\u606f\u3001\u6ce8\u610f\u529b\u63a9\u7801\u548c\u63a9\u7801\u4f4d\u7f6e\u7b49\u5173\u952e\u4fe1\u606f\n        outputs = {\n            # \u5b58\u50a8\u539f\u59cb\u6587\u672c\u6570\u636e\n            'text': '',\n            # \u5b58\u50a8\u6587\u672c\u7ecf\u8fc7\u5206\u8bcd\u548c\u6570\u503c\u5316\u540e\u7684\u8f93\u5165ID\u5e8f\u5217\n            'input_ids': [],\n            # \u5b58\u50a8\u6bb5\u5d4c\u5165\uff08token type embeddings\uff09\u7684ID\u5e8f\u5217\uff0c\u7528\u4e8e\u533a\u5206\u4e0d\u540c\u53e5\u5b50\n            'token_type_ids': [],\n            # \u5b58\u50a8\u6ce8\u610f\u529b\u63a9\u7801\uff0c\u7528\u4e8e\u6307\u793a\u6bcf\u4e2atoken\u662f\u5426\u5e94\u8be5\u88ab\u5173\u6ce8\n            'attention_mask': [],\n            # \u5b58\u50a8\u63a9\u7801\u4f4d\u7f6e\uff0c\u5373\u5728\u8f93\u5165\u5e8f\u5217\u4e2d\u88ab\u63a9\u7801\u7684token\u7684\u4f4d\u7f6e\n            'mask_position': []\n        }\n\n        # print(f'inputs_dict--&gt;{inputs_dict}')\n        # \u521d\u59cb\u5316\u4e00\u4e2a\u7a7a\u5b57\u7b26\u4e32\uff0c\u7528\u4e8e\u6784\u5efa\u6700\u7ec8\u7684\u683c\u5f0f\u5316\u5b57\u7b26\u4e32\n        str_formated = ''\n        # \u904d\u5386\u8f93\u5165\u5217\u8868\u4e2d\u7684\u6bcf\u4e2a\u503c\n        for value in self.inputs_list:\n            # \u68c0\u67e5\u5f53\u524d\u503c\u662f\u5426\u5728custom_tokens\u4e2d\n            if value in self.custom_tokens:\n                # \u5982\u679c\u5f53\u524d\u503c\u662f'MASK'\uff0c\u4f7f\u7528mask_length\u526f\u672c\u7684inputs_dict\u4e2d\u7684\u5bf9\u5e94\u503c\n                if value == 'MASK':\n                    str_formated += inputs_dict[value] * mask_length\n                else:\n                    # \u5bf9\u4e8e\u5176\u4ed6\u81ea\u5b9a\u4e49\u503c\uff0c\u76f4\u63a5\u6dfb\u52a0inputs_dict\u4e2d\u7684\u5bf9\u5e94\u503c\n                    str_formated += inputs_dict[value]\n            else:\n                # \u5982\u679c\u5f53\u524d\u503c\u4e0d\u662fcustom_tokens\u4e2d\u7684\u503c\uff0c\u76f4\u63a5\u6dfb\u52a0\u5230\u683c\u5f0f\u5316\u5b57\u7b26\u4e32\u4e2d\n                str_formated += value\n        # \u6253\u5370\u683c\u5f0f\u5316\u540e\u7684\u5b57\u7b26\u4e32\uff0c\u7528\u4e8e\u8c03\u8bd5\u548c\u9a8c\u8bc1\n        # print(f'str_formated--&gt;{str_formated}')\n\n        # \u4f7f\u7528tokenizer\u5bf9\u683c\u5f0f\u5316\u540e\u7684\u5b57\u7b26\u4e32\u8fdb\u884c\u7f16\u7801\n        # \u7f16\u7801\u914d\u7f6e\u5305\u62ec\u622a\u65ad\u3001\u6700\u5927\u957f\u5ea6\u8bbe\u7f6e\u548c\u586b\u5145\uff0c\u4ee5\u6ee1\u8db3\u6a21\u578b\u8f93\u5165\u7684\u8981\u6c42\n        encoded = tokenizer(text=str_formated,\n                            truncation=True,\n                            max_length=max_seq_len,\n                            padding='max_length')\n        # print('*' * 80)\n        # print(f'encoded---&gt;{encoded}')\n        # \u5c06\u7f16\u7801\u540e\u7684\u8f93\u5165ID\u8d4b\u503c\u7ed9\u8f93\u51fa\u5b57\u5178\u4e2d\u7684'input_ids'\u952e\n        outputs['input_ids'] = encoded['input_ids']\n        # \u5c06\u7f16\u7801\u540e\u7684token\u7c7b\u578bID\u8d4b\u503c\u7ed9\u8f93\u51fa\u5b57\u5178\u4e2d\u7684'token_type_ids'\u952e\n        outputs['token_type_ids'] = encoded['token_type_ids']\n        # \u5c06\u7f16\u7801\u540e\u7684\u6ce8\u610f\u529b\u63a9\u7801\u8d4b\u503c\u7ed9\u8f93\u51fa\u5b57\u5178\u4e2d\u7684'attention_mask'\u952e\n        outputs['attention_mask'] = encoded['attention_mask']\n\n        # print(tokenizer.convert_ids_to_tokens(encoded['input_ids']))\n        # \u5c06\u7f16\u7801\u540e\u7684\u8f93\u5165ID\u8f6c\u6362\u4e3a\u6587\u672c\uff0c\u5e76\u5b58\u50a8\u5230\u8f93\u51fa\u5b57\u5178\u4e2d\n        outputs['text'] = ''.join(tokenizer.convert_ids_to_tokens(encoded['input_ids']))\n        # print(f'outputs--&gt;{outputs}')\n\n        # \u5c06\u63a9\u7801\u6807\u8bb0 '[MASK]' \u8f6c\u6362\u4e3a\u5176\u5bf9\u5e94\u7684ID\n        mask_token_id = tokenizer.convert_tokens_to_ids('[MASK]')\n        # print(f'mask_token_id--&gt;{mask_token_id}')\n        # print(np.array(outputs['input_ids']) == mask_token_id)\n        # print(np.where(np.array(outputs['input_ids']) == mask_token_id))\n        # \u8ba1\u7b97\u5e76\u83b7\u53d6\u8f93\u5165ID\u4e2d'mask'\u6807\u8bb0\u7684\u4f4d\u7f6e\uff0c\u5e76\u5c06\u5176\u8f6c\u6362\u4e3a\u5217\u8868\n        mask_position = np.where(np.array(outputs['input_ids']) == mask_token_id)[0].tolist()\n        # print(f'mask_position--&gt;{mask_position}')\n        # \u5c06\u8ba1\u7b97\u51fa\u7684mask_position\u6dfb\u52a0\u5230outputs\u5b57\u5178\u4e2d\n        outputs['mask_position'] = mask_position\n        return outputs\n\n\nif __name__ == '__main__':\n    # \u521b\u5efaProjectConfig\u5bf9\u8c61\u4ee5\u83b7\u53d6\u9879\u76ee\u914d\u7f6e\n    pc = ProjectConfig()\n\n    # \u6839\u636e\u9884\u8bad\u7ec3\u6a21\u578b\u914d\u7f6e\uff0c\u52a0\u8f7d\u5206\u8bcd\u5668\n    tokenizer = AutoTokenizer.from_pretrained(pc.pre_model)\n\n    # \u5b9a\u4e49\u4e00\u4e2a\u786c\u6a21\u677f\u5bf9\u8c61\uff0c\u7528\u4e8e\u6784\u5efa\u7279\u5b9a\u683c\u5f0f\u7684\u8f93\u5165\u6587\u672c\n    hard_template = HardTemplate(prompt='\u8fd9\u662f\u4e00\u6761{MASK}\u8bc4\u8bba\uff1a{textA}')\n    # \u6253\u5370\u786c\u6a21\u677f\u7684\u8f93\u5165\u5217\u8868\u548c\u81ea\u5b9a\u4e49token\u4fe1\u606f\uff0c\u4ee5\u4fbf\u8c03\u8bd5\n    print(f'inputs_list--&gt;{hard_template.inputs_list}')\n    print(f'custom_tokens--&gt;{hard_template.custom_tokens}')\n\n    # \u4f7f\u7528\u786c\u6a21\u677f\u3001\u5206\u8bcd\u5668\u548c\u6307\u5b9a\u7684\u8f93\u5165\u5b57\u5178\u6784\u5efa\u4e00\u4e2a\u6a21\u677f\u5b9e\u4f8b\n    # \u8c03\u7528\u6a21\u677f\u5bf9\u8c61, \u81ea\u52a8\u8c03\u7528__call__\u65b9\u6cd5\n    tep = hard_template(\n        inputs_dict={'textA': '\u5305\u88c5\u4e0d\u9519\uff0c\u82f9\u679c\u633a\u751c\u7684\uff0c\u4e2a\u5934\u4e5f\u5927\u3002', 'MASK': '[MASK]'},\n        tokenizer=tokenizer,\n        mask_length=2,\n        max_seq_len=30\n    )\n    print(f'tep---&gt;{tep}')\n</code></pre>"},{"location":"03-%E6%96%B0%E9%9B%B6%E5%94%AE%E8%A1%8C%E4%B8%9A%E8%AF%84%E4%BB%B7%E5%86%B3%E7%AD%96%E7%B3%BB%E7%BB%9F/03-BERT%2BPET%E6%96%B9%E5%BC%8F%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E4%BB%8B%E7%BB%8D.html#32-data_preprocesspy","title":"3.2 data_preprocess.py","text":"<ul> <li>\u76ee\u7684: \u5c06\u6837\u672c\u6570\u636e\u8f6c\u6362\u4e3a\u6a21\u578b\u63a5\u53d7\u7684\u8f93\u5165\u6570\u636e</li> <li>\u5b9a\u4e49\u6570\u636e\u8f6c\u6362\u65b9\u6cd5convert_example()\uff0c\u4ee3\u7801\u5982\u4e0b\uff1a</li> </ul> <pre><code>import numpy as np\nimport torch\nfrom datasets import load_dataset\n# partial\uff1a\u628a\u4e00\u4e2a\u51fd\u6570\u7684\u67d0\u4e9b\u53c2\u6570\u7ed9\u56fa\u5b9a\u4f4f\uff08\u4e5f\u5c31\u662f\u8bbe\u7f6e\u9ed8\u8ba4\u503c\uff09\uff0c\u8fd4\u56de\u4e00\u4e2a\u65b0\u7684\u51fd\u6570\uff0c\u8c03\u7528\u8fd9\u4e2a\u65b0\u51fd\u6570\u4f1a\u66f4\u7b80\u5355\nfrom functools import partial\nfrom transformers import AutoTokenizer\n\nfrom prompt_tasks.PET.data_handle.template import HardTemplate\nfrom prompt_tasks.PET.pet_config import ProjectConfig\n\n\ndef convert_example(\n        examples: dict,\n        tokenizer,\n        max_seq_len: int,\n        max_label_len: int,\n        hard_template: HardTemplate,\n        train_mode=True,\n        return_tensor=False) -&gt; dict:\n    '''\n    \u5c06\u6837\u672c\u6570\u636e\u8f6c\u6362\u4e3a\u6a21\u578b\u63a5\u6536\u7684\u8f93\u5165\u6570\u636e\u3002\n    :param examples: dict\u7c7b\u578b\u3002\u8bad\u7ec3\u6570\u636e\u6837\u672c,\n    e.g. -&gt; {\n            \"text\": ['\u624b\u673a    \u8fd9\u4e2a\u624b\u673a\u4e5f\u592a\u5361\u4e86\u3002',\n                     '\u4f53\u80b2    \u4e16\u754c\u676f\u4e3a\u4f55\u8fdf\u8fdf\u4e0d\u89c1\u5ba3\u4f20',\n                     ...\n            ]}\n    :param tokenizer: \u5206\u8bcd\u5668\u5bf9\u8c61\n    :param max_seq_len: int\u7c7b\u578b\u3002\u53e5\u5b50\u7684\u6700\u5927\u957f\u5ea6\uff0c\u82e5\u6ca1\u6709\u8fbe\u5230\u6700\u5927\u957f\u5ea6\uff0c\u5219padding\u4e3a\u6700\u5927\u957f\u5ea6\n    :param max_label_len: int\u7c7b\u578b\u3002\u6700\u5927label\u957f\u5ea6\uff0c\u82e5\u6ca1\u6709\u8fbe\u5230\u6700\u5927\u957f\u5ea6\uff0c\u5219padding\u4e3a\u6700\u5927\u957f\u5ea6\n    :param hard_template: HardTemplate\u7c7b\u578b\u3002\u6a21\u677f\u7c7b\n    :param train_mode: bool\u7c7b\u578b\u3002\u8bad\u7ec3\u9636\u6bb5 or \u63a8\u7406\u9636\u6bb5\n    :param return_tensor: bool\u7c7b\u578b\u3002\u662f\u5426\u8fd4\u56detensor\u7c7b\u578b\uff0c\u5982\u4e0d\u662f\uff0c\u5219\u8fd4\u56denumpy\u7c7b\u578b\u3002\n    :return:\n        dict (str: np.array) -&gt; tokenized_output = {\n                        'input_ids': [[1, 47, 10, 7, 304, 3, 3, 3, 3, 47, 27, 247, 98, 105, 512, 777, 15, 12043, 2], ...],\n                        'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], ...],\n                        'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], ...],\n                        'mask_positions': [[5, 6, 7, 8], ...],\n                        'mask_labels': [[2372, 3442, 0, 0], [2643, 4434, 2334, 0], ...]\n                    }\n    '''\n    # \u521d\u59cb\u5316\u4e00\u4e2a\u5b57\u5178\uff0c\u7528\u4e8e\u5b58\u50a8token\u5316\u7684\u8f93\u51fa\u4fe1\u606f\n    tokenized_output = {\n        'input_ids': [],  # \u8f93\u5165\u6587\u672c\u7684token ID\u5e8f\u5217\n        'token_type_ids': [],  # token\u7c7b\u578bID\u5e8f\u5217\uff0c\u7528\u4e8e\u533a\u5206\u4e0d\u540c\u53e5\u5b50\u7684token\n        'attention_mask': [],  # \u6ce8\u610f\u529b\u63a9\u7801\u5e8f\u5217\uff0c\u7528\u4e8e\u6807\u8bc6\u771f\u5b9etoken\u4e0epadding token\n        'mask_positions': [],  # mask\u6807\u7b7e\u5728\u8f93\u5165\u5e8f\u5217\u4e2d\u7684\u4f4d\u7f6e\n        'mask_labels': []  # \u9700\u8981\u9884\u6d4b\u7684mask\u6807\u7b7e\u7684\u771f\u5b9e\u503c\n    }\n    # print(f'examples--\u300b{examples}')\n    # \u904d\u5386examples\u4e2d\u7684'text'\u5217\u8868\uff0c\u83b7\u53d6\u7d22\u5f15\u548c\u6587\u672c\u5185\u5bb9\n    for i, example in enumerate(examples['text']):\n        # \u5224\u65ad\u662f\u5426\u5904\u4e8e\u8bad\u7ec3\u6a21\u5f0f\n        if train_mode:\n            # print(f'example--&gt;{example}')\n            # \u5c06\u6587\u672c\u5185\u5bb9\u6309\u5236\u8868\u7b26\u5206\u5272\uff0c\u83b7\u53d6\u6807\u7b7e\u548c\u5185\u5bb9\n            label, content = example.strip().split('\\t')\n            # print(f'label--&gt;{label}')\n            # print(f'content--&gt;{content}')\n\n            # \u4f7f\u7528tokenizer\u5bf9\u6807\u7b7e\u8fdb\u884c\u7f16\n            label_encoded = tokenizer(label, add_special_tokens=False)['input_ids']\n            # print(f'label_encoded--&gt;{label_encoded}')\n\n            # \u5982\u679c\u6807\u7b7e\u957f\u5ea6\u8d85\u8fc7\u6700\u5927\u6807\u7b7e\u957f\u5ea6, \u5c06\u6807\u7b7e\u7f16\u7801\u5e8f\u5217\u7684\u957f\u5ea6\u9650\u5236\u5728\u6700\u5927\u6807\u7b7e\u957f\u5ea6\u5185\n            if len(label_encoded) &gt;= max_label_len:\n                label_encoded = label_encoded[:max_label_len]\n            # \u5982\u679c\u6807\u7b7e\u957f\u5ea6\u5c0f\u4e8e\u6700\u5927\u6807\u7b7e\u957f\u5ea6, \u5c06\u6807\u7b7e\u7f16\u7801\u5e8f\u5217\u8fdb\u884c\u586b\u5145\uff0c\u4ee5\u786e\u4fdd\u5176\u957f\u5ea6\u4e0emax_label_len\u76f8\u7b49\n            else:\n                # \u8fd9\u91cc\u4f7f\u7528\u4e86tokenizer\u7684pad_token_id\u5c5e\u6027\u4f5c\u4e3a\u586b\u5145\u5143\u7d20\n                # print(f'tokenizer.pad_token_id--&gt;{tokenizer.pad_token_id}')\n                label_encoded = label_encoded + [tokenizer.pad_token_id] * (max_label_len - len(label_encoded))\n\n            # \u5c06\u7f16\u7801\u540e\u7684\u6807\u7b7e\u6dfb\u52a0\u5230tokenized_output\u5b57\u5178\u4e2d\u7684'mask_labels'\u5217\u8868\u4e2d\n            tokenized_output['mask_labels'].append(label_encoded)\n        else:\n            # \u5982\u679c\u4e0d\u662f\u8bad\u7ec3\u6a21\u5f0f\uff0c\u76f4\u63a5\u5c06\u6587\u672c\u5185\u5bb9\u8fdb\u884c\u4fee\u526a\u5e76\u4f7f\u7528\n            content = example.strip()\n\n        # \u521d\u59cb\u5316\u8f93\u5165\u5b57\u5178\uff0c\u7528\u4e8e\u51c6\u5907\u6587\u672c\u6570\u636e\u548c\u7279\u6b8a\u6807\u8bb0\n        inputs_dict = {\n            'textA': content,  # 'textA' \u952e\u5bf9\u5e94\u7684\u662f\u540e\u7eed\u5904\u7406\u7684\u4e3b\u8981\u6587\u672c\u5185\u5bb9\n            'MASK': '[MASK]'  # 'MASK' \u952e\u7528\u4e8e\u6807\u8bc6\u7279\u6b8a\u7684\u63a9\u7801\u6807\u8bb0\uff0c\u5e38\u7528\u4e8e\u8bed\u8a00\u6a21\u578b\u4e2d\n        }\n        # print(f'inputs_dict--&gt;{inputs_dict}')\n\n        # \u4f7f\u7528\u786c\u6a21\u677f\u7f16\u7801\u65b9\u6cd5\u5904\u7406\u8f93\u5165\u6570\u636e\n        # \u8be5\u65b9\u6cd5\u5c06\u8f93\u5165\u6570\u636e\u5b57\u5178\u3001tokenizer\u3001\u6700\u5927\u5e8f\u5217\u957f\u5ea6\u548c\u6700\u5927\u6807\u7b7e\u957f\u5ea6\u4f5c\u4e3a\u53c2\u6570\n        # \u76ee\u7684\u662f\u5c06\u8f93\u5165\u6570\u636e\u7f16\u7801\u6210\u6a21\u578b\u6240\u9700\u7684\u683c\u5f0f\n        encoded_inputs = hard_template(\n            inputs_dict=inputs_dict,\n            tokenizer=tokenizer,\n            max_seq_len=max_seq_len,\n            mask_length=max_label_len)\n        # print(f'encoded_inputs--&gt;{encoded_inputs}')\n\n        # \u5c06\u7f16\u7801\u540e\u7684\u8f93\u5165ID\u6dfb\u52a0\u5230\u8f93\u51fa\u5b57\u5178\u4e2d\u7684input_ids\u5217\u8868\n        tokenized_output['input_ids'].append(encoded_inputs[\"input_ids\"])\n        # \u5c06\u7f16\u7801\u540e\u7684token\u7c7b\u578bID\u6dfb\u52a0\u5230\u8f93\u51fa\u5b57\u5178\u4e2d\u7684token_type_ids\u5217\u8868\n        tokenized_output['token_type_ids'].append(encoded_inputs[\"token_type_ids\"])\n        # \u5c06\u7f16\u7801\u540e\u7684\u6ce8\u610f\u529b\u63a9\u7801\u6dfb\u52a0\u5230\u8f93\u51fa\u5b57\u5178\u4e2d\u7684attention_mask\u5217\u8868\n        tokenized_output['attention_mask'].append(encoded_inputs[\"attention_mask\"])\n        # \u5c06\u906e\u7f69\u4f4d\u7f6e\u4fe1\u606f\u6dfb\u52a0\u5230\u8f93\u51fa\u5b57\u5178\u4e2d\u7684mask_positions\u5217\u8868\n        tokenized_output['mask_positions'].append(encoded_inputs[\"mask_position\"])\n    # print(f'tokenized_output--&gt;{tokenized_output}')\n\n    # \u904d\u5386tokenized_output\u5b57\u5178\uff0c\u5176\u4e2dk\u662f\u952e\uff0cv\u662f\u503c\n    for k, v in tokenized_output.items():\n        # \u5982\u679creturn_tensor\u4e3aTrue\uff0c\u5c06\u503c\u8f6c\u6362\u4e3atorch.LongTensor\u7c7b\u578b\n        if return_tensor:\n            tokenized_output[k] = torch.LongTensor(v)\n        # \u5426\u5219\uff0c\u5c06\u503c\u8f6c\u6362\u4e3anumpy\u6570\u7ec4\n        else:\n            tokenized_output[k] = np.array(v)\n\n    return tokenized_output\n\n\nif __name__ == '__main__':\n    # \u521b\u5efaProjectConfig\u5bf9\u8c61\u4ee5\u83b7\u53d6\u9879\u76ee\u914d\u7f6e\n    pc = ProjectConfig()\n\n    # \u4f7f\u7528\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u5206\u8bcd\u5668\u8fdb\u884c\u521d\u59cb\u5316\n    tokenizer = AutoTokenizer.from_pretrained(pc.pre_model)\n\n    # \u5b9a\u4e49\u4e00\u4e2a\u786c\u6a21\u677f\uff0c\u7528\u4e8e\u5c06\u7279\u5b9a\u7684\u6587\u672c\u7ed3\u6784\u5316\u5230\u6a21\u578b\u8f93\u5165\u4e2d\n    # {MASK}\u7528\u4e8e\u6307\u793a\u6a21\u578b\u9700\u8981\u9884\u6d4b\u7684\u4f4d\u7f6e\uff0c{textA}\u662f\u8f93\u5165\u6587\u672c\u7684\u5360\u4f4d\u7b26\n    hard_template = HardTemplate(prompt='\u8fd9\u662f\u4e00\u6761{MASK}\u8bc4\u8bba\uff1a{textA}')\n\n    # \u5b9a\u4e49\u793a\u4f8b\u8f93\u5165\uff0c\u5305\u542b\u9700\u8981\u5904\u7406\u7684\u6587\u672c\u6570\u636e\n    # \u6bcf\u4e2a\u5143\u7d20\u662f\u4e00\u4e2a\u5305\u542b\u7c7b\u522b\u548c\u8bc4\u8bba\u6587\u672c\u7684\u5b57\u7b26\u4e32\uff0c\u7528\u5236\u8868\u7b26\u5206\u9694\n    examples = {\"text\": ['\u624b\u673a    \u8fd9\u4e2a\u624b\u673a\u4e5f\u592a\u5361\u4e86\u3002', '\u4f53\u80b2 \u4e16\u754c\u676f\u4e3a\u4f55\u8fdf\u8fdf\u4e0d\u89c1\u5ba3\u4f20']}\n    tokenized_output = convert_example(examples, tokenizer, max_seq_len=30, max_label_len=2, hard_template=hard_template)\n    print(f'tokenized_output--&gt;{tokenized_output}')\n\n    print('*' * 80)\n\n    # \u4f7f\u7528functools.partial\u51fd\u6570\u521b\u5efa\u4e00\u4e2a\u90e8\u5206\u5e94\u7528\u51fd\u6570convert_func\n    # \u6b64\u51fd\u6570\u57fa\u4e8econvert_example\u51fd\u6570\uff0c\u9884\u5148\u8bbe\u7f6e\u4e86\u4e00\u4e9b\u53c2\u6570\uff0c\u4ee5\u4fbf\u4e8e\u540e\u7eed\u7684\u8c03\u7528\u4e2d\u7b80\u5316\u64cd\u4f5c\n    # \u8fd9\u6837\u505a\u662f\u4e3a\u4e86\u4f18\u5316\u6837\u672c\u5904\u7406\u6d41\u7a0b\uff0c\u5c06\u9891\u7e41\u4f7f\u7528\u7684\u53c2\u6570\u56fa\u5b9a\u4e0b\u6765\uff0c\u63d0\u9ad8\u4ee3\u7801\u590d\u7528\u6027\u548c\u7075\u6d3b\u6027\n    convert_func = partial(convert_example,\n                           tokenizer=tokenizer,\n                           hard_template=hard_template,\n                           max_seq_len=30,\n                           max_label_len=2)\n    # \u52a0\u8f7d\u8bad\u7ec3\u6570\u636e\u96c6\n    # \u4f7f\u7528ProjectConfig\u4e2d\u5b9a\u4e49\u7684\u8bad\u7ec3\u6570\u636e\u8def\u5f84\n    train_dataset = load_dataset('text', data_files=pc.train_path)\n    print(type(train_dataset))\n    print(f'train_dataset--&gt;{train_dataset}')\n    # print(train_dataset['train'])\n    # print(train_dataset['train']['text'])\n\n    # \u4f7f\u7528map\u65b9\u6cd5\u5bf9\u8bad\u7ec3\u6570\u636e\u96c6\u8fdb\u884c\u6279\u91cf\u8f6c\u6362\n    # batched=True\u76f8\u5f53\u4e8e\u5c06train_dataset\u770b\u6210\u4e00\u4e2a\u6279\u6b21\u7684\u6837\u672c\u76f4\u63a5\u5bf9\u6570\u636e\u8fdb\u884c\u5904\u7406\uff0c\u8282\u7701\u65f6\u95f4\n    dataset = train_dataset.map(convert_func, batched=True)\n    print(f'dataset--&gt;{dataset}')\n\n    # \u904d\u5386\u6570\u636e\u96c6\u4e2d\u7684\u8bad\u7ec3\u6570\u636e\u90e8\u5206\n    for value in dataset['train']:\n        # \u6253\u5370\u5f53\u524d\u8bad\u7ec3\u6570\u636e\u793a\u4f8b\n        print(value)\n        # \u6253\u5370\u8f93\u5165ID\u5e8f\u5217\u7684\u957f\u5ea6\n        print(len(value['input_ids']))\n        # \u6253\u5370\u8f93\u5165ID\u5e8f\u5217\u7684\u6570\u636e\u7c7b\u578b\n        print(type(value['input_ids']))\n        # \u4ec5\u6253\u5370\u7b2c\u4e00\u4e2a\u8bad\u7ec3\u6570\u636e\u793a\u4f8b\u540e\u8df3\u51fa\u5faa\u73af\n        break\n</code></pre>"},{"location":"03-%E6%96%B0%E9%9B%B6%E5%94%AE%E8%A1%8C%E4%B8%9A%E8%AF%84%E4%BB%B7%E5%86%B3%E7%AD%96%E7%B3%BB%E7%BB%9F/03-BERT%2BPET%E6%96%B9%E5%BC%8F%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E4%BB%8B%E7%BB%8D.html#33-data_loaderpy","title":"3.3 data_loader.py","text":"<ul> <li>\u76ee\u7684\uff1a\u5b9a\u4e49\u6570\u636e\u52a0\u8f7d\u5668</li> <li>\u5b9a\u4e49\u83b7\u53d6\u6570\u636e\u52a0\u8f7d\u5668\u7684\u65b9\u6cd5get_data()\uff0c\u4ee3\u7801\u5982\u4e0b\uff1a</li> </ul> <pre><code>from functools import partial\nfrom datasets import load_dataset\nfrom torch.utils.data import DataLoader\nfrom transformers import AutoTokenizer, default_data_collator\n\nfrom prompt_tasks.PET.data_handle.data_preprocess import convert_example\nfrom prompt_tasks.PET.data_handle.template import HardTemplate\nfrom prompt_tasks.PET.pet_config import ProjectConfig\n\n# \u5b9e\u4f8b\u5316\u9879\u76ee\u914d\u7f6e\u6587\u4ef6\npc = ProjectConfig()\n\n# \u4f7f\u7528\u9879\u76ee\u914d\u7f6e\u6587\u4ef6\u4e2d\u6307\u5b9a\u7684\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u521d\u59cb\u5316\u4e00\u4e2a\u81ea\u52a8\u5206\u8bcd\u5668\ntokenizer = AutoTokenizer.from_pretrained(pc.pre_model)\n\ndef get_data():\n    '''\n    \u52a0\u8f7d\u8bad\u7ec3\u548c\u9a8c\u8bc1\u6570\u636e\u96c6\uff0c\u5e76\u8fdb\u884c\u9884\u5904\u7406\u4ee5\u9002\u5e94\u6a21\u578b\u8bad\u7ec3\u3002\n    \u8be5\u51fd\u6570\u9996\u5148\u8bfb\u53d6\u63d0\u793a\u6a21\u677f\u6587\u4ef6\uff0c\u7136\u540e\u4f7f\u7528\u8be5\u6a21\u677f\u521b\u5efa\u4e00\u4e2a\u786c\u6a21\u677f\u5bf9\u8c61\u3002\n    \u63a5\u7740\uff0c\u5b83\u52a0\u8f7d\u539f\u59cb\u6570\u636e\u96c6\uff0c\u5e76\u5c06\u5176\u8f6c\u6362\u4e3a\u9002\u5408\u6a21\u578b\u8bad\u7ec3\u7684\u683c\u5f0f\u3002\n    \u6700\u540e\uff0c\u5b83\u5c06\u5904\u7406\u540e\u7684\u6570\u636e\u96c6\u5305\u88c5\u5728DataLoader\u5bf9\u8c61\u4e2d\uff0c\u4ee5\u4fbf\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u65b9\u4fbf\u5730\u8bbf\u95ee\u6570\u636e\u3002\n    :return: train_dataloader: \u8bad\u7ec3\u6570\u636e\u96c6\u7684DataLoader\u5bf9\u8c61\u3002\n    dev_dataloader: \u9a8c\u8bc1\u6570\u636e\u96c6\u7684DataLoader\u5bf9\u8c61\u3002\n    '''\n    # \u8bfb\u53d6\u63d0\u793a\u6a21\u677f\u6587\u4ef6\u7684\u7b2c\u4e00\u884c\u4f5c\u4e3aprompt\n    prompt = open(pc.prompt_file, 'r', encoding='utf-8').readlines()[0].strip()  # prompt\u5b9a\u4e49\n    # print(f'prompt--&gt;{prompt}')\n\n    # \u4f7f\u7528\u8bfb\u53d6\u7684prompt\u521b\u5efa\u4e00\u4e2a\u786c\u6a21\u677f\u5bf9\u8c61\n    hard_template = HardTemplate(prompt=prompt)\n\n    # \u521b\u5efa\u4e00\u4e2a\u65b0\u51fd\u6570\uff0c\u7528\u4e8e\u5c06\u793a\u4f8b\u8f6c\u6362\u4e3a\u6a21\u578b\u8bad\u7ec3\u6240\u9700\u7684\u683c\u5f0f\n    new_func = partial(convert_example,\n                       tokenizer=tokenizer,\n                       hard_template=hard_template,\n                       max_seq_len=pc.max_seq_len,\n                       max_label_len=pc.max_label_len)\n\n    # \u52a0\u8f7d\u539f\u59cb\u6587\u672c\u6570\u636e\u96c6\n    dataset = load_dataset('text',\n                           data_files={'train': pc.train_path, 'dev': pc.dev_path})\n    # print(f'dataset--&gt;{dataset}')\n\n    # \u4f7f\u7528\u65b0\u51fd\u6570\u5bf9\u6570\u636e\u96c6\u8fdb\u884c\u6620\u5c04\uff0c\u8fdb\u884c\u6279\u91cf\u5904\u7406\n    dataset = dataset.map(new_func, batched=True)\n    # print(f'dataset\u6539\u53d8\u4e4b\u540e\u7684--&gt;{dataset}')\n\n    # \u63d0\u53d6\u8bad\u7ec3\u6570\u636e\u96c6\u548c\u9a8c\u8bc1\u6570\u636e\u96c6\n    train_dataset = dataset[\"train\"]\n    # print(f'train_dataset--&gt;{train_dataset}')\n    # print(f'train_dataset[0]--&gt;{train_dataset[0]}')\n    dev_dataset = dataset[\"dev\"]\n    # print(f'dev_dataset--&gt;{dev_dataset}')\n    # print('dev_dataset', dev_dataset[0])\n\n    # \u4f7f\u7528default_data_collator\u5c06\u6570\u636e\u8f6c\u6362\u4e3atensor\u6570\u636e\u7c7b\u578b\n    train_dataloader = DataLoader(train_dataset,\n                                  shuffle=True,\n                                  collate_fn=default_data_collator,\n                                  batch_size=pc.batch_size)\n    # print(f'train_dataloader--&gt;{train_dataloader}')\n    dev_dataloader = DataLoader(dev_dataset,\n                                collate_fn=default_data_collator,\n                                batch_size=pc.batch_size)\n\n    # \u8fd4\u56de\u5904\u7406\u540e\u7684\u8bad\u7ec3\u548c\u9a8c\u8bc1\u6570\u636e\u96c6\u7684DataLoader\u5bf9\u8c61\n    return train_dataloader, dev_dataloader\n\n\nif __name__ == '__main__':\n    # \u83b7\u53d6\u8bad\u7ec3\u548c\u9a8c\u8bc1\u6570\u636e\u96c6\u7684\u52a0\u8f7d\u5668\n    train_dataloader, dev_dataloader = get_data()\n    print(len(train_dataloader))\n    print(len(dev_dataloader))\n\n    # \u904d\u5386\u8bad\u7ec3\u6570\u636e\u96c6\u52a0\u8f7d\u5668\n    for i, value in enumerate(train_dataloader):\n        print(f'i---&gt;{i}')\n        print(f'value---&gt;{value}')\n        # \u6253\u5370\u5f53\u524d\u6570\u636e\u9879\u4e2d'input_ids'\u7684\u6570\u636e\u7c7b\u578b\n        print(value['input_ids'].dtype)\n        break\n</code></pre>"},{"location":"03-%E6%96%B0%E9%9B%B6%E5%94%AE%E8%A1%8C%E4%B8%9A%E8%AF%84%E4%BB%B7%E5%86%B3%E7%AD%96%E7%B3%BB%E7%BB%9F/04-BERT%2BPET%E6%96%B9%E5%BC%8F%E6%A8%A1%E5%9E%8B%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0%E4%B8%8E%E8%AE%AD%E7%BB%83.html","title":"BERT+PET\u65b9\u5f0f\u6a21\u578b\u4ee3\u7801\u5b9e\u73b0\u548c\u8bad\u7ec3","text":""},{"location":"03-%E6%96%B0%E9%9B%B6%E5%94%AE%E8%A1%8C%E4%B8%9A%E8%AF%84%E4%BB%B7%E5%86%B3%E7%AD%96%E7%B3%BB%E7%BB%9F/04-BERT%2BPET%E6%96%B9%E5%BC%8F%E6%A8%A1%E5%9E%8B%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0%E4%B8%8E%E8%AE%AD%E7%BB%83.html#_1","title":"\u5b66\u4e60\u76ee\u6807","text":"<ul> <li>\u638c\u63e1\u57fa\u4e8eBERT+PET\u65b9\u5f0f\u6a21\u578b\u642d\u5efa\u4ee3\u7801\u7684\u5b9e\u73b0.</li> <li>\u638c\u63e1\u6a21\u578b\u7684\u8bad\u7ec3,\u9a8c\u8bc1\u53ca\u76f8\u5173\u5de5\u5177\u4ee3\u7801\u7684\u5b9e\u73b0.</li> <li>\u638c\u63e1\u4f7f\u7528\u6a21\u578b\u9884\u6d4b\u4ee3\u7801\u7684\u5b9e\u73b0.</li> </ul>"},{"location":"03-%E6%96%B0%E9%9B%B6%E5%94%AE%E8%A1%8C%E4%B8%9A%E8%AF%84%E4%BB%B7%E5%86%B3%E7%AD%96%E7%B3%BB%E7%BB%9F/04-BERT%2BPET%E6%96%B9%E5%BC%8F%E6%A8%A1%E5%9E%8B%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0%E4%B8%8E%E8%AE%AD%E7%BB%83.html#_2","title":"\u6a21\u578b\u642d\u5efa","text":"<ul> <li>\u672c\u9879\u76ee\u4e2d\u5b8c\u6210BERT+PET\u6a21\u578b\u642d\u5efa\u3001\u8bad\u7ec3\u53ca\u5e94\u7528\u7684\u6b65\u9aa4\u5982\u4e0b\uff08\u6ce8\u610f\uff1a\u56e0\u4e3a\u672c\u9879\u76ee\u4e2d\u4f7f\u7528\u7684\u662fBERT\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u6240\u4ee5\u76f4\u63a5\u52a0\u8f7d\u5373\u53ef\uff0c\u65e0\u9700\u91cd\u590d\u642d\u5efa\u6a21\u578b\u67b6\u6784\uff09:</li> <li>1.\u5b9e\u73b0\u6a21\u578b\u5de5\u5177\u7c7b\u51fd\u6570</li> <li>2.\u5b9e\u73b0\u6a21\u578b\u8bad\u7ec3\u51fd\u6570,\u9a8c\u8bc1\u51fd\u6570</li> <li>3.\u5b9e\u73b0\u6a21\u578b\u9884\u6d4b\u51fd\u6570</li> </ul>"},{"location":"03-%E6%96%B0%E9%9B%B6%E5%94%AE%E8%A1%8C%E4%B8%9A%E8%AF%84%E4%BB%B7%E5%86%B3%E7%AD%96%E7%B3%BB%E7%BB%9F/04-BERT%2BPET%E6%96%B9%E5%BC%8F%E6%A8%A1%E5%9E%8B%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0%E4%B8%8E%E8%AE%AD%E7%BB%83.html#1","title":"1. \u5b9e\u73b0\u6a21\u578b\u5de5\u5177\u7c7b\u51fd\u6570","text":"<ul> <li>\u76ee\u7684\uff1a\u6a21\u578b\u5728\u8bad\u7ec3\u3001\u9a8c\u8bc1\u3001\u9884\u6d4b\u65f6\u9700\u8981\u7684\u51fd\u6570</li> <li>\u4ee3\u7801\u8def\u5f84\uff1allm_tuning/prompt_tasks/PET/utils<ul> <li>utils\u6587\u4ef6\u5939\u5171\u5305\u542b3\u4e2apy\u811a\u672c\uff1averbalizer.py\u3001metirc_utils.py\u4ee5\u53cacommon_utils.py</li> </ul> </li> </ul>"},{"location":"03-%E6%96%B0%E9%9B%B6%E5%94%AE%E8%A1%8C%E4%B8%9A%E8%AF%84%E4%BB%B7%E5%86%B3%E7%AD%96%E7%B3%BB%E7%BB%9F/04-BERT%2BPET%E6%96%B9%E5%BC%8F%E6%A8%A1%E5%9E%8B%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0%E4%B8%8E%E8%AE%AD%E7%BB%83.html#11-verbalizerpy","title":"1.1 verbalizer.py","text":"<ul> <li>\u76ee\u7684\uff1a\u5b9a\u4e49\u4e00\u4e2aVerbalizer\u7c7b\uff0c\u7528\u4e8e\u5c06\u4e00\u4e2a\u4e3b\u6807\u7b7e\u6620\u5c04\u5230\u5b50\u6807\u7b7e\u6216\u8005\u5c06\u5b50\u6807\u7b7e\u6620\u5c04\u5230\u4e3b\u6807\u7b7e\u3002</li> <li>\u5177\u4f53\u5b9e\u73b0\u4ee3\u7801\uff1a</li> </ul> <pre><code># Union \u662f typing \u6a21\u5757\u4e2d\u5b9a\u4e49\u7684\u4e00\u4e2a\u7c7b,\u7528\u4e8e\u8868\u793a\u591a\u4e2a\u7c7b\u578b\u4e2d\u7684\u4efb\u610f\u4e00\u79cd\u7c7b\u578b\nfrom typing import Union, List\nfrom transformers import AutoTokenizer\n\nfrom prompt_tasks.PET.pet_config import ProjectConfig\n\npc = ProjectConfig()\n\n\n# Verbalizer\u7c7b\uff0c\u7528\u4e8e\u5c06\u4e00\u4e2aLabel\u5bf9\u5e94\u5230\u5176\u5b50Label\u7684\u6620\u5c04\u3002\nclass Verbalizer(object):\n    def __init__(self,\n                 verbalizer_file: str,\n                 tokenizer, max_label_len: int\n                 ):\n        '''\n        :param verbalizer_file: verbalizer\u6587\u4ef6\u5b58\u653e\u5730\u5740\u3002\n        :param tokenizer: \u7528\u4e8e\u6587\u672c\u548cid\u4e4b\u95f4\u7684\u8f6c\u6362\u3002\n        :param max_label_len: \u6807\u7b7e\u957f\u5ea6\uff0c\u82e5\u5927\u4e8e\u5219\u622a\u65ad\uff0c\u82e5\u5c0f\u4e8e\u5219\u8865\u9f50\n        '''\n        self.tokenizer = tokenizer\n        self.label_dict = self.load_label_dict(verbalizer_file)\n        self.max_label_len = max_label_len\n\n    def load_label_dict(self, verbalizer_file: str):\n        '''\n        \u8bfb\u53d6\u672c\u5730\u6587\u4ef6\uff0c\u6784\u5efaverbalizer\u5b57\u5178\u3002\n        :param verbalizer_file: verbalizer\u6587\u4ef6\u5b58\u653e\u5730\u5740\u3002\n        :return:\n        dict -&gt; {\n            '\u4f53\u80b2': ['\u7bee\u7403', '\u8db3\u7403','\u7f51\u7403', '\u6392\u7403',  ...],\n            '\u9152\u5e97': ['\u5bbe\u9986', '\u65c5\u9986', '\u65c5\u5e97', '\u9152\u5e97', ...],\n            ...\n            }\n        '''\n        # \u521d\u59cb\u5316\u4e00\u4e2a\u7a7a\u5b57\u5178\uff0c\u7528\u4e8e\u5b58\u50a8\u6807\u7b7e\u548c\u5b50\u6807\u7b7e\u7684\u5173\u7cfb\n        label_dict = {}\n\n        # \u6253\u5f00verbalizer\u6587\u4ef6\uff0c\u4ee5\u53ea\u8bfb\u6a21\u5f0f\uff0c\u4f7f\u7528utf8\u7f16\u7801\n        with open(verbalizer_file, 'r', encoding='utf-8') as f:\n            # \u8bfb\u53d6\u6587\u4ef6\u7684\u6bcf\u4e00\u884c\n            for line in f:\n                # \u79fb\u9664\u884c\u5c3e\u7684\u6362\u884c\u7b26\uff0c\u5e76\u6309\u5236\u8868\u7b26('\\t')\u5206\u5272\u6807\u7b7e\u548c\u5b50\u6807\u7b7e\n                label, sub_labels = line.strip().split('\\t')\n                # \u5c06\u5b50\u6807\u7b7e\u6309\u9017\u53f7(,)\u5206\u5272\u6210\u5217\u8868\uff0c\u4f7f\u7528set\u53bb\u91cd\u540e\u518d\u8f6c\u56de\u5217\u8868\uff0c\u5b58\u50a8\u5230label_dict\u4e2d\n                label_dict[label] = list(set(sub_labels.split(',')))\n        # \u8fd4\u56de\u5904\u7406\u540e\u7684\u6807\u7b7e\u548c\u5b50\u6807\u7b7e\u7684\u5b57\u5178\n        return label_dict\n\n    def find_sub_labels(self, label: Union[list, str]):\n        '''\n        \u901a\u8fc7\u4e3b\u6807\u7b7e\u627e\u5230\u6240\u6709\u7684\u5b50\u6807\u7b7e\u3002\n        :param label: \u6807\u7b7e, \u6587\u672c\u578b \u6216 id_list, e.g. -&gt; '\u4f53\u80b2' or [860, 5509]\n        :return:\n        dict -&gt; {\n            'sub_labels': ['\u8db3\u7403', '\u7f51\u7403'],\n            'token_ids': [[6639, 4413], [5381, 4413]]\n        }\n        '''\n        # \u5982\u679c\u4f20\u5165\u7684label\u4e3aid\u5217\u8868\uff0c\u5219\u901a\u8fc7tokenizer\u8f6c\u6362\u56de\u5b57\u7b26\u4e32\n        if type(label) == list:\n            # \u79fb\u9664label\u4e2d\u7684pad_token_id\uff0c\u76f4\u5230label\u4e2d\u4e0d\u518d\u5305\u542b\u5b83\n            while self.tokenizer.pad_token_id in label:\n                label.remove(self.tokenizer.pad_token_id)\n            # \u5c06\u5904\u7406\u540e\u7684id\u5217\u8868\u8f6c\u6362\u4e3atokens\uff0c\u5e76\u62fc\u63a5\u6210\u5b57\u7b26\u4e32\n            label = ''.join(self.tokenizer.convert_ids_to_tokens(label))\n        # print(f'label--&gt;{label}')\n        # \u68c0\u67e5\u8f6c\u6362\u540e\u7684label\u662f\u5426\u5728\u6807\u7b7e\u5b57\u5178\u4e2d\uff0c\u5982\u679c\u4e0d\u5728\u5219\u629b\u51fa\u5f02\u5e38\n        if label not in self.label_dict:\n            raise ValueError(f'Lable Error: \"{label}\" \u4e0d\u5728 label_dict {list(self.label_dict)}.')\n\n        # \u4ece\u6807\u7b7e\u5b57\u5178\u4e2d\u83b7\u53d6\u4e0elabel\u5bf9\u5e94\u7684\u5b50\u6807\u7b7e\n        sub_labels = self.label_dict[label]\n        # print(f'sub_labels--&gt;{sub_labels}')\n        # \u5c06\u5b50\u6807\u7b7e\u4f5c\u4e3a\u7ed3\u679c\u7684\u4e00\u4e2a\u90e8\u5206\u5b58\u50a8\u5728\u5b57\u5178\u4e2d\n        ret = {'sub_labels': sub_labels}\n\n        # \u5bf9\u6bcf\u4e2a\u5b50\u6807\u7b7e\u8fdb\u884ctoken\u5316\uff0c\u4e0d\u542b\u7279\u6b8a\u7b26\u53f7\n        token_ids = [token_id for token_id in self.tokenizer(sub_labels, add_special_tokens=False)['input_ids']]\n        # print(f'token_ids--&gt;{token_ids}')\n        # \u904d\u5386\u6240\u6709\u7684token_ids\uff0c\u8fdb\u884c\u622a\u65ad\u4e0e\u8865\u9f50\u64cd\u4f5c\n        for i in range(len(token_ids)):\n            # \u5bf9\u6807\u7b7e\u8fdb\u884c\u622a\u65ad\n            token_ids[i] = token_ids[i][:self.max_label_len]\n            # \u5982\u679c\u957f\u5ea6\u4e0d\u8db3max_label_len\uff0c\u5219\u4f7f\u7528pad_token_id\u8fdb\u884c\u8865\u9f50\n            if len(token_ids[i]) &lt; self.max_label_len:\n                token_ids[i] = token_ids[i] + [self.tokenizer.pad_token_id] * (self.max_label_len - len(token_ids[i]))\n        # \u5c06\u5904\u7406\u540e\u7684token_ids\u5b58\u5165ret\u5b57\u5178\u4e2d\n        ret['token_ids'] = token_ids\n        return ret\n\n    def batch_find_sub_labels(self, label: List[Union[list, str]]):\n        '''\n        \u6279\u91cf\u627e\u5230\u5b50\u6807\u7b7e\u3002\n        :param label: \u6807\u7b7e\u5217\u8868, [[4510, 5554], [860, 5509]] or ['\u4f53\u80b2', '\u7535\u8111']\n        :return:\n        list -&gt; [\n                {\n                    'sub_labels': ['\u7b14\u8bb0\u672c', '\u7535\u8111'],\n                    'token_ids': [[5011, 6381, 3315], [4510, 5554]]\n                },\n                ...\n            ]\n        '''\n        return [self.find_sub_labels(l) for l in label]\n\n    def get_common_sub_str(self,\n                           str1: str,\n                           str2: str\n                           ):\n        '''\n        \u5bfb\u627e\u6700\u5927\u516c\u5171\u5b50\u4e32(\u8fde\u7eed\u5b50\u5e8f\u5217)\u3002\n        :param str1: abcd\n        :param str2: abadbcdba\n        :return:\n        '''\n        # \u521d\u59cb\u5316\u4e24\u4e2a\u5b57\u7b26\u4e32\u7684\u957f\u5ea6\n        lstr1, lstr2 = len(str1), len(str2)\n        # \u751f\u62100\u77e9\u9635\uff0c\u4e3a\u65b9\u4fbf\u540e\u7eed\u8ba1\u7b97\uff0c\u6bd4\u5b57\u7b26\u4e32\u957f\u5ea6\u591a\u4e86\u4e00\u5217\uff0c\u751f\u6210\u4e00\u4e2a lstr1+1 * lstr2+1 \u7684\u4e8c\u7ef4\u77e9\u9635\n        record = [[0 for i in range(lstr2 + 1)] for j in range(lstr1 + 1)]\n        # \u521d\u59cb\u5316\u6700\u957f\u5339\u914d\u5bf9\u5e94\u5728str1\u4e2d\u7684\u6700\u540e\u4e00\u4f4d\n        p = 0\n        # \u521d\u59cb\u5316\u6700\u957f\u5339\u914d\u957f\u5ea6\n        maxNum = 0\n        # \u904d\u5386\u4e24\u4e2a\u5b57\u7b26\u4e32\uff0c\u5bfb\u627e\u6700\u957f\u516c\u5171\u5b50\u4e32\n        for i in range(1, lstr1 + 1):\n            for j in range(1, lstr2 + 1):\n                # \u5f53\u53d1\u73b0\u76f8\u540c\u5b57\u7b26\u65f6\n                if str1[i - 1] == str2[j - 1]:\n                    # \u5728record\u77e9\u9635\u4e2d\u8bb0\u5f55\u5339\u914d\u957f\u5ea6\n                    record[i][j] = record[i - 1][j - 1] + 1\n                    # \u66f4\u65b0\u6700\u957f\u5339\u914d\u957f\u5ea6\u548c\u5bf9\u5e94\u5728str1\u4e2d\u7684\u6700\u540e\u4e00\u4f4d\n                    if record[i][j] &gt; maxNum:\n                        maxNum = record[i][j]\n                        p = i\n\n        # \u8fd4\u56de\u6700\u957f\u516c\u5171\u5b50\u4e32\u548c\u5176\u957f\u5ea6\n        return str1[p - maxNum:p], maxNum\n\n    def hard_mapping(self, sub_label: str):\n        '''\n        \u5f3a\u5339\u914d\u51fd\u6570\uff0c\u5f53\u6a21\u578b\u751f\u6210\u7684\u5b50label\u4e0d\u5b58\u5728\u65f6\uff0c\u901a\u8fc7\u6700\u5927\u516c\u5171\u5b50\u4e32\u627e\u5230\u91cd\u5408\u5ea6\u6700\u9ad8\u7684\u4e3blabel\u3002\n        :param sub_label: \u5b50label\n        :return: \u4e3blabel\n        '''\n        # \u521d\u59cb\u5316\u53d8\u91cflabel\u548cmax_overlap_str\uff0c\u7528\u4e8e\u8bb0\u5f55\u6700\u5927\u91cd\u53e0\u5ea6\u7684\u6807\u7b7e\u548c\u5bf9\u5e94\u7684\u91cd\u53e0\u5ea6\u503c\n        label, max_overlap_str = '', 0\n\n        # \u904d\u5386\u6807\u7b7e\u5b57\u5178\uff0c\u5176\u4e2dmain_label\u662f\u4e3b\u6807\u7b7e\uff0csub_labels\u662f\u4e0e\u4e3b\u6807\u7b7e\u76f8\u5173\u7684\u5b50\u6807\u7b7e\u5217\u8868\n        for main_label, sub_labels in self.label_dict.items():\n            overlap_num = 0\n            # \u5bf9\u4e8e\u6bcf\u4e2a\u5b50\u6807\u7b7e\uff0c\u8ba1\u7b97\u5b83\u4e0e\u5f53\u524d\u63a8\u7406\u6807\u7b7e\u4e4b\u95f4\u7684\u6700\u957f\u516c\u5171\u5b50\u4e32\u957f\u5ea6\u603b\u548c\n            for s_label in sub_labels:\n                # \u7d2f\u52a0\u6bcf\u4e2a\u5b50\u6807\u7b7e\u4e0e\u5f53\u524d\u63a8\u7406\u6807\u7b7e\u4e4b\u95f4\u7684\u6700\u957f\u516c\u5171\u5b50\u4e32\u957f\u5ea6\n                overlap_num += self.get_common_sub_str(sub_label, s_label)[1]\n\n            # \u5982\u679c\u5f53\u524d\u7684\u91cd\u53e0\u5ea6\u5927\u4e8e\u6216\u7b49\u4e8e\u4e4b\u524d\u7684\u6700\u5927\u91cd\u53e0\u5ea6\uff0c\u5219\u66f4\u65b0\u6700\u5927\u91cd\u53e0\u5ea6\u548c\u5bf9\u5e94\u7684\u6807\u7b7e\n            if overlap_num &gt;= max_overlap_str:\n                max_overlap_str = overlap_num\n                label = main_label\n\n        return label\n\n    def find_main_label(self,\n                        sub_label: Union[list, str],\n                        hard_mapping=True\n                        ):\n        '''\n        \u901a\u8fc7\u5b50\u6807\u7b7e\u627e\u5230\u7236\u6807\u7b7e\u3002\n        :param sub_label: \u5b50\u6807\u7b7e, \u6587\u672c\u578b \u6216 id_list, e.g. -&gt; '\u82f9\u679c' or [5741, 3362]\n        :param hard_mapping: \u5f53\u751f\u6210\u7684\u8bcd\u8bed\u4e0d\u5b58\u5728\u65f6\uff0c\u662f\u5426\u4e00\u5b9a\u8981\u5339\u914d\u5230\u4e00\u4e2a\u6700\u76f8\u4f3c\u7684label\u3002\n        :return:\n        dict -&gt; {\n            'label': '\u6c34\u679c',\n            'token_ids': [3717, 3362]\n        }\n        '''\n        # \u5982\u679c\u4f20\u5165\u7684sub_label\u4e3aid\u5217\u8868\uff0c\u5219\u901a\u8fc7tokenizer\u8f6c\u6362\u56de\u5b57\u7b26\u4e32\n        if type(sub_label) == list:\n            pad_token_id = self.tokenizer.pad_token_id\n            # \u79fb\u9664\u5217\u8868\u4e2d\u7684[PAD]token\uff0c\u907f\u514d\u5f71\u54cd\u540e\u7eed\u5904\u7406\n            while pad_token_id in sub_label:\n                sub_label.remove(pad_token_id)\n            # \u5c06id\u5217\u8868\u8f6c\u6362\u4e3a\u5bf9\u5e94\u7684\u5b57\u7b26\u4e32\n            sub_label = ''.join(self.tokenizer.convert_ids_to_tokens(sub_label))\n        # print(f'sub_label--&gt;{sub_label}')\n        # \u521d\u59cb\u5316\u4e3b\u6807\u7b7e\u4e3a'\u65e0'\uff0c\u4f5c\u4e3a\u672a\u627e\u5230\u7279\u5b9a\u5b50\u6807\u7b7e\u65f6\u7684\u9ed8\u8ba4\u503c\n        main_label = '\u65e0'\n\n        # \u904d\u5386\u6807\u7b7e\u5b57\u5178\uff0c\u5bfb\u627e\u4e0e\u5b50\u6807\u7b7e\u5339\u914d\u7684\u4e3b\u6807\u7b7e\n        for label, sub_labels in self.label_dict.items():\n            # \u68c0\u67e5\u5f53\u524d\u5b50\u6807\u7b7e\u662f\u5426\u5728\u5b57\u5178\u4e2d\u5bf9\u5e94\u7684\u5b50\u6807\u7b7e\u5217\u8868\u4e2d\n            if sub_label in sub_labels:\n                # \u5f53\u627e\u5230\u5339\u914d\u65f6\uff0c\u66f4\u65b0\u4e3b\u6807\u7b7e\u5e76\u7ec8\u6b62\u5faa\u73af\n                main_label = label\n                break\n        # print(f'main_label--&gt;{main_label}')\n        # \u5982\u679c\u4e3b\u6807\u7b7e\u4e3a'\u65e0'\u4e14\u542f\u7528\u4e86\u5f3a\u5339\u914d\u529f\u80fd\uff0c\u5219\u4f7f\u7528\u5f3a\u5339\u914d\u65b9\u6cd5\u66f4\u65b0\u4e3b\u6807\u7b7e\n        if main_label == '\u65e0' and hard_mapping:\n            main_label = self.hard_mapping(sub_label)\n        # print('\u5f3a\u5339\u914d', main_label)\n        ret = {\n            'label': main_label,\n            'token_ids': self.tokenizer(main_label, add_special_tokens=False)['input_ids']\n        }\n        return ret\n\n    def batch_find_main_label(self,\n                              sub_label: List[Union[list, str]],\n                              hard_mapping=True\n                              ):\n        '''\n        \u6279\u91cf\u901a\u8fc7\u5b50\u6807\u7b7e\u627e\u7236\u6807\u7b7e\u3002\n        :param sub_label: \u5b50\u6807\u7b7e\u5217\u8868, ['\u82f9\u679c', ...] or [[5741, 3362], ...]\n        :param hard_mapping:\n        :return:\n        list: [\n                {\n                'label': '\u6c34\u679c',\n                'token_ids': [3717, 3362]\n                },\n                ...\n        ]\n        '''\n        return [self.find_main_label(l, hard_mapping) for l in sub_label]\n\n\nif __name__ == '__main__':\n    tokenizer = AutoTokenizer.from_pretrained(pc.pre_model)\n    verbalizer = Verbalizer(\n        verbalizer_file=pc.verbalizer,\n        tokenizer=tokenizer,\n        max_label_len=2)\n    print(f'label_dict--&gt;{verbalizer.label_dict}')\n\n    # \u67e5\u627e\u5355\u4e2a\u5b50\u6807\u7b7e\n    label = '\u7535\u8111'\n    ret = verbalizer.find_sub_labels(label)\n    print(f'ret--&gt;{ret}')\n    print('*' * 80)\n\n    # \u67e5\u627e\u591a\u4e2a\u5b50\u6807\u7b7e\n    labels = ['\u7535\u8111', '\u8863\u670d']\n    # labels = [[4510, 5554], [6132, 3302]]\n    result = verbalizer.batch_find_sub_labels(labels)\n    print(f'result--&gt;{result}')\n    print('*' * 80)\n\n    # \u67e5\u627e\u5355\u4e2a\u5b50\u6807\u7b7e\u5bf9\u5e94\u7684\u7236\u6807\u7b7e\n    # sub_label = [4510, 5554]\n    sub_label = '\u8863\u7535'\n    ret = verbalizer.find_main_label(sub_label)\n    print(f'ret--&gt;{ret}')\n    print('*' * 80)\n\n    # \u67e5\u627e\u591a\u4e2a\u5b50\u6807\u7b7e\u5bf9\u5e94\u7684\u7236\u6807\u7b7e\n    # sub_label = ['\u8863\u670d', '\u725b\u5976']\n    sub_label = [[6132, 3302], [5885, 4281]]\n    ret = verbalizer.batch_find_main_label(sub_label, hard_mapping=True)\n    print(f'ret--&gt;{ret}')\n</code></pre>"},{"location":"03-%E6%96%B0%E9%9B%B6%E5%94%AE%E8%A1%8C%E4%B8%9A%E8%AF%84%E4%BB%B7%E5%86%B3%E7%AD%96%E7%B3%BB%E7%BB%9F/04-BERT%2BPET%E6%96%B9%E5%BC%8F%E6%A8%A1%E5%9E%8B%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0%E4%B8%8E%E8%AE%AD%E7%BB%83.html#12-common_utilspy","title":"1.2 common_utils.py","text":"<ul> <li>\u76ee\u7684\uff1a\u5b9a\u4e49\u635f\u5931\u51fd\u6570\u3001\u5c06mask_position\u4f4d\u7f6e\u7684token logits\u8f6c\u6362\u4e3atoken\u7684id\u3002</li> <li>\u811a\u672c\u91cc\u9762\u5305\u542b\u4e24\u4e2a\u51fd\u6570\uff1amlm_loss()\u4ee5\u53caconvert_logits_to_ids()</li> <li>\u4ee3\u7801\u5982\u4e0b\uff1a</li> </ul> <pre><code>import torch\n\ndef mlm_loss(logits,\n             mask_positions,\n             sub_mask_labels,\n             cross_entropy_criterion,\n             device):\n    '''\n    \u8ba1\u7b97\u6307\u5b9a\u4f4d\u7f6e\u7684mask token\u7684output\u4e0elabel\u4e4b\u95f4\u7684cross entropy loss\u3002\n    :param logits: (torch.tensor): \u6a21\u578b\u539f\u59cb\u8f93\u51fa -&gt; (batch_size, seq_len, vocab_size)\n    :param mask_positions: (torch.tensor): mask token\u7684\u4f4d\u7f6e  -&gt; (batch_size, mask_label_num)\n    :param sub_mask_labels: (list): mask token\u7684sub label, \u7531\u4e8e\u6bcf\u4e2alabel\u7684sub_label\u6570\u76ee\u4e0d\u540c\uff0c\u6240\u4ee5\u8fd9\u91cc\u662f\u4e2a\u53d8\u957f\u7684list,\n                                    e.g. -&gt; [\n                                        [[2398, 3352]],\n                                        [[2398, 3352], [3819, 3861]]\n                                    ]\n    :param cross_entropy_criterion: (CrossEntropyLoss): CE Loss\u8ba1\u7b97\u5668\n    :param device: (str): cpu\u8fd8\u662fgpu\n    :return: CE Loss\n    '''\n    '''\n    \u83b7\u53d6logits\u7684\u5c3a\u5bf8\u4fe1\u606f\uff0c\u4e3a\u540e\u7eed\u8ba1\u7b97\u505a\u51c6\u5907\n    logits.size()\u8fd4\u56de\u4e00\u4e2a\u5305\u542b\u4e09\u4e2a\u7ef4\u5ea6\u7684\u5143\u7ec4\n    \u7b2c\u4e00\u4e2a\u7ef4\u5ea6(batch_size)\u4ee3\u8868\u6279\u6b21\u5927\u5c0f\uff0c\u5373\u4e00\u6b21\u5904\u7406\u7684\u6570\u636e\u6279\u6b21\u5305\u542b\u7684\u6837\u672c\u6570\u91cf\n    \u7b2c\u4e8c\u4e2a\u7ef4\u5ea6(seq_len)\u4ee3\u8868\u5e8f\u5217\u957f\u5ea6\uff0c\u5373\u6bcf\u4e2a\u6837\u672c\u4e2d\u5305\u542b\u7684\u5e8f\u5217\u5143\u7d20\u6570\u91cf\n    \u7b2c\u4e09\u4e2a\u7ef4\u5ea6(vocab_size)\u4ee3\u8868\u8bcd\u6c47\u8868\u5927\u5c0f\uff0c\u5373\u6bcf\u4e2a\u5e8f\u5217\u5143\u7d20\u53ef\u80fd\u7684\u7c7b\u522b\u6570\u91cf\n    '''\n    batch_size, seq_len, vocab_size = logits.size()\n    # print(f'\u6a21\u578b\u9884\u6d4b\u7ed3\u679clogits--&gt;{logits.size()}')\n    # print(f'mask_positions--&gt;{mask_positions.shape}')\n    # print(f'sub_mask_labels--&gt;{sub_mask_labels}')\n\n    # \u521d\u59cb\u5316loss\u53d8\u91cf\u4e3aNone\uff0c\u7528\u4e8e\u540e\u7eed\u53ef\u80fd\u7684\u635f\u5931\u8ba1\u7b97\n    loss = None\n    # \u904d\u5386 logits\u3001sub_mask_labels \u548c mask_positions \u7684\u5143\u7d20\n    for single_value in zip(logits, sub_mask_labels, mask_positions):\n        # \u83b7\u53d6\u5f53\u524dtoken\u7684 logits\n        single_logits = single_value[0]\n        # print(f'single_logits--&gt;{single_logits.shape}')  # \u5f62\u72b6[512, 21128]\n        # \u83b7\u53d6\u5f53\u524dtoken\u7684 sub_mask_labels\n        single_sub_mask_labels = single_value[1]\n        # print(f'single_sub_mask_labels--&gt;{single_sub_mask_labels}')\n        # \u83b7\u53d6\u5f53\u524dtoken\u7684 mask_positions\n        single_mask_positions = single_value[2]\n        # print(f'single_mask_positions--&gt;{single_mask_positions}')  # \u5f62\u72b6size[2]--&gt;\u5177\u4f53\u503c([5, 6])\n\n        # \u4ece\u5355\u4e2a\u5e8f\u5217\u7684logits\u4e2d\uff0c\u63d0\u53d6\u51fa\u88ab\u63a9\u7801\u4f4d\u7f6e\u7684logits\n        single_mask_logits = single_logits[single_mask_positions]  # (mask_label_num, vocab_size)\n        # \u6253\u5370\u88ab\u63a9\u7801\u4f4d\u7f6elogits\u7684\u5f62\u72b6\uff0c\u4ee5\u9a8c\u8bc1\u5176\u662f\u5426\u7b26\u5408\u9884\u671f\n        # print(f'single_mask_logits--&gt;{single_mask_logits.shape}')\n\n        # \u6a21\u578b\u8bad\u7ec3\u65f6\u4e3b\u6807\u7b7e\u5bf9\u5e94\u7684\u6240\u6709\u5b50\u6807\u7b7e\u90fd\u6709\u76f8\u4f3c\u7684\u7279\u5f81\u503c, \u5728\u8ba1\u7b97CE Loss\u65f6\uff0c\u9700\u8981\u5c06\u6bcf\u4e2a\u5b50\u6807\u7b7e\u7684\u5bf9\u5e94\u7684\u635f\u5931\u6c42\u5e73\u5747\uff0c\u56e0\u6b64\u9700\u8981\u5c06\u9884\u6d4b\u7684\u6982\u7387\u503c\u8fdb\u884c\u6269\u5c55\n        # \u5bf9\u5355\u4e2a single_mask_logits \u8fdb\u884c\u6269\u5c55\uff0c\u4f7f\u5176\u5728\u7b2c\u4e00\u4e2a\u7ef4\u5ea6\u4e0a\u91cd\u590d\uff0c\u4ee5\u5339\u914d single_sub_mask_labels \u7684\u6570\u91cf\n        # \u4f7f\u7528repeat\u8bbe\u7f6e\u91cd\u590d\u7684\u500d\u6570 (sub_label_num, mask_label_num, vocab_size)\n        single_mask_logits = single_mask_logits.repeat(len(single_sub_mask_labels), 1, 1)\n        # \u6253\u5370\u91cd\u590d\u540e\u7684single_mask_logits\u7684\u5f62\u72b6\uff0c\u4ee5\u4fbf\u8c03\u8bd5\u548c\u9a8c\u8bc1\u91cd\u590d\u64cd\u4f5c\u7684\u6548\u679c\n        # print(f'\u91cd\u590d\u540e\u7684single_mask_logits--&gt;{single_mask_logits.shape}')\n\n        # \u5c06\u4e09\u7ef4\u5f20\u91cf\u8c03\u6574\u4e3a\u4e8c\u7ef4\uff0c\u4ee5\u4fbf\u8ba1\u7b97\u635f\u5931\n        single_mask_logits = single_mask_logits.reshape(-1, vocab_size)  # (sub_label_num * mask_label_num, vocab_size)\n        # print(f'\u8c03\u6574\u6210\u4e8c\u7ef4\u540e\u7684single_mask_logits--&gt;{single_mask_logits.shape}')\n\n        # \u5c06\u5b50\u6807\u7b7e\u8f6c\u6362\u4e3a\u5f20\u91cf\uff0c\u5e76\u8c03\u6574\u5f62\u72b6\u4ee5\u5339\u914d\u6a21\u578b\u9884\u6d4b\u7684\u7ed3\u679c\n        single_sub_mask_labels = torch.LongTensor(single_sub_mask_labels).to(device)  # (sub_label_num, mask_label_num)\n        # \u8ba1\u7b97\u635f\u5931\u503c\u65f6\u771f\u5b9e\u5b50\u6807\u7b7e\u7ef4\u5ea6\u4e3a1\u7ef4\uff0c\u56e0\u6b64\u9700\u8981\u5c06\u5176\u5c55\u5e73\u4ee5\u5339\u914d\u6a21\u578b\u9884\u6d4b\u7684\u7ed3\u679c\n        single_sub_mask_labels = single_sub_mask_labels.reshape(-1, 1).squeeze()  # (sub_label_num * mask_label_num)\n        # print(f'\u771f\u5b9e\u5b50\u6807\u7b7emask\u503c\uff1asingle_sub_mask_labels--&gt;{single_sub_mask_labels.shape}')\n        # print(f'\u771f\u5b9e\u5b50\u6807\u7b7emask\u503c\uff1asingle_sub_mask_labels--&gt;{single_sub_mask_labels}')\n\n        # \u8ba1\u7b97\u5f53\u524d\u6279\u6b21\u6240\u6709\u5b50\u6807\u7b7e\u7684\u635f\u5931\n        cur_loss = cross_entropy_criterion(single_mask_logits, single_sub_mask_labels)\n        # \u8ba1\u7b97\u5f53\u524d\u6279\u6b21\u6240\u6709\u5b50\u6807\u7b7e\u7684\u5e73\u5747\u635f\u5931\n        cur_loss = cur_loss / len(single_sub_mask_labels)\n\n        # \u5982\u679c\u5f53\u524d\u635f\u5931loss\u672a\u88ab\u521d\u59cb\u5316\uff08\u5373\u4e3aNone\uff09\uff0c\u5219\u5c06\u5176\u8bbe\u7f6e\u4e3a\u5f53\u524d\u6279\u6b21\u7684\u635f\u5931cur_loss\n        if not loss:\n            loss = cur_loss\n        # \u5982\u679c\u5f53\u524d\u635f\u5931loss\u5df2\u7ecf\u5b58\u5728\uff0c\u5219\u5c06\u5f53\u524d\u6279\u6b21\u7684\u635f\u5931cur_loss\u7d2f\u52a0\u5230loss\u4e2d\n        else:\n            loss += cur_loss\n\n    # \u8ba1\u7b97\u5e73\u5747\u635f\u5931\uff1a\u5c06\u7d2f\u8ba1\u7684\u635f\u5931loss\u9664\u4ee5\u6279\u6b21\u5927\u5c0fbatch_size\n    loss = loss / batch_size  # (1,)\n    return loss\n\ndef convert_logits_to_ids(\n        logits: torch.tensor,\n        mask_positions: torch.tensor):\n    '''\n    \u8f93\u5165Language Model\u7684\u8bcd\u8868\u6982\u7387\u5206\u5e03\uff08LMModel\u7684logits\uff09\uff0c\u5c06mask_position\u4f4d\u7f6e\u7684token logits\u8f6c\u6362\u4e3atoken\u7684id\u3002\n    :param logits: (torch.tensor): model output -&gt; (batch, seq_len, vocab_size) [8, 512, 21128]\n    :param mask_positions: (torch.tensor): mask token\u7684\u4f4d\u7f6e -&gt; (batch, mask_label_num) [8, 2]\n    :return: \u5bf9\u5e94mask position\u4e0a\u6700\u5927\u6982\u7387\u7684\u63a8\u7406token -&gt; (batch, mask_label_num) [8, 2]\n    '''\n    # \u83b7\u53d6\u6807\u7b7e\u7684\u957f\u5ea6\uff0cmask_positions.size()\u8fd4\u56de\u7684\u662f\u4e00\u4e2a\u5305\u542b\u7ef4\u5ea6\u7684\u5143\u7ec4\uff0c[1]\u8868\u793a\u83b7\u53d6\u7b2c\u4e8c\u4e2a\u7ef4\u5ea6\u7684\u5927\u5c0f\n    label_length = mask_positions.size()[1]\n    # print(f'label_length--&gt;{label_length}')\n\n    # \u83b7\u53d6\u6279\u6b21\u5927\u5c0f\u3001\u5e8f\u5217\u957f\u5ea6\u548c\u8bcd\u6c47\u8868\u5927\u5c0f\uff0clogits.size()\u8fd4\u56de\u7684\u662f\u4e00\u4e2a\u5305\u542b\u7ef4\u5ea6\u7684\u5143\u7ec4\n    batch_size, seq_len, vocab_size = logits.size()\n\n    # \u521d\u59cb\u5316\u4e00\u4e2a\u7a7a\u5217\u8868\uff0c\u7528\u4e8e\u5b58\u50a8\u91cd\u5851\u540e\u7684 mask_positions\n    mask_positions_after_reshaped = []\n\n    # print(f'mask_positions.detach().cpu().numpy().tolist()--&gt;{mask_positions.detach().cpu().numpy().tolist()}')\n    # \u904d\u5386\u6bcf\u4e2a\u6279\u6b21\u7684mask_positions\n    for batch, mask_pos in enumerate(mask_positions.detach().cpu().numpy().tolist()):\n        # \u904d\u5386\u6bcf\u4e2amask\u4f4d\u7f6e\n        for pos in mask_pos:\n            # \u5c06\u6279\u6b21\u53f7\u548c\u5e8f\u5217\u4e2d\u7684mask\u4f4d\u7f6e\u7ed3\u5408\u8d77\u6765\uff0c\u5f97\u5230\u91cd\u5851\u540e\u7684mask_positions\n            mask_positions_after_reshaped.append(batch * seq_len + pos)\n    # print(f'mask_positions_after_reshaped--&gt;{mask_positions_after_reshaped}')\n    # print(f'\u539f\u59cb\u7684logits--&gt;{logits.shape}')\n\n    # \u5c06\u539f\u59cb\u7684logits\u91cd\u5851\u4e3a(batch_size * seq_len, vocab_size)\u7684\u5f62\u72b6\n    logits = logits.reshape(batch_size * seq_len, -1)  # (batch_size * seq_len, vocab_size)\n    # print(f'\u6539\u53d8\u539f\u59cb\u6a21\u578b\u8f93\u51fa\u7684\u7ed3\u679c\u5f62\u72b6--&gt;{logits.shape}')\n\n    # \u4ece\u91cd\u5851\u540e\u7684logits\u4e2d\uff0c\u9009\u62e9\u51fa\u88ab\u63a9\u7801\u4f4d\u7f6e\u7684logits\n    mask_logits = logits[mask_positions_after_reshaped]\n    # print(f'\u88ab\u63a9\u7801\u4f4d\u7f6e\u7684logits--&gt;{mask_logits.shape}')\n\n    # \u83b7\u53d6\u6bcf\u4e2a\u6837\u672cmask\u4f4d\u7f6e\u6240\u9884\u6d4b\u7684tokens\n    predict_tokens = mask_logits.argmax(dim=-1)  # (batch * label_num)\n    # print(f'\u83b7\u53d6\u6bcf\u4e2a\u6837\u672cmask\u4f4d\u7f6e\u9884\u6d4b\u7684tokens', predict_tokens)\n\n    # \u5c06\u6bcf\u4e2a\u6837\u672cmask\u4f4d\u7f6e\u9884\u6d4b\u7684tokens\u91cd\u5851\u4e3a(batch, label_num)\u7684\u5f62\u72b6\n    predict_tokens = predict_tokens.reshape(-1, label_length)  # (batch, label_num)\n    # print(f'predict_tokens--&gt;{predict_tokens}')\n\n    return predict_tokens\n</code></pre>"},{"location":"03-%E6%96%B0%E9%9B%B6%E5%94%AE%E8%A1%8C%E4%B8%9A%E8%AF%84%E4%BB%B7%E5%86%B3%E7%AD%96%E7%B3%BB%E7%BB%9F/04-BERT%2BPET%E6%96%B9%E5%BC%8F%E6%A8%A1%E5%9E%8B%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0%E4%B8%8E%E8%AE%AD%E7%BB%83.html#13-metirc_utilspy","title":"1.3 metirc_utils.py","text":"<ul> <li>\u76ee\u7684\uff1a\u5b9a\u4e49\uff08\u591a\uff09\u5206\u7c7b\u95ee\u9898\u4e0b\u7684\u6307\u6807\u8bc4\u4f30\uff08acc, precision, recall, f1\uff09\u3002</li> <li>\u5b9a\u4e49ClassEvaluator\u7c7b\uff0c\u4ee3\u7801\u5982\u4e0b\uff1a</li> </ul> <pre><code>from typing import List\nimport numpy as np\nfrom sklearn.metrics import accuracy_score, precision_score, f1_score\nfrom sklearn.metrics import recall_score, confusion_matrix\n\n\nclass ClassEvaluator(object):\n    def __init__(self):\n        # \u521d\u59cb\u5316\u771f\u5b9e\u7ed3\u679c\u548c\u9884\u6d4b\u7ed3\u679c\u7684\u5217\u8868\n        self.goldens = []  # \u5b58\u50a8\u771f\u5b9e\u7ed3\u679c\u6570\u636e\n        self.predictions = []  # \u5b58\u50a8\u9884\u6d4b\u7ed3\u679c\u6570\u636e\n\n    def add_batch(self,\n                  pred_batch: List[List],\n                  gold_batch: List[List]):\n        '''\n        \u6dfb\u52a0\u4e00\u4e2abatch\u4e2d\u7684prediction\u548cgold\u5217\u8868\uff0c\u7528\u4e8e\u540e\u7eed\u7edf\u4e00\u8ba1\u7b97\u3002\n        :param pred_batch: (list): \u6a21\u578b\u9884\u6d4b\u6807\u7b7e\u5217\u8868, e.g. -&gt;  [['\u4f53', '\u80b2'], ['\u8d22', '\u7ecf'], ...]\n        :param gold_batch: (list): \u771f\u5b9e\u6807\u7b7e\u6807\u7b7e\u5217\u8868, e.g. -&gt;  [['\u4f53', '\u80b2'], ['\u8d22', '\u7ecf'], ...]\n        :return:\n        '''\n        # \u786e\u4fdd\u9884\u6d4b\u6279\u6b21\u548c\u771f\u5b9e\u6279\u6b21\u957f\u5ea6\u4e00\u81f4\uff0c\u8fd9\u662f\u540e\u7eed\u5904\u7406\u7684\u524d\u63d0\u6761\u4ef6\n        assert len(pred_batch) == len(gold_batch)\n        # print(f'pred_batch0--&gt;{pred_batch}')\n        # print(f'gold_batch0--&gt;{gold_batch}')\n\n        # \u82e5\u9047\u5230\u591a\u4e2a\u5b50\u6807\u7b7e\u6784\u6210\u4e00\u4e2a\u6807\u7b7e\u7684\u60c5\u51b5\n        # \u5224\u65adgold_batch\u7684\u7b2c\u4e00\u4e2a\u5143\u7d20\u662f\u5426\u4e3a\u5217\u8868\u6216\u5143\u7ec4\u7c7b\u578b\n        if type(gold_batch[0]) in [list, tuple]:\n            # \u5982\u679c\u662f\uff0c\u5219\u5c06pred_batch\u4e2d\u7684\u6bcf\u4e2a\u5143\u7d20\u8f6c\u6362\u4e3a\u5b57\u7b26\u4e32\u540e\u62fc\u63a5\u8d77\u6765\n            pred_batch = [''.join([str(e) for e in ele]) for ele in pred_batch]\n            # \u540c\u6837\u5730\uff0c\u4e5f\u5c06gold_batch\u4e2d\u7684\u6bcf\u4e2a\u5143\u7d20\u8f6c\u6362\u4e3a\u5b57\u7b26\u4e32\u540e\u62fc\u63a5\u8d77\u6765\n            gold_batch = [''.join([str(e) for e in ele]) for ele in gold_batch]\n        # print(f'pred_batch--&gt;{pred_batch}')\n        # print(f'gold_batch--&gt;{gold_batch}')\n\n        # \u5c06\u771f\u5b9e\u7ed3\u679c\u7684\u6279\u6b21\u6570\u636e\u6dfb\u52a0\u5230self.goldens\u5217\u8868\u4e2d\n        self.goldens.extend(gold_batch)\n        # print(f'self.goldens--&gt;{self.goldens}')\n        # \u5c06\u9884\u6d4b\u7ed3\u679c\u7684\u6279\u6b21\u6570\u636e\u6dfb\u52a0\u5230self.predictions\u5217\u8868\u4e2d\n        self.predictions.extend(pred_batch)\n        # print(f'self.predictions--&gt;{self.predictions}')\n\n    def compute(self, round_num=2) -&gt; dict:\n        '''\n        \u6839\u636e\u5f53\u524d\u7c7b\u4e2d\u7d2f\u79ef\u7684\u53d8\u91cf\u503c\uff0c\u8ba1\u7b97\u5f53\u524d\u7684P, R, F1\u3002\n        :param round_num: (int): \u8ba1\u7b97\u7ed3\u679c\u4fdd\u7559\u5c0f\u6570\u70b9\u540e\u51e0\u4f4d, \u9ed8\u8ba4\u5c0f\u6570\u70b9\u540e2\u4f4d\u3002\n        :return:\n        dict -&gt; {\n            'accuracy': \u51c6\u786e\u7387,\n            'precision': \u7cbe\u51c6\u7387,\n            'recall': \u53ec\u56de\u7387,\n            'f1': f1\u503c,\n            'class_metrics': {\n                '0': {\n                        'precision': \u8be5\u7c7b\u522b\u4e0b\u7684precision,\n                        'recall': \u8be5\u7c7b\u522b\u4e0b\u7684recall,\n                        'f1': \u8be5\u7c7b\u522b\u4e0b\u7684f1\n                    },\n                ...\n            }\n        }\n        '''\n        # print(f'self.goldens--&gt;{self.goldens}')\n        # print(f'self.predictions--&gt;{self.predictions}')\n        # \u521d\u59cb\u5316\u7c7b\u522b\u96c6\u5408\u3001\u7c7b\u522b\u6307\u6807\u5b57\u5178\u548c\u7ed3\u679c\u5b57\u5178\uff0c\u7528\u4e8e\u5b58\u50a8\u5168\u5c40\u6307\u6807\n        # \u5c06 self.goldens \u548c self.predictions \u7684\u96c6\u5408\u5408\u5e76\uff0c\u5e76\u8fdb\u884c\u6392\u5e8f\uff0c\u7ed3\u679c\u5b58\u50a8\u5728\u53d8\u91cf classes \u4e2d\u3002\n        classes = sorted(list(set(self.goldens) | set(self.predictions)))\n        class_metrics = {}\n        res = {}\n        # print(f'classes--&gt;{classes}')\n\n        # \u6784\u5efa\u5168\u5c40\u6307\u6807\n        # \u8ba1\u7b97\u5e76\u5b58\u50a8\u5168\u5c40\u51c6\u786e\u7387\n        res['accuracy'] = round(accuracy_score(self.goldens, self.predictions), round_num)\n        # \u8ba1\u7b97\u5e76\u5b58\u50a8\u5168\u5c40\u7cbe\u786e\u7387\n        res['precision'] = round(precision_score(self.goldens, self.predictions, average='weighted'), round_num)\n        # \u8ba1\u7b97\u5e76\u5b58\u50a8\u5168\u5c40\u53ec\u56de\u7387\n        res['recall'] = round(recall_score(self.goldens, self.predictions, average='weighted'), round_num)\n        # \u8ba1\u7b97\u5e76\u5b58\u50a8\u5168\u5c40F1\u5206\u6570\n        res['f1'] = round(f1_score(self.goldens, self.predictions, average='weighted'), round_num)\n        # print(f'res--&gt;{res}')\n\n        try:\n            # \u8ba1\u7b97\u6df7\u6dc6\u77e9\u9635\uff0c\u5e76\u5c06\u5176\u8f6c\u6362\u4e3anumpy\u6570\u7ec4\uff0c\u5f62\u72b6\u4e3a(n_class, n_class)\n            conf_matrix = np.array(confusion_matrix(self.goldens, self.predictions))\n            # print(f'conf_matrix--&gt;{conf_matrix}')\n            # \u786e\u4fdd\u6df7\u6dc6\u77e9\u9635\u7684\u7ef4\u5ea6\u4e0e\u7c7b\u522b\u6570\u91cf\u5339\u914d\n            assert conf_matrix.shape[0] == len(classes)\n            # \u904d\u5386\u6bcf\u4e2a\u7c7b\u522b\uff0c\u8ba1\u7b97\u7cbe\u786e\u5ea6(precision)\u3001\u53ec\u56de\u7387(recall)\u548cF1\u5206\u6570(f1)\n            for i in range(conf_matrix.shape[0]):\n                # \u8ba1\u7b97\u5f53\u524d\u7c7b\u522b\u7684\u7cbe\u786e\u5ea6\n                precision = 0 if sum(conf_matrix[:, i]) == 0 else (conf_matrix[i, i] / sum(conf_matrix[:, i]))\n                # \u8ba1\u7b97\u5f53\u524d\u7c7b\u522b\u7684\u53ec\u56de\u7387\n                recall = 0 if sum(conf_matrix[i, :]) == 0 else (conf_matrix[i, i] / sum(conf_matrix[i, :]))\n                # \u8ba1\u7b97\u5f53\u524d\u7c7b\u522b\u7684F1\u5206\u6570\n                f1 = 0 if (precision + recall) == 0 else (2 * precision * recall / (precision + recall))\n                # \u5c06\u5f53\u524d\u7c7b\u522b\u7684\u7cbe\u786e\u5ea6\u3001\u53ec\u56de\u7387\u548cF1\u5206\u6570\u4fdd\u5b58\u5230\u5b57\u5178\u4e2d\n                class_metrics[classes[i]] = {\n                    'precision': round(precision, round_num),\n                    'recall': round(recall, round_num),\n                    'f1': round(f1, round_num)\n                }\n            # \u5c06\u6240\u6709\u7c7b\u522b\u7684\u6307\u6807\u4fdd\u5b58\u5230\u7ed3\u679c\u5b57\u5178\u4e2d\n            res['class_metrics'] = class_metrics\n        except Exception as e:\n            # \u5f02\u5e38\u5904\u7406\uff1a\u5f53\u8ba1\u7b97\u7c7b\u522b\u6307\u6807\u65f6\u53d1\u751f\u5f02\u5e38\uff0c\u6253\u5370\u8b66\u544a\u4fe1\u606f\u548c\u76f8\u5173\u6570\u636e\n            print(f'[Warning] Something wrong when calculate class_metrics: {e}')\n            print(f'--&gt; goldens: {set(self.goldens)}')\n            print(f'--&gt; predictions: {set(self.predictions)}')\n            print(f'--&gt; diff elements: {set(self.predictions) - set(self.goldens)}')\n            # \u5c06\u7ed3\u679c\u5b57\u5178\u4e2d\u7684\u7c7b\u522b\u6307\u6807\u8bbe\u7f6e\u4e3a\u7a7a\u5b57\u5178\n            res['class_metrics'] = {}\n\n        return res\n\n    def reset(self):\n        \"\"\"\n        \u91cd\u7f6e\u79ef\u7d2f\u7684\u6570\u503c\u3002\n        \"\"\"\n        self.goldens = []\n        self.predictions = []\n\n\nif __name__ == '__main__':\n    metric = ClassEvaluator()\n    metric.add_batch(\n        [['\u8d22', '\u7ecf'], ['\u8d22', '\u7ecf'], ['\u4f53', '\u80b2'], ['\u4f53', '\u80b2'], ['\u8ba1', '\u7b97', '\u673a']],\n        [['\u4f53', '\u80b2'], ['\u8d22', '\u7ecf'], ['\u4f53', '\u80b2'], ['\u8ba1', '\u7b97', '\u673a'], ['\u8ba1', '\u7b97', '\u673a']],\n    )\n    # metric.add_batch(\n    #     [0, 0, 1, 1, 0],\n    #     [1, 1, 1, 0, 0]\n    # )\n    res = metric.compute()\n    print(res)\n</code></pre>"},{"location":"03-%E6%96%B0%E9%9B%B6%E5%94%AE%E8%A1%8C%E4%B8%9A%E8%AF%84%E4%BB%B7%E5%86%B3%E7%AD%96%E7%B3%BB%E7%BB%9F/04-BERT%2BPET%E6%96%B9%E5%BC%8F%E6%A8%A1%E5%9E%8B%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0%E4%B8%8E%E8%AE%AD%E7%BB%83.html#2","title":"2. \u5b9e\u73b0\u6a21\u578b\u8bad\u7ec3\u51fd\u6570,\u9a8c\u8bc1\u51fd\u6570","text":"<ul> <li> <p>\u76ee\u7684\uff1a\u5b9e\u73b0\u6a21\u578b\u7684\u8bad\u7ec3\u548c\u9a8c\u8bc1</p> </li> <li> <p>\u811a\u672c\u91cc\u9762\u5305\u542b\u4e24\u4e2a\u51fd\u6570\uff1amodel2train()\u548cevaluate_model()</p> </li> <li> <p>\u4ee3\u7801\u8def\u5f84\uff1allm_tuning/prompt_tasks/PET/train.py</p> <p>\u4ee3\u7801\u5982\u4e0b\uff1a</p> </li> </ul> <pre><code>import os\nimport time\n\nimport torch\nfrom tqdm import tqdm\nfrom transformers import AutoModelForMaskedLM, AutoTokenizer, get_scheduler\n\nfrom prompt_tasks.PET.data_handle.data_loader import get_data\nfrom prompt_tasks.PET.pet_config import ProjectConfig\nfrom prompt_tasks.PET.utils.common_utils import mlm_loss, convert_logits_to_ids\nfrom prompt_tasks.PET.utils.metirc_utils import ClassEvaluator\nfrom prompt_tasks.PET.utils.verbalizer import Verbalizer\n\npc = ProjectConfig()\n\n\ndef model2train():\n    # \u52a0\u8f7d\u8bad\u7ec3\u6570\u636e\u548c\u9a8c\u8bc1\u6570\u636e\n    train_dataloader, dev_dataloader = get_data()\n\n    # \u52a0\u8f7d\u9884\u8bad\u7ec3\u6a21\u578b\n    model = AutoModelForMaskedLM.from_pretrained(pc.pre_model).to(pc.device)\n    # print(f'\u9884\u8bad\u7ec3\u6a21\u578b\u5e26MLM\u5934\u7684--&gt;{model}')\n    # \u52a0\u8f7d\u5206\u8bcd\u5668\n    tokenizer = AutoTokenizer.from_pretrained(pc.pre_model)\n    # \u52a0\u8f7d\u6620\u5c04\u8bcd\u8868\n    verbalizer = Verbalizer(verbalizer_file=pc.verbalizer,\n                            tokenizer=tokenizer,\n                            max_label_len=pc.max_label_len)\n    # print(f'verbalizer--&gt;{verbalizer.label_dict}')\n\n    # \u4e0d\u9700\u8981\u6743\u91cd\u8870\u51cf\u7684\u53c2\u6570\n    no_decay = [\"bias\", \"LayerNorm.weight\"]\n    # print(type(model.parameters()))\n    # \u5b9a\u4e49\u4f18\u5316\u5668\u7684\u53c2\u6570\u7ec4\uff0c\u4ee5\u4fbf\u5bf9\u6a21\u578b\u7684\u4e0d\u540c\u90e8\u5206\u5e94\u7528\u4e0d\u540c\u7684\u6743\u91cd\u8870\u51cf\n    optimizer_grouped_parameters = [\n        # \u7b2c\u4e00\u7ec4\u53c2\u6570\uff1a\u5305\u542b\u6240\u6709\u9002\u7528\u6743\u91cd\u8870\u51cf\u7684\u6a21\u578b\u53c2\u6570\n        {\n            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n            \"weight_decay\": pc.weight_decay,\n        },\n        # \u7b2c\u4e8c\u7ec4\u53c2\u6570\uff1a\u5305\u542b\u6240\u6709\u4e0d\u9002\u7528\u6743\u91cd\u8870\u51cf\u7684\u6a21\u578b\u53c2\u6570\n        {\n            \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n            \"weight_decay\": 0.0,\n        },\n    ]\n    # \u521d\u59cb\u5316AdamW\u4f18\u5316\u5668\uff0c\u7528\u4e8e\u6a21\u578b\u53c2\u6570\u7684\u4f18\u5316\n    # AdamW\u662fAdam\u7b97\u6cd5\u7684\u53d8\u4f53\uff0c\u52a0\u5165\u4e86\u6743\u91cd\u8870\u51cf\uff08L2\u6b63\u5219\u5316\uff09\uff0c\u6709\u52a9\u4e8e\u9632\u6b62\u8fc7\u62df\u5408\n    # \u53c2\u6570optimizer_grouped_parameters\u662f\u5206\u7ec4\u7684\u6a21\u578b\u53c2\u6570\uff0c\u5141\u8bb8\u5bf9\u4e0d\u540c\u7684\u53c2\u6570\u5e94\u7528\u4e0d\u540c\u7684\u5b66\u4e60\u7387\u6216\u6b63\u5219\u5316\u5f3a\u5ea6\n    optimizer = torch.optim.AdamW(optimizer_grouped_parameters, lr=pc.learning_rate)\n\n    # \u6839\u636e\u8bad\u7ec3\u8f6e\u6570\u8ba1\u7b97\u6700\u5927\u8bad\u7ec3\u6b65\u6570\uff0c\u4ee5\u4fbf\u4e8escheduler\u52a8\u6001\u8c03\u6574lr\n    num_update_steps_per_epoch = len(train_dataloader)\n    # \u6307\u5b9a\u603b\u7684\u8bad\u7ec3\u6b65\u6570\uff0c\u5b83\u4f1a\u88ab\u5b66\u4e60\u7387\u8c03\u5ea6\u5668\u7528\u6765\u786e\u5b9a\u5b66\u4e60\u7387\u7684\u53d8\u5316\u89c4\u5f8b\uff0c\u786e\u4fdd\u5b66\u4e60\u7387\u5728\u6574\u4e2a\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u5f97\u4ee5\u5408\u7406\u5730\u8c03\u8282\n    max_train_steps = pc.epochs * num_update_steps_per_epoch\n    # \u8ba1\u7b97\u9884\u70ed\u9636\u6bb5\u7684\u8bad\u7ec3\u6b65\u6570\uff0c\u7528\u4e8e\u521d\u59cb\u5316\u5b66\u4e60\u7387\u8c03\u5ea6\n    warm_steps = int(pc.warmup_ratio * max_train_steps)  # \u9884\u70ed\u9636\u6bb5\u7684\u8bad\u7ec3\u6b65\u6570\n    # \u521b\u5efa\u5b66\u4e60\u7387\u8c03\u5ea6\u5668\uff0c\u4f7f\u7528\u7ebf\u6027\u8c03\u5ea6\u7b56\u7565\uff0c\u6839\u636e\u8bad\u7ec3\u7684\u8fdb\u884c\u9010\u6b65\u8c03\u6574\u5b66\u4e60\u7387\n    lr_scheduler = get_scheduler(\n        name='linear',\n        optimizer=optimizer,\n        num_warmup_steps=warm_steps,\n        num_training_steps=max_train_steps)\n\n    # \u521d\u59cb\u5316\u635f\u5931\u5217\u8868\uff0c\u7528\u4e8e\u8bb0\u5f55\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7684\u635f\u5931\u503c\n    loss_list = []\n    # \u8bb0\u5f55\u8bad\u7ec3\u5f00\u59cb\u7684\u65f6\u95f4\uff0c\u7528\u4e8e\u8ba1\u7b97\u8bad\u7ec3\u65f6\u957f\n    tic_train = time.time()\n    # \u521b\u5efa\u5206\u7c7b\u8bc4\u4f30\u5668\uff0c\u7528\u4e8e\u8bc4\u4f30\u6a21\u578b\u6027\u80fd\n    metric = ClassEvaluator()\n    # \u5b9a\u4e49\u635f\u5931\u51fd\u6570\uff0c\u7528\u4e8e\u8ba1\u7b97\u6a21\u578b\u9884\u6d4b\u503c\u4e0e\u771f\u5b9e\u6807\u7b7e\u4e4b\u95f4\u7684\u5dee\u5f02\n    criterion = torch.nn.CrossEntropyLoss()\n    # \u521d\u59cb\u5316\u8bad\u7ec3\u6b21\u6570\u548c\u6700\u4f73F1\u5206\u6570\uff0c\u7528\u4e8e\u8ddf\u8e2a\u8bad\u7ec3\u8fdb\u5ea6\u548c\u6a21\u578b\u6027\u80fd\n    global_step, best_f1 = 0, 0\n\n    print('\u5f00\u59cb\u8bad\u7ec3\uff1a')\n    for epoch in range(pc.epochs):\n        for batch in tqdm(train_dataloader, desc='\u6a21\u578b\u8bad\u7ec3'):\n            # print(f'batch--&gt;{batch}')\n            # \u5c06\u6279\u6b21\u6570\u636e\u8f93\u5165\u6a21\u578b\uff0c\u83b7\u53d6logits\n            logits = model(input_ids=batch['input_ids'].to(pc.device),\n                           token_type_ids=batch['token_type_ids'].to(pc.device),\n                           attention_mask=batch['attention_mask'].to(pc.device)).logits\n            # print(f'logits-&gt;{logits.shape}')\n\n            # \u771f\u5b9e\u6807\u7b7e\n            mask_labels = batch['mask_labels'].numpy().tolist()\n            # print(f'mask_labels---&gt;{mask_labels}')\n            # \u63d0\u53d6\u5b50\u6807\u7b7e\n            sub_labels = verbalizer.batch_find_sub_labels(mask_labels)\n            # print(f'sub_labels---&gt;{sub_labels}')\n            # \u83b7\u53d6\u5b50\u6807\u7b7e\u7684token_ids\n            sub_labels = [ele['token_ids'] for ele in sub_labels]\n            # print(f'sub_labels_token_ids---&gt;{sub_labels}')\n\n            # \u8ba1\u7b97\u63a9\u7801\u8bed\u8a00\u6a21\u578b\u7684\u635f\u5931\u503c\n            loss = mlm_loss(logits,\n                            batch['mask_positions'].to(pc.device),\n                            sub_labels,\n                            criterion,\n                            pc.device)\n            # print(f'\u8ba1\u7b97\u635f\u5931\u503c--&gt;{loss}')\n            # \u6e05\u96f6\u4f18\u5316\u5668\u7684\u68af\u5ea6\n            optimizer.zero_grad()\n            # \u53cd\u5411\u4f20\u64ad\u8ba1\u7b97\u68af\u5ea6\n            loss.backward()\n            # \u66f4\u65b0\u6a21\u578b\u53c2\u6570\n            optimizer.step()\n            # \u66f4\u65b0\u5b66\u4e60\u7387\u8c03\u5ea6\u5668\n            lr_scheduler.step()\n\n            # \u5c06\u635f\u5931\u503c\u6dfb\u52a0\u5230\u635f\u5931\u5217\u8868\u4e2d\n            loss_list.append(loss)\n            # \u8bad\u7ec3\u6b21\u6570\u589e\u52a01\n            global_step += 1\n            # \u6253\u5370\u8bad\u7ec3\u65e5\u5fd7\n            if global_step % pc.logging_steps == 0:\n                time_diff = time.time() - tic_train\n                loss_avg = sum(loss_list) / len(loss_list)\n                print(\"global step %d, epoch: %d, loss: %.5f, speed: %.2f step/s\"\n                      % (global_step, epoch, loss_avg, pc.logging_steps / time_diff))\n                tic_train = time.time()\n        # \u6a21\u578b\u9a8c\u8bc1\n        # \u4f7f\u7528\u7ed9\u5b9a\u7684\u6a21\u578b\u3001\u8bc4\u4f30\u6307\u6807\u3001\u6570\u636e\u52a0\u8f7d\u5668\u3001\u5206\u8bcd\u5668\u548c\u6807\u8bb0\u5316\u5668\u8fdb\u884c\u6a21\u578b\u8bc4\u4f30\n        acc, precision, recall, f1, class_metrics = evaluate_model(model,\n                                                                   metric,\n                                                                   dev_dataloader,\n                                                                   tokenizer,\n                                                                   verbalizer)\n\n        # \u6253\u5370\u8bc4\u4f30\u7ed3\u679c\u4e2d\u7684\u7cbe\u786e\u5ea6\u3001\u53ec\u56de\u7387\u548cF1\u5206\u6570\n        print(\"\u9a8c\u8bc1\u96c6\u7684 precision: %.5f, recall: %.5f, F1: %.5f\" % (precision, recall, f1))\n        # \u5982\u679c\u5f53\u524dF1\u5206\u6570\u9ad8\u4e8e\u6700\u4f73F1\u5206\u6570\uff0c\u5219\u66f4\u65b0\u6700\u4f73F1\u5206\u6570\u548c\u76f8\u5173\u6a21\u578b\u53ca\u5206\u8bcd\u5668\n        if f1 &gt; best_f1:\n            print(\n                f\"\u6700\u597d\u7684f1\u5206\u6570\u88ab\u66f4\u65b0: {best_f1:.5f} --&gt; {f1:.5f}\"\n            )\n            print(f'\u6bcf\u79cd\u7c7b\u578b\u7684Metrics\u4e3a: {class_metrics}')\n            # \u66f4\u65b0\u5f53\u524d\u6700\u4f73\u7684F1\u5206\u6570\n            best_f1 = f1\n            # \u5b9a\u4e49\u5f53\u524d\u4fdd\u5b58\u6a21\u578b\u548c\u5206\u8bcd\u5668\u7684\u76ee\u5f55\n            cur_save_dir = os.path.join(pc.save_dir, \"model_best\")\n            print(cur_save_dir)\n            # \u68c0\u67e5\u5e76\u521b\u5efa\u4fdd\u5b58\u76ee\u5f55\uff08\u5982\u679c\u4e0d\u5b58\u5728\uff09\n            if not os.path.exists(cur_save_dir):\n                os.makedirs(cur_save_dir)\n            # \u4fdd\u5b58\u6a21\u578b\u5230\u6307\u5b9a\u76ee\u5f55\n            model.save_pretrained(cur_save_dir)\n            # \u4fdd\u5b58\u5206\u8bcd\u5668\u5230\u6307\u5b9a\u76ee\u5f55\n            tokenizer.save_pretrained(cur_save_dir)\n        tic_train = time.time()\n\n    print('\u8bad\u7ec3\u7ed3\u675f')\n\n\ndef evaluate_model(model,\n                   metric,\n                   data_loader,\n                   tokenizer,\n                   verbalizer):\n    '''\n    \u5728\u6d4b\u8bd5\u96c6\u4e0a\u8bc4\u4f30\u5f53\u524d\u6a21\u578b\u7684\u8bad\u7ec3\u6548\u679c\u3002\n    :param model: \u5f53\u524d\u6a21\u578b\n    :param metric: \u8bc4\u4f30\u6307\u6807\u7c7b(metric)\n    :param data_loader: \u6d4b\u8bd5\u96c6\u7684dataloader\n    :param tokenizer: \u5206\u8bcd\u5668\n    :param verbalizer: \u6620\u5c04\u8868\n    :return:\n    '''\n    model.eval()\n    metric.reset()\n\n    with torch.no_grad():\n        for step, batch in enumerate(tqdm(data_loader, desc='\u6a21\u578b\u9a8c\u8bc1')):\n            # print(f'batch--&gt;{batch}')\n            logits = model(input_ids=batch['input_ids'].to(pc.device),\n                           token_type_ids=batch['token_type_ids'].to(pc.device),\n                           attention_mask=batch['attention_mask'].to(pc.device)).logits\n            # print(f'\u9a8c\u8bc1\u96c6\u6a21\u578b\u9884\u6d4b\u7684\u7ed3\u679c\u2014\u2014\u2014\u2014&gt;{logits.shape}')\n\n            mask_labels = batch['mask_labels'].numpy().tolist()  # (batch, label_num)\n            # print(f\"mask_labels-0--&gt;{mask_labels}\")\n\n            for i in range(len(mask_labels)):  # \u53bb\u6389label\u4e2d\u7684[PAD] token\n                while tokenizer.pad_token_id in mask_labels[i]:\n                    mask_labels[i].remove(tokenizer.pad_token_id)\n            # print(f'mask_labels-1--&gt;{mask_labels}')\n            # \u5c06mask_labels id\u8f6c\u6362\u4e3a\u6587\u5b57\n            mask_labels = [''.join(tokenizer.convert_ids_to_tokens(t)) for t in mask_labels]\n            # print(f'\u771f\u5b9e\u7684\u7ed3\u679c\u4e3b\u6807\u7b7e\uff1amask_labels_str--&gt;{mask_labels}')\n\n            # \u83b7\u53d6\u6a21\u578b\u9884\u6d4b\u7684\u5b50\u6807\u7b7e\n            predictions = convert_logits_to_ids(logits,\n                                                batch['mask_positions']).cpu().numpy().tolist()  # (batch, label_num)\n            # print(f'\u6a21\u578b\u9884\u6d4b\u7684\u5b50\u6807\u7b7e\u7684\u7ed3\u679c--&gt;{predictions}')\n\n            # \u6839\u636e\u6a21\u578b\u9884\u6d4b\u7684\u5b50\u6807\u7b7e\uff0c\u627e\u5230\u5b50label\u5c5e\u4e8e\u7684\u4e3blabel\n            predictions = verbalizer.batch_find_main_label(predictions)  # \u627e\u5230\u5b50label\u5c5e\u4e8e\u7684\u4e3blabel\n            # print(f\"\u627e\u5230\u6a21\u578b\u9884\u6d4b\u7684\u5b50\u6807\u7b7e\u5bf9\u5e94\u7684\u4e3b\u6807\u7b7e\u7684\u7ed3\u679c--&gt;{predictions}')\")\n\n            # \u83b7\u5f97\u9884\u6d4b\u7684\u4e3b\u6807\u7b7e\u540d\n            predictions = [ele['label'] for ele in predictions]\n            # print(f\"\u53ea\u83b7\u5f97\u9884\u6d4b\u7684\u4e3b\u6807\u7b7e\u7684\u7ed3\u679cstring--&gt;{predictions}')\")\n\n            # \u8c03\u7528add_batch\u65b9\u6cd5, \u5c06\u6a21\u578b\u9884\u6d4b\u7684\u4e3b\u6807\u7b7e\u4e0e\u771f\u5b9e\u4e3b\u6807\u7b7e\u4fdd\u5b58\u5230metric\u5c5e\u6027\u4e2d\n            metric.add_batch(pred_batch=predictions, gold_batch=mask_labels)\n    eval_metric = metric.compute()\n    model.train()\n\n    return eval_metric['accuracy'], eval_metric['precision'], \\\n        eval_metric['recall'], eval_metric['f1'], \\\n        eval_metric['class_metrics']\n\n\nif __name__ == '__main__':\n    model2train()\n</code></pre> <ul> <li>\u8f93\u51fa\u7ed3\u679c:</li> </ul> <pre><code>.....\nglobal step 40, epoch: 4, loss: 0.62105, speed: 1.27 step/s\nEvaluation precision: 0.78000, recall: 0.77000, F1: 0.76000\nEach Class Metrics are: {'\u4e66\u7c4d': {'precision': 0.97, 'recall': 0.82, 'f1':\n0.89}, '\u5e73\u677f': {'precision': 0.57, 'recall': 0.84, 'f1': 0.68}, '\u624b\u673a':\n{'precision': 0.0, 'recall': 0.0, 'f1': 0}, '\u6c34\u679c': {'precision': 0.95,\n'recall': 0.81, 'f1': 0.87}, '\u6d17\u6d74': {'precision': 0.7, 'recall': 0.71, 'f1':\n0.7}, '\u7535\u5668': {'precision': 0.0, 'recall': 0.0, 'f1': 0}, '\u7535\u8111': {'precision':\n0.86, 'recall': 0.38, 'f1': 0.52}, '\u8499\u725b': {'precision': 1.0, 'recall': 0.68,\n'f1': 0.81}, '\u8863\u670d': {'precision': 0.71, 'recall': 0.91, 'f1': 0.79}, '\u9152\u5e97':\n{'precision': 1.0, 'recall': 0.88, 'f1': 0.93}}\nglobal step 50, epoch: 6, loss: 0.50076, speed: 1.23 step/s\nglobal step 60, epoch: 7, loss: 0.41744, speed: 1.23 step/s\n...\nglobal step 390, epoch: 48, loss: 0.06674, speed: 1.20 step/s\nglobal step 400, epoch: 49, loss: 0.06507, speed: 1.21 step/s\nEvaluation precision: 0.78000, recall: 0.76000, F1: 0.75000\n</code></pre> <ul> <li>\u7ed3\u8bba: BERT+PET\u6a21\u578b\u5728\u8bad\u7ec3\u96c6\u4e0a\u7684\u8868\u73b0\u662f\u7cbe\u786e\u7387=78%</li> <li>\u6ce8\u610f\uff1a\u672c\u9879\u76ee\u4e2d\u53ea\u7528\u4e8660\u6761\u6837\u672c\uff0c\u5728\u63a5\u8fd1600\u6761\u6837\u672c\u4e0a\u7cbe\u786e\u7387\u5c31\u5df2\u7ecf\u8fbe\u5230\u4e8678%\uff0c\u5982\u679c\u60f3\u8ba9\u6307\u6807\u66f4\u9ad8\uff0c\u53ef\u4ee5\u6269\u589e\u6837\u672c\u3002</li> </ul>"},{"location":"03-%E6%96%B0%E9%9B%B6%E5%94%AE%E8%A1%8C%E4%B8%9A%E8%AF%84%E4%BB%B7%E5%86%B3%E7%AD%96%E7%B3%BB%E7%BB%9F/04-BERT%2BPET%E6%96%B9%E5%BC%8F%E6%A8%A1%E5%9E%8B%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0%E4%B8%8E%E8%AE%AD%E7%BB%83.html#3","title":"3. \u5b9e\u73b0\u6a21\u578b\u9884\u6d4b\u51fd\u6570","text":"<ul> <li> <p>\u76ee\u7684\uff1a\u52a0\u8f7d\u8bad\u7ec3\u597d\u7684\u6a21\u578b\u5e76\u6d4b\u8bd5\u6548\u679c</p> </li> <li> <p>\u4ee3\u7801\u8def\u5f84\uff1allm_tuning/prompt_tasks/PET/inference.py</p> <p>\u4ee3\u7801\u5982\u4e0b\uff1a</p> </li> </ul> <pre><code>import os\nimport time\nfrom typing import List\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForMaskedLM\n\nfrom prompt_tasks.PET.data_handle.data_preprocess import convert_example\nfrom prompt_tasks.PET.data_handle.template import HardTemplate\nfrom prompt_tasks.PET.pet_config import ProjectConfig\nfrom prompt_tasks.PET.utils.common_utils import convert_logits_to_ids\nfrom prompt_tasks.PET.utils.verbalizer import Verbalizer\n\npc = ProjectConfig()\n\nmodel_path = os.path.join(pc.save_dir, 'model_best')\ntokenizer = AutoTokenizer.from_pretrained(model_path)\nmodel = AutoModelForMaskedLM.from_pretrained(model_path).to(pc.device)\nmodel.eval()\n\nmax_label_len = 2  # \u6807\u7b7e\u6700\u5927\u957f\u5ea6\nverbalizer = Verbalizer(\n    verbalizer_file=pc.verbalizer,\n    tokenizer=tokenizer,\n    max_label_len=max_label_len)\nprompt = open(pc.prompt_file, 'r', encoding='utf8').readlines()[0].strip()  # prompt\u5b9a\u4e49\nprint(f'\u63d0\u793a\u8bcd--&gt; {prompt}')\nhard_template = HardTemplate(prompt=prompt)  # \u6a21\u677f\u8f6c\u6362\u5668\u5b9a\u4e49\n\n\ndef inference(contents: List[str]):\n    '''\n    \u63a8\u7406\u51fd\u6570\uff0c\u8f93\u5165\u539f\u59cb\u53e5\u5b50\uff0c\u8f93\u51famask label\u7684\u9884\u6d4b\u503c\u3002\n    :param contents:\n    :return: \u63cf\u539f\u59cb\u53e5\u5b50\u5217\u8868\u3002\n    '''\n    with (torch.no_grad()):\n        start_time = time.time()\n\n        # \u5c06\u5185\u5bb9\u5c01\u88c5\u4e3a\u793a\u4f8b\u5b57\u5178\uff0c\u51c6\u5907\u8fdb\u884c\u6807\u8bb0\u5316\u5904\u7406\n        examples = {'text': contents}\n\n        # \u5bf9\u793a\u4f8b\u8fdb\u884c\u6807\u8bb0\u5316\u5904\u7406\uff0c\u8fd4\u56de\u6807\u8bb0\u5316\u8f93\u51fa\n        tokenized_output = convert_example(\n            examples,\n            tokenizer,\n            hard_template=hard_template,\n            max_seq_len=128,\n            max_label_len=max_label_len,\n            train_mode=False,\n            return_tensor=True)\n\n        # \u4f7f\u7528\u6a21\u578b\u8fdb\u884c\u9884\u6d4b\uff0c\u83b7\u53d6logits\n        logits = model(input_ids=tokenized_output['input_ids'].to(pc.device),\n                       token_type_ids=tokenized_output['token_type_ids'].to(pc.device),\n                       attention_mask=tokenized_output['attention_mask'].to(pc.device)).logits\n\n        # \u5c06logits\u8f6c\u6362\u4e3a\u9884\u6d4b\u6807\u7b7e\n        predictions = convert_logits_to_ids(logits, tokenized_output['mask_positions']\n                                            ).cpu().numpy().tolist()  # (batch, label_num)\n\n        # \u627e\u5230\u5b50label\u5c5e\u4e8e\u7684\u4e3blabel\n        predictions = verbalizer.batch_find_main_label(predictions)\n\n        # \u63d0\u53d6\u9884\u6d4b\u7684\u6807\u7b7e\n        predictions = [ele['label'] for ele in predictions]\n\n        used = time.time() - start_time\n        print(f'\u8017\u65f6 {used} \u79d2\u3002')\n        return predictions\n\n\nif __name__ == '__main__':\n    contents = [\n        '\u5929\u53f0\u5f88\u597d\u770b\uff0c\u8eba\u5728\u8eba\u6905\u4e0a\u5f88\u60a0\u95f2\uff0c\u56e0\u4e3a\u6d3b\u52a8\u6240\u4ee5\u6211\u89c9\u5f97\u6027\u4ef7\u6bd4\u8fd8\u4e0d\u9519\uff0c\u9002\u5408\u4e00\u5bb6\u51fa\u884c\uff0c\u7279\u522b\u662f\u53bb\u8fea\u58eb\u5c3c\u4e5f\u86ee\u8fd1\u7684\uff0c\u4e0b\u6b21\u6709\u673a\u4f1a\u80af\u5b9a\u8fd8\u4f1a\u518d\u6765\u7684\uff0c\u503c\u5f97\u63a8\u8350',\n        '\u73af\u5883\uff0c\u8bbe\u65bd\uff0c\u5f88\u68d2\uff0c\u5468\u8fb9\u914d\u5957\u8bbe\u65bd\u9f50\u5168\uff0c\u524d\u53f0\u5c0f\u59d0\u59d0\u8d85\u7ea7\u6f02\u4eae\uff01\u9152\u5e97\u5f88\u8d5e\uff0c\u65e9\u9910\u4e0d\u9519\uff0c\u670d\u52a1\u6001\u5ea6\u5f88\u597d\uff0c\u524d\u53f0\u7f8e\u7709\u5f88\u6f02\u4eae\u3002\u6027\u4ef7\u6bd4\u8d85\u9ad8\u7684\u4e00\u5bb6\u9152\u5e97\u3002\u5f3a\u70c8\u63a8\u8350',\n        \"\u7269\u6d41\u8d85\u5feb\uff0c\u9694\u5929\u5c31\u5230\u4e86\uff0c\u8fd8\u6ca1\u7528\uff0c\u5c6f\u7740\u51fa\u6e38\u7684\u65f6\u5019\u7528\u7684\uff0c\u542c\u65b9\u4fbf\u7684\uff0c\u5360\u5730\u5c0f\",\n        \"\u798f\u884c\u5e02\u6765\u5230\u65e0\u65e9\u96c6\u5e02\uff0c\u56e0\u4e3a\u662f\u559c\u6b22\u7684\u9762\u5305\u5e97\uff0c\u6240\u4ee5\u8dd1\u6765\u96c6\u5e02\u770b\u770b\u3002\u7b2c\u4e00\u773c\u5c31\u770b\u5230\u4e86\uff0c\u4e4b\u524d\u5728\u5fae\u5e97\u4e70\u4e86\u5c0f\u5218\uff0c\u8fd9\u6b21\u4e70\u4e86\u8001\u5218\uff0c\u8fd8\u6709\u4e00\u76f4\u559c\u6b22\u7684\u5de7\u514b\u529b\u78c5\u86cb\u7cd5\u3002\u597d\u5947\u8001\u677f\u4e3a\u5565\u4e0d\u505a\u67e0\u6aac\u78c5\u86cb\u7cd5\u4e86\uff0c\u5fae\u5e97\u4e00\u76f4\u90fd\u662f\u4e70\u4e0d\u5230\u7684\u72b6\u6001\u3002\u56e0\u4e3a\u4e0d\u7231\u78b1\u6c34\u786c\u6b27\u4e4b\u7c7b\u7684\uff0c\u6240\u4ee5\u671f\u5f85\u8001\u677f\u591a\u6765\u70b9\u5176\u4ed6\u5c0f\u70b9\uff0c\u997c\u5e72\u4e00\u76f4\u4e5f\u662f\u5927\u7231\uff0c\u90a3\u5929\u597d\u50cf\u4e5f\u6ca1\u770b\u5230\",\n        \"\u670d\u52a1\u5f88\u7528\u5fc3\uff0c\u623f\u578b\u4e5f\u5f88\u8212\u670d\uff0c\u5c0f\u670b\u53cb\u5f88\u559c\u6b22\uff0c\u4e0b\u6b21\u53bb\u5609\u5b9a\u8fd8\u4f1a\u518d\u9009\u62e9\u3002\u5e8a\u94fa\u67d4\u8f6f\u8212\u9002\uff0c\u665a\u4e0a\u4f11\u606f\u5f88\u5b89\u9038\uff0c\u9694\u97f3\u6548\u679c\u4e0d\u9519\u8d5e\uff0c\u4e0b\u6b21\u8fd8\u4f1a\u6765\"\n    ]\n    print(\"\u9488\u5bf9\u4e0b\u9762\u7684\u6587\u672c\u8bc4\u8bba\uff0c\u8bf7\u5206\u522b\u7ed9\u51fa\u5bf9\u5e94\u6240\u5c5e\u7c7b\u522b\uff1a\")\n    res = inference(contents)\n    print('\u63a8\u65ad\u7684\u7c7b\u522b\u4e3a\uff1a', res)\n    new_dict = {}\n    for i in range(len(contents)):\n        new_dict[contents[i]] = res[i]\n    print(f'new_dict--&gt;{new_dict}')\n</code></pre> <ul> <li>\u7ed3\u679c\u5c55\u793a</li> </ul> <pre><code>{\n    '\u5929\u53f0\u5f88\u597d\u770b\uff0c\u8eba\u5728\u8eba\u6905\u4e0a\u5f88\u60a0\u95f2\uff0c\u56e0\u4e3a\u6d3b\u52a8\u6240\u4ee5\u6211\u89c9\u5f97\u6027\u4ef7\u6bd4\u8fd8\u4e0d\u9519\uff0c\u9002\u5408\u4e00\u5bb6\u51fa\n\u884c\uff0c\u7279\u522b\u662f\u53bb\u8fea\u58eb\u5c3c\u4e5f\u86ee\u8fd1\u7684\uff0c\u4e0b\u6b21\u6709\u673a\u4f1a\u80af\u5b9a\u8fd8\u4f1a\u518d\u6765\u7684\uff0c\u503c\u5f97\u63a8\u8350': '\u9152\u5e97',\n    '\u73af\u5883\uff0c\u8bbe\u65bd\uff0c\u5f88\u68d2\uff0c\u5468\u8fb9\u914d\u5957\u8bbe\u65bd\u9f50\u5168\uff0c\u524d\u53f0\u5c0f\u59d0\u59d0\u8d85\u7ea7\u6f02\u4eae\uff01\u9152\u5e97\u5f88\u8d5e\uff0c\u65e9\u9910\u4e0d\n\u9519\uff0c\u670d\u52a1\u6001\u5ea6\u5f88\u597d\uff0c\u524d\u53f0\u7f8e\u7709\u5f88\u6f02\u4eae\u3002\u6027\u4ef7\u6bd4\u8d85\u9ad8\u7684\u4e00\u5bb6\u9152\u5e97\u3002\u5f3a\u70c8\u63a8\u8350': '\u9152\u5e97',\n    '\u7269\u6d41\u8d85\u5feb\uff0c\u9694\u5929\u5c31\u5230\u4e86\uff0c\u8fd8\u6ca1\u7528\uff0c\u5c6f\u7740\u51fa\u6e38\u7684\u65f6\u5019\u7528\u7684\uff0c\u542c\u65b9\u4fbf\u7684\uff0c\u5360\u5730\u5c0f': '\u5e73\u677f',\n    '\u798f\u884c\u5e02\u6765\u5230\u65e0\u65e9\u96c6\u5e02\uff0c\u56e0\u4e3a\u662f\u559c\u6b22\u7684\u9762\u5305\u5e97\uff0c\u6240\u4ee5\u8dd1\u6765\u96c6\u5e02\u770b\u770b\u3002\u7b2c\u4e00\u773c\u5c31\u770b\u5230\u4e86\n\uff0c\u4e4b\u524d\u5728\u5fae\u5e97\u4e70\u4e86\u5c0f\u5218\uff0c\u8fd9\u6b21\u4e70\u4e86\u8001\u5218\uff0c\u8fd8\u6709\u4e00\u76f4\u559c\u6b22\u7684\u5de7\u514b\u529b\u78c5\u86cb\u7cd5\u3002\u597d\u5947\u8001\u677f\u4e3a\u5565\u4e0d\u505a\n\u67e0\u6aac\u78c5\u86cb\u7cd5\u4e86\uff0c\u5fae\u5e97\u4e00\u76f4\u90fd\u662f\u4e70\u4e0d\u5230\u7684\u72b6\u6001\u3002\u56e0\u4e3a\u4e0d\u7231\u78b1\u6c34\u786c\u6b27\u4e4b\u7c7b\u7684\uff0c\u6240\u4ee5\u671f\u5f85\u8001\u677f\u591a\u6765\n\u70b9\u5176\u4ed6\u5c0f\u70b9\uff0c\u997c\u5e72\u4e00\u76f4\u4e5f\u662f\u5927\u7231\uff0c\u90a3\u5929\u597d\u50cf\u4e5f\u6ca1\u770b\u5230': '\u6c34\u679c',\n    '\u670d\u52a1\u5f88\u7528\u5fc3\uff0c\u623f\u578b\u4e5f\u5f88\u8212\u670d\uff0c\u5c0f\u670b\u53cb\u5f88\u559c\u6b22\uff0c\u4e0b\u6b21\u53bb\u5609\u5b9a\u8fd8\u4f1a\u518d\u9009\u62e9\u3002\u5e8a\u94fa\u67d4\u8f6f\u8212\n\u9002\uff0c\u665a\u4e0a\u4f11\u606f\u5f88\u5b89\u9038\uff0c\u9694\u97f3\u6548\u679c\u4e0d\u9519\u8d5e\uff0c\u4e0b\u6b21\u8fd8\u4f1a\u6765': '\u9152\u5e97'\n}\n</code></pre>"},{"location":"03-%E6%96%B0%E9%9B%B6%E5%94%AE%E8%A1%8C%E4%B8%9A%E8%AF%84%E4%BB%B7%E5%86%B3%E7%AD%96%E7%B3%BB%E7%BB%9F/04-BERT%2BPET%E6%96%B9%E5%BC%8F%E6%A8%A1%E5%9E%8B%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0%E4%B8%8E%E8%AE%AD%E7%BB%83.html#_3","title":"\u5c0f\u8282\u603b\u7ed3","text":"<ul> <li>\u672c\u5c0f\u8282\u5b9e\u73b0\u4e86\u57fa\u4e8eBERT+PET\u6a21\u578b\u7684\u6784\u5efa, \u5e76\u5b8c\u6210\u4e86\u8bad\u7ec3\u548c\u6d4b\u8bd5\u8bc4\u4f30.</li> </ul>"},{"location":"03-%E6%96%B0%E9%9B%B6%E5%94%AE%E8%A1%8C%E4%B8%9A%E8%AF%84%E4%BB%B7%E5%86%B3%E7%AD%96%E7%B3%BB%E7%BB%9F/05-BERT%2BP-Tuning%E6%96%B9%E5%BC%8F%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E4%BB%8B%E7%BB%8D.html","title":"\u57fa\u4e8eBERT+P-Tuning\u65b9\u5f0f\u6587\u672c\u5206\u7c7b\u4ecb\u7ecd","text":""},{"location":"03-%E6%96%B0%E9%9B%B6%E5%94%AE%E8%A1%8C%E4%B8%9A%E8%AF%84%E4%BB%B7%E5%86%B3%E7%AD%96%E7%B3%BB%E7%BB%9F/05-BERT%2BP-Tuning%E6%96%B9%E5%BC%8F%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E4%BB%8B%E7%BB%8D.html#_1","title":"\u5b66\u4e60\u76ee\u6807","text":"<ul> <li>\u7406\u89e3P-Tuning\u65b9\u5f0f\u7684\u601d\u60f3</li> <li>\u4e86\u89e3\u57fa\u4e8eBERT+P-Tuning\u65b9\u5f0f\u5b9e\u73b0\u6587\u672c\u5206\u7c7b\u7684\u6574\u4f53\u9879\u76ee\u67b6\u6784</li> </ul>"},{"location":"03-%E6%96%B0%E9%9B%B6%E5%94%AE%E8%A1%8C%E4%B8%9A%E8%AF%84%E4%BB%B7%E5%86%B3%E7%AD%96%E7%B3%BB%E7%BB%9F/05-BERT%2BP-Tuning%E6%96%B9%E5%BC%8F%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E4%BB%8B%E7%BB%8D.html#1","title":"1 \u9879\u76ee\u80cc\u666f","text":"<ul> <li>\u672c\u7ae0\u6211\u4eec\u5c06\u4ee5\"\u7535\u5546\u5e73\u53f0\u7528\u6237\u8bc4\u8bba\"\u4e3a\u80cc\u666f\uff0c\u57fa\u4e8eBERT+P-Tuning\uff08\u8f6f\u6a21\u7248\uff09\u65b9\u6cd5\u5b9e\u73b0\u8bc4\u8bba\u6587\u672c\u7684\u51c6\u786e\u5206\u7c7b\u3002</li> <li>\u901a\u8fc7\u6df1\u5165\u4e86\u89e3\u7528\u6237\u5bf9\u4e0d\u540c\u5546\u54c1\u6216\u670d\u52a1\u7684\u8bc4\u4ef7\uff0c\u5e73\u53f0\u80fd\u591f\u5feb\u901f\u56de\u5e94\u7528\u6237\u9700\u6c42\uff0c\u6539\u8fdb\u4ea7\u54c1\u548c\u670d\u52a1\u3002\u81ea\u52a8\u5206\u7c7b\u4e5f\u4e3a\u4e2a\u6027\u5316\u63a8\u8350\u5960\u5b9a\u57fa\u7840\uff0c\u5e2e\u52a9\u7528\u6237\u66f4\u8f7b\u677e\u5730\u627e\u5230\u7b26\u5408\u5176\u504f\u597d\u7684\u5546\u54c1\u3002\u540c\u65f6\uff0c\u8fd9\u9879\u6280\u672f\u964d\u4f4e\u4e86\u8fd0\u8425\u6210\u672c\uff0c\u66ff\u4ee3\u4e86\u7e41\u91cd\u7684\u4eba\u5de5\u5904\u7406\u5de5\u4f5c\u3002\u901a\u8fc7\u8bc4\u8bba\u5206\u6790\uff0c\u7535\u5546\u5e73\u53f0\u8fd8\u80fd\u8fc5\u901f\u83b7\u53d6\u5e02\u573a\u53cd\u9988\uff0c\u4e3a\u5546\u5bb6\u63d0\u4f9b\u6709\u9488\u5bf9\u6027\u7684\u6570\u636e\uff0c\u52a9\u529b\u5236\u5b9a\u7cbe\u51c6\u7684\u8fd0\u8425\u7b56\u7565\u3002</li> </ul>"},{"location":"03-%E6%96%B0%E9%9B%B6%E5%94%AE%E8%A1%8C%E4%B8%9A%E8%AF%84%E4%BB%B7%E5%86%B3%E7%AD%96%E7%B3%BB%E7%BB%9F/05-BERT%2BP-Tuning%E6%96%B9%E5%BC%8F%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E4%BB%8B%E7%BB%8D.html#2-p-tuning","title":"2 P-Tuning\u56de\u987e","text":"<ul> <li>P-Tuning\uff08Pattern-Tuning\uff09\u662f\u4e00\u79cd\u8fde\u7eed\u7a7a\u95f4\u53ef\u5b66\u4e60\u6a21\u677f\uff0cPET\u7684\u76ee\u7684\u89e3\u51b3PET\u7684\u7f3a\u70b9\uff0c\u4f7f\u7528\u53ef\u5b66\u4e60\u7684\u5411\u91cf\u4f5c\u4e3a\u4f2a\u6a21\u677f\uff0c\u4e0d\u518d\u624b\u52a8\u6784\u5efa\u6a21\u677f\u3002</li> </ul> <p>\u4ee5\u65b0\u95fb\u5206\u7c7b\u4efb\u52a1\u4e3a\u4f8b\uff1a\u539f\u59cb\u6587\u672c\uff1a\u4e2d\u56fd\u5973\u6392\u518d\u593a\u51a0\uff01P-Tuning\u53ef\u5b66\u4e60\u6a21\u677f\uff1a[u1] [u2] \u2026[MASK]\u2026[un], Label\uff1a\u4f53\u80b2/\u8d22\u7ecf/\u65f6\u653f/\u519b\u4e8b</p> <ul> <li>P-tuning \u7684\u6838\u5fc3\u601d\u60f3\u662f\uff1a\u7528\u4e00\u4e2a\u5c0f\u7684\u53ef\u8bad\u7ec3\u6a21\u5757\u628a\u4e00\u7ec4\u201c\u8fde\u7eed\u63d0\u793a\u5411\u91cf\u201d\u751f\u6210\u5e76\u63d2\u5165\u5230\u539f\u59cb\u8f93\u5165 embedding \u4e2d\uff0c\u4ee4**\u51bb\u7ed3\u7684\u9884\u8bad\u7ec3\u6a21\u578b**\u5728\u4e0b\u6e38\u4efb\u52a1\u4e0a\u4ea7\u751f\u6b63\u786e\u8f93\u51fa\uff0c\u8bad\u7ec3\u65f6\u4ec5\u66f4\u65b0 prompt encoder\uff08\u6216\u63d0\u793a\u5411\u91cf\uff09\uff0c\u4ece\u800c\u5b9e\u73b0\u4f4e\u6210\u672c\u9ad8\u6548\u7684\u8c03\u4f18\u3002</li> </ul>"},{"location":"03-%E6%96%B0%E9%9B%B6%E5%94%AE%E8%A1%8C%E4%B8%9A%E8%AF%84%E4%BB%B7%E5%86%B3%E7%AD%96%E7%B3%BB%E7%BB%9F/05-BERT%2BP-Tuning%E6%96%B9%E5%BC%8F%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E4%BB%8B%E7%BB%8D.html#3","title":"3  \u73af\u5883\u51c6\u5907","text":"<p>\u672c\u9879\u76ee\u57fa\u4e8e pytorch + transformers \u5b9e\u73b0\uff0c\u8fd0\u884c\u524d\u8bf7\u5b89\u88c5\u76f8\u5173\u4f9d\u8d56\u5305\uff1a</p> <ul> <li>python==3.10</li> <li>transformers==4.40.2</li> <li>torch==2.5.1+cu121</li> <li>datasets==3.6.0</li> <li>scikit-learn==1.7.0</li> </ul>"},{"location":"03-%E6%96%B0%E9%9B%B6%E5%94%AE%E8%A1%8C%E4%B8%9A%E8%AF%84%E4%BB%B7%E5%86%B3%E7%AD%96%E7%B3%BB%E7%BB%9F/05-BERT%2BP-Tuning%E6%96%B9%E5%BC%8F%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E4%BB%8B%E7%BB%8D.html#4","title":"4 \u9879\u76ee\u67b6\u6784","text":"<ul> <li>\u9879\u76ee\u67b6\u6784\u6d41\u7a0b\u56fe\uff1a</li> </ul> <ul> <li>\u9879\u76ee\u6574\u4f53\u4ee3\u7801\u4ecb\u7ecd\uff1a</li> </ul>"},{"location":"03-%E6%96%B0%E9%9B%B6%E5%94%AE%E8%A1%8C%E4%B8%9A%E8%AF%84%E4%BB%B7%E5%86%B3%E7%AD%96%E7%B3%BB%E7%BB%9F/05-BERT%2BP-Tuning%E6%96%B9%E5%BC%8F%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E4%BB%8B%E7%BB%8D.html#_2","title":"\u5c0f\u7ed3\u603b\u7ed3","text":"<p>\u672c\u7ae0\u8282\u4e3b\u8981\u4ecb\u7ecd\u4e86\u9879\u76ee\u5f00\u53d1\u7684\u80cc\u666f\u53ca\u610f\u4e49\uff0c\u660e\u786e\u4e86\u9879\u76ee\u7684\u6574\u4f53\u67b6\u6784\uff0c\u5e76\u5bf9\u9879\u76ee\u4e2d\u6574\u4f53\u4ee3\u7801\u7ed3\u6784\u8fdb\u884c\u4e86\u4ecb\u7ecd\u3002</p>"},{"location":"03-%E6%96%B0%E9%9B%B6%E5%94%AE%E8%A1%8C%E4%B8%9A%E8%AF%84%E4%BB%B7%E5%86%B3%E7%AD%96%E7%B3%BB%E7%BB%9F/06-BERT%2BP-Tuning%E6%96%B9%E5%BC%8F%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E4%BB%8B%E7%BB%8D.html","title":"3.6 BERT+P-Tuning\u65b9\u5f0f\u6570\u636e\u9884\u5904\u7406","text":""},{"location":"03-%E6%96%B0%E9%9B%B6%E5%94%AE%E8%A1%8C%E4%B8%9A%E8%AF%84%E4%BB%B7%E5%86%B3%E7%AD%96%E7%B3%BB%E7%BB%9F/06-BERT%2BP-Tuning%E6%96%B9%E5%BC%8F%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E4%BB%8B%E7%BB%8D.html#bertp-tuning","title":"\u57fa\u4e8eBERT+P-Tuning\u65b9\u5f0f\u6570\u636e\u9884\u5904\u7406\u4ecb\u7ecd","text":""},{"location":"03-%E6%96%B0%E9%9B%B6%E5%94%AE%E8%A1%8C%E4%B8%9A%E8%AF%84%E4%BB%B7%E5%86%B3%E7%AD%96%E7%B3%BB%E7%BB%9F/06-BERT%2BP-Tuning%E6%96%B9%E5%BC%8F%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E4%BB%8B%E7%BB%8D.html#_1","title":"\u5b66\u4e60\u76ee\u6807","text":"<ul> <li>\u4e86\u89e3\u672c\u9879\u76ee\u6570\u636e\u7c7b\u578b\u548c\u8868\u73b0\u683c\u5f0f</li> <li>\u638c\u63e1\u6570\u636e\u5904\u7406\u7684\u5de5\u5177\u51fd\u6570\u4ee3\u7801\u5b9e\u73b0</li> </ul>"},{"location":"03-%E6%96%B0%E9%9B%B6%E5%94%AE%E8%A1%8C%E4%B8%9A%E8%AF%84%E4%BB%B7%E5%86%B3%E7%AD%96%E7%B3%BB%E7%BB%9F/06-BERT%2BP-Tuning%E6%96%B9%E5%BC%8F%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E4%BB%8B%E7%BB%8D.html#bertp-tuning_1","title":"BERT+P-Tuning\u65b9\u5f0f\u6570\u636e\u9884\u5904\u7406","text":"<p>\u672c\u9879\u76ee\u4e2d\u5bf9\u6570\u636e\u90e8\u5206\u7684\u9884\u5904\u7406\u6b65\u9aa4\u5982\u4e0b: - 1.\u67e5\u770b\u9879\u76ee\u6570\u636e\u96c6 - 2.\u7f16\u5199Config\u7c7b\u9879\u76ee\u6587\u4ef6\u914d\u7f6e\u4ee3\u7801 - 3.\u7f16\u5199\u6570\u636e\u5904\u7406\u76f8\u5173\u4ee3\u7801</p>"},{"location":"03-%E6%96%B0%E9%9B%B6%E5%94%AE%E8%A1%8C%E4%B8%9A%E8%AF%84%E4%BB%B7%E5%86%B3%E7%AD%96%E7%B3%BB%E7%BB%9F/06-BERT%2BP-Tuning%E6%96%B9%E5%BC%8F%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E4%BB%8B%E7%BB%8D.html#1","title":"1 \u67e5\u770b\u9879\u76ee\u6570\u636e\u96c6","text":"<ul> <li> <p>\u6570\u636e\u5b58\u653e\u4f4d\u7f6e\uff1allm_tuning/prompt_tasks/P-Tuning/data</p> </li> <li> <p>data\u6587\u4ef6\u5939\u91cc\u9762\u5305\u542b3\u4e2atxt\u6587\u6863\uff0c\u5206\u522b\u4e3a\uff1atrain.txt\u3001dev.txt\u3001verbalizer.txt</p> </li> </ul>"},{"location":"03-%E6%96%B0%E9%9B%B6%E5%94%AE%E8%A1%8C%E4%B8%9A%E8%AF%84%E4%BB%B7%E5%86%B3%E7%AD%96%E7%B3%BB%E7%BB%9F/06-BERT%2BP-Tuning%E6%96%B9%E5%BC%8F%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E4%BB%8B%E7%BB%8D.html#11-traintxt","title":"1.1 train.txt","text":"<ul> <li>train.txt\u4e3a\u8bad\u7ec3\u6570\u636e\u96c6\uff0c\u5176\u90e8\u5206\u6570\u636e\u5c55\u793a\u5982\u4e0b\uff1a</li> </ul> <pre><code>\u6c34\u679c  \u8106\u8106\u7684\uff0c\u751c\u5473\u53ef\u4ee5\uff0c\u53ef\u80fd\u65f6\u95f4\u6709\u70b9\u957f\u4e86\uff0c\u6c34\u5206\u4e0d\u662f\u5f88\u8db3\u3002\n\u5e73\u677f  \u534e\u4e3a\u673a\u5668\u80af\u5b9a\u4e0d\u9519\uff0c\u4f46\u7b2c\u4e00\u6b21\u78b0\u4e0a\u4eac\u4e1c\u6700\u7cdf\u7cd5\u7684\u670d\u52a1\uff0c\u4ee5\u540e\u4e0d\u60f3\u5230\u4eac\u4e1c\u8d2d\u7269\u4e86\u3002\n\u4e66\u7c4d  \u4e3a\u4ec0\u4e48\u4e0d\u8ba4\u771f\u7684\u68c0\u67e5\u4e00\u4e0b\uff0c \u53d1\u8fd9\u4e48\u4e00\u672c\u810f\u810f\u7684\u4e66\u7ed9\u987e\u5ba2\u5462\uff01\n\u8863\u670d  \u624b\u611f\u4e0d\u9519\uff0c\u7528\u6599\u4e5f\u5f88\u597d\uff0c\u4e0d\u77e5\u9053\u6c34\u6d17\u540e\u600e\u6837\uff0c\u76f8\u4fe1\u5927\u54c1\u724c\uff0c\u8d28\u91cf\u8fc7\u5173\uff0c\u4e94\u661f\u597d\u8bc4\uff01\uff01\uff01\n\u6c34\u679c  \u82f9\u679c\u6709\u70b9\u5c0f\uff0c\u4e0d\u8fc7\u597d\u5403\uff0c\u8fd8\u6709\u51e0\u4e2a\u70c2\u7684\u3002\u4f30\u8ba1\u662f\u6545\u610f\u7684\u653e\u7684\u3002\u5dee\u8bc4\u3002\n\u8863\u670d  \u6389\u8272\u6389\u7684\u5389\u5bb3\uff0c\u6d17\u4e00\u6b21\u5c31\u82b1\u4e86\n</code></pre> <p>train.txt\u4e00\u5171\u5305\u542b63\u6761\u6837\u672c\u6570\u636e\uff0c\u6bcf\u4e00\u884c\u7528<code>\\t</code>\u5206\u5f00\uff0c\u524d\u534a\u90e8\u5206\u4e3a\u6807\u7b7e(label)\uff0c\u540e\u534a\u90e8\u5206\u4e3a\u539f\u59cb\u8f93\u5165 (\u7528\u6237\u8bc4\u8bba)\u3002</p> <p>\u5982\u679c\u60f3\u4f7f\u7528\u81ea\u5b9a\u4e49\u6570\u636e\u8bad\u7ec3\uff0c\u53ea\u9700\u8981\u4eff\u7167\u4e0a\u8ff0\u793a\u4f8b\u6570\u636e\u6784\u5efa\u6570\u636e\u96c6\u5373\u53ef\u3002</p>"},{"location":"03-%E6%96%B0%E9%9B%B6%E5%94%AE%E8%A1%8C%E4%B8%9A%E8%AF%84%E4%BB%B7%E5%86%B3%E7%AD%96%E7%B3%BB%E7%BB%9F/06-BERT%2BP-Tuning%E6%96%B9%E5%BC%8F%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E4%BB%8B%E7%BB%8D.html#12-devtxt","title":"1.2 dev.txt","text":"<ul> <li>dev.txt\u4e3a\u9a8c\u8bc1\u6570\u636e\u96c6\uff0c\u5176\u90e8\u5206\u6570\u636e\u5c55\u793a\u5982\u4e0b\uff1a</li> </ul> <pre><code>\u4e66\u7c4d  \"\u4e00\u70b9\u90fd\u4e0d\u597d\u7b11,\u5f88\u5931\u671b,\u5185\u5bb9\u4e5f\u4e0d\u662f\u5f88\u5b9e\u7528\"\n\u8863\u670d  \u5b8c\u5168\u662f\u4e00\u6761\u65e7\u88e4\u5b50\u3002\n\u624b\u673a  \u76f8\u673a\u8d28\u91cf\u4e0d\u9519\uff0c\u5982\u679c\u9633\u5149\u5145\u8db3\uff0c\u53ef\u4ee5\u548c\u6570\u7801\u76f8\u673a\u5ab2\u7f8e\uff0e\u754c\u9762\u6bd4\u8f83\u4eba\u6027\u5316\uff0c\u5bb9\u6613\u4f7f\u7528\uff0e\u8f6f\u4ef6\u5b89\u88c5\u7b80\u4fbf\n\u4e66\u7c4d  \u660e\u660e\u8bf4\u6709\u8d27\uff0c\u7ed3\u679c\u9001\u8d27\u53c8\u6ca1\u6709\u4e86\u3002\u5e76\u4e14\u4e5f\u4e0d\u544a\u8bc9\u6211\uff0c\u600e\u4e48\u8bc4\u554a\n\u6d17\u6d74  \u975e\u5e38\u4e0d\u6ee1\u610f\uff0c\u665a\u4e0a\u6d17\u7684\u5934\u53d1\uff0c\u7b2c\u4e8c\u5929\u5934\u75d2\u75d2\u7684\u4e0d\u884c\u4e86\uff0c\u8fd8\u90fd\u662f\u5934\u76ae\u5c51\u3002\n\u6c34\u679c  \u8fd9\u4e2a\u82f9\u679c\u611f\u89c9\u662f\u957f\u719f\u7684\u82f9\u679c\uff0c\u6ca1\u6709\u6253\u8721\uff0c\u4e0d\u9519\uff0c\u53c8\u751c\u53c8\u8106\n</code></pre> <p>dev.txt\u4e00\u5171\u5305\u542b417\u6761\u6837\u672c\u6570\u636e\uff0c\u6bcf\u4e00\u884c\u7528<code>\\t</code>\u5206\u5f00\uff0c\u524d\u534a\u90e8\u5206\u4e3a\u6807\u7b7e(label)\uff0c\u540e\u534a\u90e8\u5206\u4e3a\u539f\u59cb\u8f93\u5165 (\u7528\u6237\u8bc4\u8bba)\u3002</p> <p>\u5982\u679c\u60f3\u4f7f\u7528\u81ea\u5b9a\u4e49\u6570\u636e\u8bad\u7ec3\uff0c\u53ea\u9700\u8981\u4eff\u7167\u4e0a\u8ff0\u793a\u4f8b\u6570\u636e\u6784\u5efa\u6570\u636e\u96c6\u5373\u53ef\u3002</p>"},{"location":"03-%E6%96%B0%E9%9B%B6%E5%94%AE%E8%A1%8C%E4%B8%9A%E8%AF%84%E4%BB%B7%E5%86%B3%E7%AD%96%E7%B3%BB%E7%BB%9F/06-BERT%2BP-Tuning%E6%96%B9%E5%BC%8F%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E4%BB%8B%E7%BB%8D.html#13-verbalizertxt","title":"1.3 verbalizer.txt","text":"<ul> <li> <p>verbalizer.txt \u4e3b\u8981\u7528\u4e8e\u5b9a\u4e49\u300c\u771f\u5b9e\u6807\u7b7e\u300d\u5230\u300c\u6807\u7b7e\u9884\u6d4b\u8bcd\u300d\u4e4b\u95f4\u7684\u6620\u5c04\u3002\u5728\u6709\u4e9b\u60c5\u51b5\u4e0b\uff0c\u5c06\u300c\u771f\u5b9e\u6807\u7b7e\u300d\u4f5c\u4e3a [MASK] \u53bb\u9884\u6d4b\u53ef\u80fd\u4e0d\u5177\u5907\u5f88\u597d\u7684\u8bed\u4e49\u901a\u987a\u6027\uff0c\u56e0\u6b64\uff0c\u6211\u4eec\u4f1a\u5bf9\u300c\u771f\u5b9e\u6807\u7b7e\u300d\u505a\u4e00\u5b9a\u7684\u6620\u5c04\u3002</p> </li> <li> <p>\u4f8b\u5982\uff1a</p> </li> </ul> <pre><code>\"\u4e2d\u56fd\u7206\u51b72-1\u6218\u80dc\u97e9\u56fd\"\u662f\u4e00\u5219[MASK][MASK]\u65b0\u95fb\u3002 \u4f53\u80b2\n</code></pre> <ul> <li> <p>\u8fd9\u53e5\u8bdd\u4e2d\u7684\u6807\u7b7e\u4e3a\u300c\u4f53\u80b2\u300d\uff0c\u4f46\u5982\u679c\u6211\u4eec\u5c06\u6807\u7b7e\u8bbe\u7f6e\u4e3a\u300c\u8db3\u7403\u300d\u4f1a\u66f4\u5bb9\u6613\u9884\u6d4b\u3002</p> </li> <li> <p>\u56e0\u6b64\uff0c\u6211\u4eec\u53ef\u4ee5\u5bf9\u300c\u4f53\u80b2\u300d\u8fd9\u4e2a label \u6784\u5efa\u8bb8\u591a\u4e2a\u5b50\u6807\u7b7e\uff0c\u5728\u63a8\u7406\u65f6\uff0c\u53ea\u8981\u9884\u6d4b\u5230\u5b50\u6807\u7b7e\u6700\u7ec8\u63a8\u7406\u51fa\u771f\u5b9e\u6807\u7b7e\u5373\u53ef\uff0c\u5982\u4e0b\uff1a</p> </li> </ul> <pre><code>\u4f53\u80b2 -&gt; \u8db3\u7403,\u7bee\u7403,\u7f51\u7403,\u68d2\u7403,\u4e52\u4e53,\u4f53\u80b2\n</code></pre> <ul> <li>\u9879\u76ee\u4e2d\u6807\u7b7e\u8bcd\u6620\u5c04\u6570\u636e\u5c55\u793a\u5982\u4e0b\uff1a</li> </ul> <pre><code>\u7535\u8111  \u7535\u8111\n\u6c34\u679c  \u6c34\u679c\n\u5e73\u677f  \u5e73\u677f\n\u8863\u670d  \u8863\u670d\n\u9152\u5e97  \u9152\u5e97\n\u6d17\u6d74  \u6d17\u6d74\n\u4e66\u7c4d  \u4e66\u7c4d\n\u8499\u725b  \u8499\u725b\n\u624b\u673a  \u624b\u673a\n\u7535\u5668  \u7535\u5668\n</code></pre> <p>verbalizer.txt \u4e00\u5171\u5305\u542b10\u4e2a\u7c7b\u522b\uff0c\u4e0a\u8ff0\u6570\u636e\u4e2d\uff0c\u6211\u4eec\u4f7f\u7528\u4e861\u5bf91\u7684verbalizer, \u5982\u679c\u60f3\u5b9a\u4e49\u4e00\u5bf9\u591a\u7684\u6620\u5c04\uff0c\u53ea\u9700\u8981\u5728\u540e\u9762\u7528\",\"\u5206\u5272\u5373\u53ef\uff0c eg: </p> <pre><code>\u6c34\u679c    \u82f9\u679c,\u9999\u8549,\u6a58\u5b50\n</code></pre> <p>\u82e5\u60f3\u4f7f\u7528\u81ea\u5b9a\u4e49\u6570\u636e\u8bad\u7ec3\uff0c\u53ea\u9700\u8981\u4eff\u7167\u793a\u4f8b\u6570\u636e\u6784\u5efa\u6570\u636e\u96c6</p>"},{"location":"03-%E6%96%B0%E9%9B%B6%E5%94%AE%E8%A1%8C%E4%B8%9A%E8%AF%84%E4%BB%B7%E5%86%B3%E7%AD%96%E7%B3%BB%E7%BB%9F/06-BERT%2BP-Tuning%E6%96%B9%E5%BC%8F%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E4%BB%8B%E7%BB%8D.html#2-config","title":"2 \u7f16\u5199Config\u7c7b\u9879\u76ee\u6587\u4ef6\u914d\u7f6e\u4ee3\u7801","text":"<ul> <li> <p>\u4ee3\u7801\u8def\u5f84\uff1allm_tuning/prompt_tasks/P-Tuning/ptune_config.py</p> </li> <li> <p>config\u6587\u4ef6\u76ee\u7684\uff1a\u914d\u7f6e\u9879\u76ee\u5e38\u7528\u53d8\u91cf\uff0c\u4e00\u822c\u8fd9\u4e9b\u53d8\u91cf\u5c5e\u4e8e\u4e0d\u7ecf\u5e38\u6539\u53d8\u7684\uff0c\u6bd4\u5982\uff1a\u8bad\u7ec3\u6587\u4ef6\u8def\u5f84\u3001\u6a21\u578b\u8bad\u7ec3\u6b21\u6570\u3001\u6a21\u578b\u8d85\u53c2\u6570\u7b49\u7b49</p> </li> </ul> <p>\u5177\u4f53\u4ee3\u7801\u5b9e\u73b0\uff1a</p> <pre><code>import torch\nimport os\n\nbase_dir = os.path.dirname(os.path.abspath(__file__))\n# print(f'base_dir--&gt;{base_dir}')\n\n\nclass ProjectConfig(object):\n    def __init__(self):\n        # \u8bbe\u7f6e\u8bbe\u5907\u4e3aCUDA:0\uff08\u5982\u679c\u53ef\u7528\uff09\uff0c\u5426\u5219\u8bbe\u7f6e\u4e3aCPU\n        self.device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n        # self.device = \"mps:0\"\n        # \u8bbe\u7f6e\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u8def\u5f84\n        self.pre_model = os.path.join(base_dir, '../../bert-base-chinese')\n        # \u8bbe\u7f6e\u8bad\u7ec3\u6570\u636e\u96c6\u7684\u8def\u5f84\n        self.train_path = os.path.join(base_dir, 'data/train.txt')\n        # \u8bbe\u7f6e\u9a8c\u8bc1\u6570\u636e\u96c6\u7684\u8def\u5f84\n        self.dev_path = os.path.join(base_dir, 'data/dev.txt')\n        # \u8bbe\u7f6everbalizer\u6587\u4ef6\u7684\u8def\u5f84\uff0c\u7528\u4e8e\u5c06\u6807\u7b7e\u6620\u5c04\u5230\u6587\u672c\n        self.verbalizer = os.path.join(base_dir, 'data/verbalizer.txt')\n        # \u8bbe\u7f6e\u6700\u5927\u5e8f\u5217\u957f\u5ea6\n        self.max_seq_len = 512\n        # \u8bbe\u7f6e\u6279\u91cf\u5927\u5c0f\n        self.batch_size = 8\n        # \u8bbe\u7f6e\u5b66\u4e60\u7387\n        self.learning_rate = 5e-5\n        # \u6743\u91cd\u8870\u51cf\u7cfb\u6570\n        self.weight_decay = 0\n        # \u5b66\u4e60\u7387\u9884\u70ed\u7684\u7cfb\u6570\n        self.warmup_ratio = 0.06\n        # \u4f2atoken\u7684\u4e2a\u6570\n        self.p_embedding_num = 6\n        # \u6700\u5927\u6807\u7b7e\u957f\u5ea6\n        self.max_label_len = 2\n        # \u8bbe\u7f6e\u8bad\u7ec3\u7684\u8f6e\u6570\n        self.epochs = 50\n        # \u8bbe\u7f6e\u65e5\u5fd7\u8bb0\u5f55\u7684\u6b65\u6570\n        self.logging_steps = 10\n        # \u8bbe\u7f6e\u9a8c\u8bc1\u7684\u6b65\u6570\n        self.valid_steps = 20\n        # \u8bbe\u7f6e\u4fdd\u5b58\u6a21\u578b\u7684\u76ee\u5f55\n        self.save_dir = os.path.join(base_dir, 'save_model')\n\n\nif __name__ == '__main__':\n    pc = ProjectConfig()\n    print(pc.verbalizer)\n</code></pre>"},{"location":"03-%E6%96%B0%E9%9B%B6%E5%94%AE%E8%A1%8C%E4%B8%9A%E8%AF%84%E4%BB%B7%E5%86%B3%E7%AD%96%E7%B3%BB%E7%BB%9F/06-BERT%2BP-Tuning%E6%96%B9%E5%BC%8F%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E4%BB%8B%E7%BB%8D.html#3","title":"3 \u7f16\u5199\u6570\u636e\u5904\u7406\u76f8\u5173\u4ee3\u7801","text":"<ul> <li> <p>\u4ee3\u7801\u8def\u5f84\uff1allm_tuning/prompt_tasks/P-Tuning/data_handle/</p> </li> <li> <p>data_handle\u6587\u4ef6\u5939\u4e2d\u4e00\u5171\u5305\u542b\u4e24\u4e2apy\u811a\u672c\uff1adata_preprocess.py\u3001data_loader.py</p> </li> </ul>"},{"location":"03-%E6%96%B0%E9%9B%B6%E5%94%AE%E8%A1%8C%E4%B8%9A%E8%AF%84%E4%BB%B7%E5%86%B3%E7%AD%96%E7%B3%BB%E7%BB%9F/06-BERT%2BP-Tuning%E6%96%B9%E5%BC%8F%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E4%BB%8B%E7%BB%8D.html#31-data_preprocesspy","title":"3.1 data_preprocess.py","text":"<ul> <li>\u76ee\u7684: \u5c06\u6a21\u677f\u4e0e\u539f\u59cb\u8f93\u5165\u6587\u672c\u8fdb\u884c\u62fc\u63a5\uff0c\u6784\u9020\u6a21\u578b\u63a5\u53d7\u7684\u8f93\u5165\u6570\u636e</li> <li>\u4ee3\u7801\u5982\u4e0b\uff1a</li> </ul> <pre><code>import torch\nimport numpy as np\nfrom datasets import load_dataset\nfrom transformers import AutoTokenizer\n\nfrom prompt_tasks.P_Tuning.ptune_config import ProjectConfig\n\n\ndef convert_example(\n        examples: dict,\n        tokenizer,\n        max_seq_len: int,\n        max_label_len: int,\n        p_embedding_num=6,\n        train_mode=True,\n        return_tensor=False\n) -&gt; dict:\n    '''\n    \u5c06\u6837\u672c\u6570\u636e\u8f6c\u6362\u4e3a\u6a21\u578b\u63a5\u6536\u7684\u8f93\u5165\u6570\u636e\u3002\n    :param examples: (dict): \u8bad\u7ec3\u6570\u636e\u6837\u672c\n    e.g. -&gt; {\n                \"text\": [\n                            '\u5a31\u4e50 \u55e8\u653e\u6d3e\u600e\u4e48\u505c\u64ad\u4e86',\n                            '\u4f53\u80b2 \u4e16\u754c\u676f\u4e3a\u4f55\u8fdf\u8fdf\u4e0d\u89c1\u5ba3\u4f20',\n                            ...\n                ]\n            }\n    :param tokenizer: \u5206\u8bcd\u5668\n    :param max_seq_len: \u6700\u5927\u53e5\u5b50\u957f\u5ea6\n    :param max_label_len: (int): \u6700\u5927label\u957f\u5ea6\uff0c\u82e5\u6ca1\u6709\u8fbe\u5230\u6700\u5927\u957f\u5ea6\uff0c\u5219padding\u4e3a\u6700\u5927\u957f\u5ea6\n    :param p_embedding_num: (int): p-tuning token \u7684\u4e2a\u6570\n    :param train_mode: \u8bad\u7ec3\u9636\u6bb5 or \u63a8\u7406\u9636\u6bb5\n    :param return_tensor: \u662f\u5426\u8fd4\u56detensor\u7c7b\u578b\uff0c\u5982\u4e0d\u662f\uff0c\u5219\u8fd4\u56denumpy\u7c7b\u578b\u3002\n    :return:\n    dict (str: np.array) -&gt; tokenized_output = {\n                    'input_ids': [[101, 3928, ...], [101, 4395, ...]],\n                    'token_type_ids': [[0, 0, ...], [0, 0, ...]],\n                    'mask_positions': [[5, 6, ...], [3, 4, ...]],\n                    'mask_labels': [[183, 234], [298, 322], ...]\n                }\n    '''\n    tokenized_output = {\n        'input_ids': [],\n        'attention_mask': [],\n        'mask_positions': [],  # \u8bb0\u5f55label\u7684\u4f4d\u7f6e\uff08\u5373MASK Token\u7684\u4f4d\u7f6e\uff09\n        'mask_labels': []  # \u8bb0\u5f55MASK Token\u7684\u539f\u59cb\u503c\uff08\u5373Label\u503c\uff09\n    }\n\n    # print(f\"examples['text']--&gt;{examples['text']}\")\n    # \u904d\u5386\u6587\u672c\u6570\u636e\u96c6\uff0c\u5176\u4e2d\u6bcf\u4e2a\u6587\u672c\u6570\u636e\u88ab\u8d4b\u4e88\u4e00\u4e2a\u7d22\u5f15\u548c\u503c\n    for i, example in enumerate(examples['text']):\n        try:\n            # \u5c06 prompt token(s) \u63d2\u5728 [CLS] \u4e4b\u540e\n            start_mask_position = 1\n            if train_mode:\n                # print(f\"example--&gt;{example}\")\n                # strip() \u65b9\u6cd5\u7528\u4e8e\u79fb\u9664\u5b57\u7b26\u4e32\u5934\u5c3e\u6307\u5b9a\u7684\u5b57\u7b26\uff08\u9ed8\u8ba4\u4e3a\u7a7a\u683c\uff09\uff0c\u8fd9\u91cc\u7528\u4e8e\u53bb\u9664\u53ef\u80fd\u5b58\u5728\u7684\u591a\u4f59\u7a7a\u683c\n                # split('\\t', 1) \u65b9\u6cd5\u7528\u4e8e\u6309\u7167\u5236\u8868\u7b26('\\t')\u5c06\u5b57\u7b26\u4e32\u5206\u5272\u6210\u4e24\u90e8\u5206\uff0c\u9650\u5236\u5206\u5272\u6b21\u6570\u4e3a1\uff0c\u786e\u4fdd\u53ea\u5206\u5272\u7b2c\u4e00\u4e2a\u5236\u8868\u7b26\n                label, content = example.strip().split('\\t', 1)\n                # print(f'label--&gt;{label}')\n                # print(f'content--&gt;{content}')\n\n                # \u4f7f\u7528tokenizer\u5bf9\u6807\u7b7e\u8fdb\u884c\u7f16\u7801\uff0clabel token \u8f6c id\n                mask_labels = tokenizer(text=label)\n                # print(f'mask_labels--&gt;{mask_labels}')\n                # \u4ece\u5b57\u5178\u4e2d\u83b7\u53d6input_ids\uff0c\u5e76\u4e22\u6389[CLS]\u548c[SEP]\n                mask_labels = mask_labels['input_ids'][1:-1]\n                # \u5c06 label \u957f\u5ea6\u9650\u5236\u4e3a\u6700\u957f\n                mask_labels = mask_labels[:max_label_len]\n                # \u5c06 label \u8865\u5230\u6700\u957f\n                mask_labels += [tokenizer.pad_token_id] * (max_label_len - len(mask_labels))  # \u5c06 label \u8865\u5230\u6700\u957f\n                # print(f'mask_labels--&gt;{mask_labels}')\n                # \u5c06\u7f16\u7801\u540e\u7684\u6807\u7b7e\u6dfb\u52a0\u5230tokenized_output\u5b57\u5178\u4e2d\u7684'mask_labels'\u5217\u8868\u4e2d\n                tokenized_output['mask_labels'].append(mask_labels)\n            else:\n                # \u5982\u679c\u4e0d\u662f\u8bad\u7ec3\u6a21\u5f0f\uff0c\u76f4\u63a5\u5c06\u6587\u672c\u5185\u5bb9\u8fdb\u884c\u4fee\u526a\u5e76\u4f7f\u7528\n                content = example.strip()\n            encoded_inputs = tokenizer(text=content,truncation=True,max_length=max_seq_len,padding='max_length')\n        except:\n            continue\n\n        # \u83b7\u53d6\u7f16\u7801\u540e\u7684\u8f93\u5165\n        input_ids = encoded_inputs['input_ids']\n        # print(f'input_ids--&gt;{input_ids}')\n        # print(f'\u539f\u59cb\u7684input_id\u7684\u957f\u5ea6--&gt;{len(input_ids)}')\n\n        # 1.\u751f\u6210 MASK Tokens, \u548clabel\u957f\u5ea6\u4e00\u81f4\n        mask_tokens = ['[MASK]'] * max_label_len\n        # print(f'mask_tokens--&gt;{mask_tokens}')\n        # \u5c06 MASK Tokens \u8f6c\u4e3a id\n        mask_ids = tokenizer.convert_tokens_to_ids(mask_tokens)\n        # print(f'mask_ids--&gt;{mask_ids}')\n\n        # 2.\u6784\u5efa prompt token(s)\n        # \u6839\u636ep_embedding_num\u751f\u6210\u5bf9\u5e94\u7684\u7279\u6b8atoken\u5217\u8868\n        p_tokens = [\"[unused{}]\".format(i + 1) for i in range(p_embedding_num)]\n        # print(f'p_tokens--&gt;{p_tokens}')\n        # token \u8f6c id\n        p_tokens_ids = tokenizer.convert_tokens_to_ids(p_tokens)\n        # print(f'p_tokens_ids--&gt;{p_tokens_ids}')\n\n        # \u6839\u636e \u6700\u5927\u957f\u5ea6-p_token\u957f\u5ea6-label\u957f\u5ea6-1\uff0c\u88c1\u526acontent\u7684\u957f\u5ea6 (\u88c1\u526a[SEP]\u524d\u7684token, \u6240\u4ee5-1)\n        tmp_input_ids = input_ids[:max_seq_len - len(mask_ids) - len(p_tokens_ids) - 1]\n        # print(f'tmp_input_ids1--&gt;{tmp_input_ids}')\n        # print(f'tmp_input_ids1--&gt;{len(tmp_input_ids)}')\n\n        # 3.\u63d2\u5165 MASK -&gt; [CLS][MASK][MASK]\u4e16\u754c\u676f...[SEP]\n        tmp_input_ids = tmp_input_ids[:start_mask_position] + mask_ids + tmp_input_ids[start_mask_position:] + [input_ids[-1]]\n        # print(f'\u63d2\u5165mask\u548csep\u4e4b\u540e\u7684 tmp_input_ids--{tmp_input_ids}')\n        # print(f'\u63d2\u5165mask\u548csep\u4e4b\u540e\u7684 tmp_input_ids\u957f\u5ea6--{len(tmp_input_ids)}')\n\n        # 4.\u63d2\u5165 prompt -&gt; [unused1][unused2]...[CLS][MASK]...[SEP]\n        input_ids = p_tokens_ids + tmp_input_ids\n        # print(f'\u63d2\u5165\u6a21\u7248\u4e4b\u540e\u7684 input_ids--&gt;{input_ids}')\n        # print(f'\u63d2\u5165\u6a21\u7248\u4e4b\u540e\u7684 input_ids\u957f\u5ea6--&gt;{len(input_ids)}')\n\n        # \u5c06\u65b0\u7684\u8f93\u5165\u6dfb\u52a0\u5230tokenized_output\u5b57\u5178\u4e2d\n        tokenized_output['input_ids'].append(input_ids)\n\n        # \u5c06 Mask Tokens \u7684\u4f4d\u7f6e\u8bb0\u5f55\u4e0b\u6765\n        mask_positions = [len(p_tokens_ids) + start_mask_position + i\n                          for i in range(max_label_len)]\n        # print(f'mask_positions--&gt;{mask_positions}')\n        # \u5c06 Mask Tokens \u7684\u4f4d\u7f6e\u8bb0\u5f55\u4e0b\u6765\n        tokenized_output['mask_positions'].append(mask_positions)\n\n        # \u5982\u679c\u8f93\u5165\u9700\u8981token_type_ids\uff0c\u53ef\u4ee5\u8fdb\u884c\u6dfb\u52a0\n        if 'token_type_ids' in encoded_inputs:  # \u517c\u5bb9\u4e0d\u9700\u8981 token_type_id \u7684\u6a21\u578b, e.g. Roberta-Base\n            tmp = encoded_inputs['token_type_ids']\n            if 'token_type_ids' not in tokenized_output:\n                tokenized_output['token_type_ids'] = [tmp]\n            else:\n                tokenized_output['token_type_ids'].append(tmp)\n\n        # print(f'\u539f\u59cb\u7684attention_mask--&gt;{encoded_inputs[\"attention_mask\"]}')\n        # \u4fee\u6539attention_mask\u76840\u548c1\u7684\u4f4d\u7f6e\uff0c\u56e0\u4e3a\u63d2\u5165\u4e86prompt\u548cMASK Tokens\uff0c\u5f71\u54cd\u4e86\u539f\u6765\u7684\u53e5\u5b50padding\u7684\u90e8\u5206\uff0c\u6240\u4ee5\u9700\u8981\u91cd\u65b0\u751f\u6210\n        attention_mask = get_attention_mask(input_ids)\n        # print(f'\u4fee\u6539\u7684attention_mask--&gt;{attention_mask}')\n        tokenized_output['attention_mask'].append(attention_mask)\n\n        # break\n\n    # \u904d\u5386tokenized_output\u5b57\u5178\uff0c\u5176\u4e2dk\u662f\u952e\uff0cv\u662f\u503c\n    for k, v in tokenized_output.items():\n        # \u5982\u679creturn_tensor\u4e3aTrue\uff0c\u5c06\u503c\u8f6c\u6362\u4e3atorch.LongTensor\u7c7b\u578b\n        if return_tensor:\n            tokenized_output[k] = torch.LongTensor(v)\n        # \u5426\u5219\uff0c\u5c06\u503c\u8f6c\u6362\u4e3anumpy\u6570\u7ec4\n        else:\n            tokenized_output[k] = np.array(v)\n\n    return tokenized_output\n\n\ndef get_attention_mask(alist):\n    '''\n    \u751f\u6210\u6ce8\u610f\u529b\u63a9\u7801\u3002\n    \u5bf9\u4e8e\u8f93\u5165\u7684\u5217\u8868\uff0c\u5c06\u5176\u4e2d\u7684\u6bcf\u4e2a\u5143\u7d20\u4e0e0\u8fdb\u884c\u6bd4\u8f83\uff0c\u5982\u679c\u5143\u7d20\u5927\u4e8e0\uff0c\u5219\u5728\u8f93\u51fa\u5217\u8868\u4e2d\u5bf9\u5e94\u4f4d\u7f6e\u8bbe\u7f6e\u4e3a1\uff0c\u5426\u5219\u8bbe\u7f6e\u4e3a0\u3002\n    \u8fd9\u4e2a\u51fd\u6570\u7684\u76ee\u7684\u662f\u4e3a\u4e86\u521b\u5efa\u4e00\u4e2a\u63a9\u7801\uff0c\u7528\u4e8e\u5728\u6ce8\u610f\u529b\u673a\u5236\u4e2d\u6307\u793a\u54ea\u4e9b\u4f4d\u7f6e\u662f\u6709\u6548\u7684\uff08\u5373\u5927\u4e8e0\uff09\uff0c\u54ea\u4e9b\u4f4d\u7f6e\u662f\u65e0\u6548\u7684\uff08\u5373\u7b49\u4e8e0\uff09\u3002\n    :param alist: (list): \u4e00\u4e2a\u5305\u542b\u6570\u5b57\u7684\u5217\u8868\uff0c\u7528\u4e8e\u751f\u6210\u6ce8\u610f\u529b\u63a9\u7801\u3002\n    :return: list: \u4e00\u4e2a\u4e0e\u8f93\u5165\u5217\u8868\u957f\u5ea6\u76f8\u540c\u7684\u5217\u8868\uff0c\u5176\u4e2d\u539f\u59cb\u5217\u8868\u4e2d\u5927\u4e8e0\u7684\u4f4d\u7f6e\u88ab\u8bbe\u7f6e\u4e3a1\uff0c\u7b49\u4e8e0\u7684\u4f4d\u7f6e\u88ab\u8bbe\u7f6e\u4e3a0\u3002\n    '''\n    # \u4f7f\u7528numpy\u7684where\u51fd\u6570\u6765\u521b\u5efa\u63a9\u7801\uff1a\u5143\u7d20\u5927\u4e8e0\u5219\u8f93\u51fa1\uff0c\u5426\u5219\u8f93\u51fa0\n    new_a = np.where(np.array(alist) &gt; 0, 1, 0)\n    # \u5c06\u751f\u6210\u7684\u63a9\u7801\u6570\u7ec4\u8f6c\u6362\u56de\u5217\u8868\u683c\u5f0f\u5e76\u8fd4\u56de\n    return new_a.tolist()\n\n\nif __name__ == '__main__':\n    pc = ProjectConfig()\n    train_dataset = load_dataset('text', data_files={'train': pc.train_path})\n    print(type(train_dataset))\n    # print(train_dataset)\n    # print(train_dataset['train']['text'])\n    tokenizer = AutoTokenizer.from_pretrained(pc.pre_model)\n    tokenized_output = convert_example(examples=train_dataset['train'],\n                                       tokenizer=tokenizer,\n                                       max_seq_len=20,\n                                       max_label_len=2,\n                                       p_embedding_num=6,\n                                       train_mode=True,\n                                       return_tensor=False)\n    print(tokenized_output)\n    print(type(tokenized_output['mask_positions']))\n</code></pre> <p>\u6253\u5370\u7ed3\u679c\u5c55\u793a\uff1a</p> <pre><code>{'input_ids': array([[   1,    2,    3, ..., 1912, 6225,  102],\n       [   1,    2,    3, ..., 3300, 5741,  102],\n       [   1,    2,    3, ..., 6574, 7030,    0],\n       ...,\n       [   1,    2,    3, ..., 8024, 2571,    0],\n       [   1,    2,    3, ..., 3221, 3175,  102],\n       [   1,    2,    3, ..., 5277, 3688,  102]]), \n 'attention_mask': array([[1, 1, 1, ..., 1, 1, 1],\n       [1, 1, 1, ..., 1, 1, 1],\n       [1, 1, 1, ..., 1, 1, 0],\n       ...,\n       [1, 1, 1, ..., 1, 1, 0],\n       [1, 1, 1, ..., 1, 1, 1],\n       [1, 1, 1, ..., 1, 1, 1]]), \n 'mask_positions': array([[7, 8],\n       [7, 8],\n       [7, 8],\n       ...,\n       [7, 8],\n       [7, 8],\n       [7, 8]]), \n 'mask_labels': array([[4510, 5554],\n       [3717, 3362],\n       [2398, 3352],\n       ...,\n       [3819, 3861],\n       [6983, 2421],\n       [3819, 3861]]), \n 'token_type_ids': array([[0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0],\n       ...,\n       [0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0]])}\n</code></pre>"},{"location":"03-%E6%96%B0%E9%9B%B6%E5%94%AE%E8%A1%8C%E4%B8%9A%E8%AF%84%E4%BB%B7%E5%86%B3%E7%AD%96%E7%B3%BB%E7%BB%9F/06-BERT%2BP-Tuning%E6%96%B9%E5%BC%8F%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E4%BB%8B%E7%BB%8D.html#32-data_loaderpy","title":"3.2 data_loader.py","text":"<ul> <li> <p>\u76ee\u7684\uff1a\u5b9a\u4e49\u6570\u636e\u52a0\u8f7d\u5668</p> </li> <li> <p>\u4ee3\u7801\u5982\u4e0b\uff1a</p> </li> </ul> <pre><code>from functools import partial\nfrom datasets import load_dataset\nfrom torch.utils.data import DataLoader\nfrom transformers import AutoTokenizer, default_data_collator\n\nfrom prompt_tasks.P_Tuning.data_handle.data_preprocess import convert_example\nfrom prompt_tasks.P_Tuning.ptune_config import ProjectConfig\n\n# \u5b9e\u4f8b\u5316\u9879\u76ee\u914d\u7f6e\u6587\u4ef6\npc = ProjectConfig()\n\n# \u4f7f\u7528\u9879\u76ee\u914d\u7f6e\u6587\u4ef6\u4e2d\u6307\u5b9a\u7684\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u521d\u59cb\u5316\u4e00\u4e2a\u81ea\u52a8\u5206\u8bcd\u5668\ntokenizer = AutoTokenizer.from_pretrained(pc.pre_model)\n\n\ndef get_data():\n    '''\n    \u52a0\u8f7d\u5e76\u5904\u7406\u6570\u636e\u96c6\u3002\n    \u8be5\u51fd\u6570\u4ece\u6307\u5b9a\u8def\u5f84\u52a0\u8f7d\u8bad\u7ec3\u548c\u5f00\u53d1\u6570\u636e\u96c6\uff0c\u5e76\u5c06\u5b83\u4eec\u8f6c\u6362\u4e3a\u9002\u5408\u6a21\u578b\u8bad\u7ec3\u7684\u683c\u5f0f\u3002\n    \u4f7f\u7528Hugging Face\u7684`load_dataset`\u51fd\u6570\u52a0\u8f7d\u6570\u636e\uff0c\u7136\u540e\u4f7f\u7528`partial`\u51fd\u6570\u521b\u5efa\u4e00\u4e2a\u5e26\u6709\u56fa\u5b9a\u53c2\u6570\u7684\u65b0\u51fd\u6570`new_func`\uff0c\n    \u7528\u4e8e\u8f6c\u6362\u6570\u636e\u96c6\u793a\u4f8b\u3002\u8f6c\u6362\u540e\u7684\u6570\u636e\u96c6\u4f7f\u7528`DataLoader`\u5305\u88c5\uff0c\u4ee5\u4fbf\u4e8e\u6279\u91cf\u5904\u7406\u548c\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u4f7f\u7528\u3002\n    :return: train_dataloader: \u8bad\u7ec3\u6570\u636e\u52a0\u8f7d\u5668\u3002\n             dev_dataloader: \u6d4b\u8bd5\u6570\u636e\u52a0\u8f7d\u5668\u3002\n    '''\n    # \u52a0\u8f7d\u6570\u636e\u96c6\uff0c\u5305\u62ec\u8bad\u7ec3\u548c\u6d4b\u8bd5\u96c6\n    dataset = load_dataset('text', data_files={'train': pc.train_path,\n                                               'dev': pc.dev_path})\n    # print(f'dataset--&gt;{dataset}')\n\n    # \u521b\u5efa\u4e00\u4e2a\u5e26\u6709\u56fa\u5b9a\u53c2\u6570\u7684\u51fd\u6570\uff0c\u7528\u4e8e\u8f6c\u6362\u6570\u636e\u96c6\u793a\u4f8b\n    new_func = partial(convert_example,\n                       tokenizer=tokenizer,\n                       max_seq_len=pc.max_seq_len,\n                       max_label_len=pc.max_label_len,\n                       p_embedding_num=pc.p_embedding_num)\n\n    # \u5e94\u7528\u8f6c\u6362\u51fd\u6570\u5230\u6570\u636e\u96c6\u4e0a\n    dataset = dataset.map(new_func, batched=True)\n\n    # \u5206\u79bb\u8bad\u7ec3\u548c\u6d4b\u8bd5\u6570\u636e\u96c6\n    train_dataset = dataset[\"train\"]\n    # print(f'train_dataset--&gt;{train_dataset}')\n    dev_dataset = dataset[\"dev\"]\n    # print(f'dev_dataset--&gt;{dev_dataset}')\n\n    # \u521b\u5efa\u8bad\u7ec3\u6570\u636e\u52a0\u8f7d\u5668\n    train_dataloader = DataLoader(train_dataset,\n                                  shuffle=True,\n                                  collate_fn=default_data_collator,\n                                  batch_size=pc.batch_size)\n\n    # \u521b\u5efa\u6d4b\u8bd5\u6570\u636e\u52a0\u8f7d\u5668\n    dev_dataloader = DataLoader(dev_dataset,\n                                collate_fn=default_data_collator,\n                                batch_size=pc.batch_size)\n\n    # \u8fd4\u56de\u8bad\u7ec3\u548c\u6d4b\u8bd5\u6570\u636e\u52a0\u8f7d\u5668\n    return train_dataloader, dev_dataloader\n\n\nif __name__ == '__main__':\n    # \u52a0\u8f7d\u8bad\u7ec3\u548c\u6d4b\u8bd5\u6570\u636e\n    train_dataloader, dev_dataloader = get_data()\n\n    # \u6253\u5370\u8bad\u7ec3\u548c\u6d4b\u8bd5\u6570\u636e\u52a0\u8f7d\u5668\u7684\u957f\u5ea6\n    print(f'len(train_dataloader)--&gt;{len(train_dataloader)}')\n    print(f'len(dev_dataloader)--&gt;{len(dev_dataloader)}')\n\n    # \u904d\u5386\u8bad\u7ec3\u6570\u636e\u52a0\u8f7d\u5668\uff0c\u67e5\u770b\u6570\u636e\n    for i, value in enumerate(train_dataloader):\n        print(value)\n        # \u6253\u5370\u8f93\u5165ID\u7684Tensor\u7c7b\u578b\n        print(value['input_ids'].dtype)\n        break\n</code></pre> <p>\u6253\u5370\u7ed3\u679c\u5c55\u793a\uff1a</p> <pre><code>{'input_ids': tensor([[1, 2, 3,  ..., 0, 0, 0],\n           [1, 2, 3,  ..., 0, 0, 0],\n           [1, 2, 3,  ..., 0, 0, 0],\n           ...,\n           [1, 2, 3,  ..., 0, 0, 0],\n           [1, 2, 3,  ..., 0, 0, 0],\n           [1, 2, 3,  ..., 0, 0, 0]]), \n    'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n           [1, 1, 1,  ..., 0, 0, 0],\n           [1, 1, 1,  ..., 0, 0, 0],\n           ...,\n           [1, 1, 1,  ..., 0, 0, 0],\n           [1, 1, 1,  ..., 0, 0, 0],\n           [1, 1, 1,  ..., 0, 0, 0]]), \n    'mask_positions': tensor([[7, 8],\n           [7, 8],\n           [7, 8],\n           [7, 8],\n           [7, 8],\n           [7, 8],\n           [7, 8],\n           [7, 8]]), \n    'mask_labels': tensor([[6132, 3302],\n           [3717, 3362],\n           [6132, 3302],\n           [6983, 2421],\n           [6983, 2421],\n           [6132, 3302],\n           [3717, 3362],\n           [2398, 3352]]), \n    'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n           [0, 0, 0,  ..., 0, 0, 0],\n           [0, 0, 0,  ..., 0, 0, 0],\n           ...,\n           [0, 0, 0,  ..., 0, 0, 0],\n           [0, 0, 0,  ..., 0, 0, 0],\n           [0, 0, 0,  ..., 0, 0, 0]])}\n   torch.int64\n</code></pre>"},{"location":"03-%E6%96%B0%E9%9B%B6%E5%94%AE%E8%A1%8C%E4%B8%9A%E8%AF%84%E4%BB%B7%E5%86%B3%E7%AD%96%E7%B3%BB%E7%BB%9F/06-BERT%2BP-Tuning%E6%96%B9%E5%BC%8F%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E4%BB%8B%E7%BB%8D.html#_2","title":"\u5c0f\u7ed3\u603b\u7ed3","text":"<p>\u672c\u7ae0\u8282\u4e3b\u8981\u4ecb\u7ecd\u4e86\u57fa\u4e8eBERT+P-Tuning\u65b9\u5f0f\u5b9e\u73b0\u6587\u672c\u5206\u7c7b\u4efb\u52a1\u65f6\u6570\u636e\u5904\u7406\u6b65\u9aa4\uff0c\u5e76\u4e14\u901a\u8fc7\u4ee3\u7801\u5b9e\u73b0\uff1a\u63d0\u793a\u6a21\u677f\u6570\u636e\u683c\u5f0f\u7684\u8f6c\u6362\uff0c\u6570\u636e\u52a0\u8f7d\u5668\u7684\u7f16\u7801\u7b49\u3002</p>"},{"location":"03-%E6%96%B0%E9%9B%B6%E5%94%AE%E8%A1%8C%E4%B8%9A%E8%AF%84%E4%BB%B7%E5%86%B3%E7%AD%96%E7%B3%BB%E7%BB%9F/07-BERT%2BP-Tuning%E6%96%B9%E5%BC%8F%E6%A8%A1%E5%9E%8B%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0%E4%B8%8E%E8%AE%AD%E7%BB%83.html","title":"3.7 BERT+P-Tuning\u65b9\u5f0f\u6a21\u578b\u4ee3\u7801\u5b9e\u73b0\u548c\u8bad\u7ec3","text":""},{"location":"03-%E6%96%B0%E9%9B%B6%E5%94%AE%E8%A1%8C%E4%B8%9A%E8%AF%84%E4%BB%B7%E5%86%B3%E7%AD%96%E7%B3%BB%E7%BB%9F/07-BERT%2BP-Tuning%E6%96%B9%E5%BC%8F%E6%A8%A1%E5%9E%8B%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0%E4%B8%8E%E8%AE%AD%E7%BB%83.html#bertp-tuning","title":"\u57fa\u4e8eBERT+P-Tuning\u65b9\u5f0f\u6587\u672c\u5206\u7c7b\u6a21\u578b\u642d\u5efa","text":""},{"location":"03-%E6%96%B0%E9%9B%B6%E5%94%AE%E8%A1%8C%E4%B8%9A%E8%AF%84%E4%BB%B7%E5%86%B3%E7%AD%96%E7%B3%BB%E7%BB%9F/07-BERT%2BP-Tuning%E6%96%B9%E5%BC%8F%E6%A8%A1%E5%9E%8B%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0%E4%B8%8E%E8%AE%AD%E7%BB%83.html#_1","title":"\u5b66\u4e60\u76ee\u6807","text":"<ul> <li>\u638c\u63e1\u57fa\u4e8eBERT+P-Tuning\u65b9\u5f0f\u6a21\u578b\u642d\u5efa\u4ee3\u7801\u7684\u5b9e\u73b0.</li> <li>\u638c\u63e1\u6a21\u578b\u7684\u8bad\u7ec3,\u9a8c\u8bc1\u53ca\u76f8\u5173\u5de5\u5177\u4ee3\u7801\u7684\u5b9e\u73b0.</li> <li>\u638c\u63e1\u4f7f\u7528\u6a21\u578b\u9884\u6d4b\u4ee3\u7801\u7684\u5b9e\u73b0.</li> </ul>"},{"location":"03-%E6%96%B0%E9%9B%B6%E5%94%AE%E8%A1%8C%E4%B8%9A%E8%AF%84%E4%BB%B7%E5%86%B3%E7%AD%96%E7%B3%BB%E7%BB%9F/07-BERT%2BP-Tuning%E6%96%B9%E5%BC%8F%E6%A8%A1%E5%9E%8B%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0%E4%B8%8E%E8%AE%AD%E7%BB%83.html#_2","title":"\u6a21\u578b\u642d\u5efa","text":"<p>\u672c\u9879\u76ee\u4e2d\u5b8c\u6210BERT+P-Tuning\u6a21\u578b\u642d\u5efa\u3001\u8bad\u7ec3\u53ca\u5e94\u7528\u7684\u6b65\u9aa4\u5982\u4e0b\uff08\u6ce8\u610f\uff1a\u56e0\u4e3a\u672c\u9879\u76ee\u4e2d\u4f7f\u7528\u7684\u662fBERT\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u6240\u4ee5\u76f4\u63a5\u52a0\u8f7d\u5373\u53ef\uff0c\u65e0\u9700\u91cd\u590d\u642d\u5efa\u6a21\u578b\u67b6\u6784\uff09: - 1.\u5b9e\u73b0\u6a21\u578b\u5de5\u5177\u7c7b\u51fd\u6570 - 2.\u5b9e\u73b0\u6a21\u578b\u8bad\u7ec3\u51fd\u6570,\u9a8c\u8bc1\u51fd\u6570 - 3.\u5b9e\u73b0\u6a21\u578b\u9884\u6d4b\u51fd\u6570</p>"},{"location":"03-%E6%96%B0%E9%9B%B6%E5%94%AE%E8%A1%8C%E4%B8%9A%E8%AF%84%E4%BB%B7%E5%86%B3%E7%AD%96%E7%B3%BB%E7%BB%9F/07-BERT%2BP-Tuning%E6%96%B9%E5%BC%8F%E6%A8%A1%E5%9E%8B%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0%E4%B8%8E%E8%AE%AD%E7%BB%83.html#1","title":"1 \u5b9e\u73b0\u6a21\u578b\u5de5\u5177\u7c7b\u51fd\u6570","text":"<ul> <li>\u76ee\u7684\uff1a\u6a21\u578b\u5728\u8bad\u7ec3\u3001\u9a8c\u8bc1\u3001\u9884\u6d4b\u65f6\u9700\u8981\u7684\u51fd\u6570</li> <li>\u4ee3\u7801\u8def\u5f84\uff1allm_tuning/prompt_tasks/P_Tuning/utils</li> <li>utils\u6587\u4ef6\u5939\u5171\u5305\u542b3\u4e2apy\u811a\u672c\uff1averbalizer.py\u3001metirc_utils.py\u4ee5\u53cacommon_utils.py</li> </ul>"},{"location":"03-%E6%96%B0%E9%9B%B6%E5%94%AE%E8%A1%8C%E4%B8%9A%E8%AF%84%E4%BB%B7%E5%86%B3%E7%AD%96%E7%B3%BB%E7%BB%9F/07-BERT%2BP-Tuning%E6%96%B9%E5%BC%8F%E6%A8%A1%E5%9E%8B%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0%E4%B8%8E%E8%AE%AD%E7%BB%83.html#11-verbalizerpy","title":"1.1 verbalizer.py","text":"<ul> <li>\u76ee\u7684\uff1a\u5b9a\u4e49\u4e00\u4e2aVerbalizer\u7c7b\uff0c\u7528\u4e8e\u5c06\u4e00\u4e2aLabel\u5bf9\u5e94\u5230\u5176\u5b50Label\u7684\u6620\u5c04\u3002</li> <li>\u5177\u4f53\u4ee3\u7801\u5982\u4e0b\uff1a</li> </ul> <pre><code># Union \u662f typing \u6a21\u5757\u4e2d\u5b9a\u4e49\u7684\u4e00\u4e2a\u7c7b,\u7528\u4e8e\u8868\u793a\u591a\u4e2a\u7c7b\u578b\u4e2d\u7684\u4efb\u610f\u4e00\u79cd\u7c7b\u578b\nfrom typing import Union, List\nfrom transformers import AutoTokenizer\n\nfrom prompt_tasks.P_Tuning.ptune_config import ProjectConfig\n\npc = ProjectConfig()\n\n\n# Verbalizer\u7c7b\uff0c\u7528\u4e8e\u5c06\u4e00\u4e2aLabel\u5bf9\u5e94\u5230\u5176\u5b50Label\u7684\u6620\u5c04\u3002\nclass Verbalizer(object):\n    def __init__(self,\n                 verbalizer_file: str,\n                 tokenizer, max_label_len: int\n                 ):\n        '''\n        :param verbalizer_file: verbalizer\u6587\u4ef6\u5b58\u653e\u5730\u5740\u3002\n        :param tokenizer: \u7528\u4e8e\u6587\u672c\u548cid\u4e4b\u95f4\u7684\u8f6c\u6362\u3002\n        :param max_label_len: \u6807\u7b7e\u957f\u5ea6\uff0c\u82e5\u5927\u4e8e\u5219\u622a\u65ad\uff0c\u82e5\u5c0f\u4e8e\u5219\u8865\u9f50\n        '''\n        self.tokenizer = tokenizer\n        self.label_dict = self.load_label_dict(verbalizer_file)\n        self.max_label_len = max_label_len\n\n    def load_label_dict(self, verbalizer_file: str):\n        '''\n        \u8bfb\u53d6\u672c\u5730\u6587\u4ef6\uff0c\u6784\u5efaverbalizer\u5b57\u5178\u3002\n        :param verbalizer_file: verbalizer\u6587\u4ef6\u5b58\u653e\u5730\u5740\u3002\n        :return:\n        dict -&gt; {\n            '\u4f53\u80b2': ['\u7bee\u7403', '\u8db3\u7403','\u7f51\u7403', '\u6392\u7403',  ...],\n            '\u9152\u5e97': ['\u5bbe\u9986', '\u65c5\u9986', '\u65c5\u5e97', '\u9152\u5e97', ...],\n            ...\n            }\n        '''\n        # \u521d\u59cb\u5316\u4e00\u4e2a\u7a7a\u5b57\u5178\uff0c\u7528\u4e8e\u5b58\u50a8\u6807\u7b7e\u548c\u5b50\u6807\u7b7e\u7684\u5173\u7cfb\n        label_dict = {}\n\n        # \u6253\u5f00verbalizer\u6587\u4ef6\uff0c\u4ee5\u53ea\u8bfb\u6a21\u5f0f\uff0c\u4f7f\u7528utf8\u7f16\u7801\n        with open(verbalizer_file, 'r', encoding='utf-8') as f:\n            # \u8bfb\u53d6\u6587\u4ef6\u7684\u6bcf\u4e00\u884c\n            for line in f:\n                # \u79fb\u9664\u884c\u5c3e\u7684\u6362\u884c\u7b26\uff0c\u5e76\u6309\u5236\u8868\u7b26('\\t')\u5206\u5272\u6807\u7b7e\u548c\u5b50\u6807\u7b7e\n                label, sub_labels = line.strip().split('\\t')\n                # \u5c06\u5b50\u6807\u7b7e\u6309\u9017\u53f7(,)\u5206\u5272\u6210\u5217\u8868\uff0c\u4f7f\u7528set\u53bb\u91cd\u540e\u518d\u8f6c\u56de\u5217\u8868\uff0c\u5b58\u50a8\u5230label_dict\u4e2d\n                label_dict[label] = list(set(sub_labels.split(',')))\n        # \u8fd4\u56de\u5904\u7406\u540e\u7684\u6807\u7b7e\u548c\u5b50\u6807\u7b7e\u7684\u5b57\u5178\n        return label_dict\n\n    def find_sub_labels(self, label: Union[list, str]):\n        '''\n        \u901a\u8fc7\u4e3b\u6807\u7b7e\u627e\u5230\u6240\u6709\u7684\u5b50\u6807\u7b7e\u3002\n        :param label: \u6807\u7b7e, \u6587\u672c\u578b \u6216 id_list, e.g. -&gt; '\u4f53\u80b2' or [860, 5509]\n        :return:\n        dict -&gt; {\n            'sub_labels': ['\u8db3\u7403', '\u7f51\u7403'],\n            'token_ids': [[6639, 4413], [5381, 4413]]\n        }\n        '''\n        # \u5982\u679c\u4f20\u5165\u7684label\u4e3aid\u5217\u8868\uff0c\u5219\u901a\u8fc7tokenizer\u8f6c\u6362\u56de\u5b57\u7b26\u4e32\n        if type(label) == list:\n            # \u79fb\u9664label\u4e2d\u7684pad_token_id\uff0c\u76f4\u5230label\u4e2d\u4e0d\u518d\u5305\u542b\u5b83\n            while self.tokenizer.pad_token_id in label:\n                label.remove(self.tokenizer.pad_token_id)\n            # \u5c06\u5904\u7406\u540e\u7684id\u5217\u8868\u8f6c\u6362\u4e3atokens\uff0c\u5e76\u62fc\u63a5\u6210\u5b57\u7b26\u4e32\n            label = ''.join(self.tokenizer.convert_ids_to_tokens(label))\n        # print(f'label--&gt;{label}')\n        # \u68c0\u67e5\u8f6c\u6362\u540e\u7684label\u662f\u5426\u5728\u6807\u7b7e\u5b57\u5178\u4e2d\uff0c\u5982\u679c\u4e0d\u5728\u5219\u629b\u51fa\u5f02\u5e38\n        if label not in self.label_dict:\n            raise ValueError(f'Lable Error: \"{label}\" \u4e0d\u5728 label_dict {list(self.label_dict)}.')\n\n        # \u4ece\u6807\u7b7e\u5b57\u5178\u4e2d\u83b7\u53d6\u4e0elabel\u5bf9\u5e94\u7684\u5b50\u6807\u7b7e\n        sub_labels = self.label_dict[label]\n        # print(f'sub_labels--&gt;{sub_labels}')\n        # \u5c06\u5b50\u6807\u7b7e\u4f5c\u4e3a\u7ed3\u679c\u7684\u4e00\u4e2a\u90e8\u5206\u5b58\u50a8\u5728\u5b57\u5178\u4e2d\n        ret = {'sub_labels': sub_labels}\n\n        # \u5bf9\u6bcf\u4e2a\u5b50\u6807\u7b7e\u8fdb\u884ctoken\u5316\uff0c\u4e0d\u542b\u7279\u6b8a\u7b26\u53f7\n        token_ids = [token_id for token_id in self.tokenizer(sub_labels, add_special_tokens=False)['input_ids']]\n        # print(f'token_ids--&gt;{token_ids}')\n        # \u904d\u5386\u6240\u6709\u7684token_ids\uff0c\u8fdb\u884c\u622a\u65ad\u4e0e\u8865\u9f50\u64cd\u4f5c\n        for i in range(len(token_ids)):\n            # \u5bf9\u6807\u7b7e\u8fdb\u884c\u622a\u65ad\n            token_ids[i] = token_ids[i][:self.max_label_len]\n            # \u5982\u679c\u957f\u5ea6\u4e0d\u8db3max_label_len\uff0c\u5219\u4f7f\u7528pad_token_id\u8fdb\u884c\u8865\u9f50\n            if len(token_ids[i]) &lt; self.max_label_len:\n                token_ids[i] = token_ids[i] + [self.tokenizer.pad_token_id] * (self.max_label_len - len(token_ids[i]))\n        # \u5c06\u5904\u7406\u540e\u7684token_ids\u5b58\u5165ret\u5b57\u5178\u4e2d\n        ret['token_ids'] = token_ids\n        return ret\n\n    def batch_find_sub_labels(self, label: List[Union[list, str]]):\n        '''\n        \u6279\u91cf\u627e\u5230\u5b50\u6807\u7b7e\u3002\n        :param label: \u6807\u7b7e\u5217\u8868, [[4510, 5554], [860, 5509]] or ['\u4f53\u80b2', '\u7535\u8111']\n        :return:\n        list -&gt; [\n                {\n                    'sub_labels': ['\u7b14\u8bb0\u672c', '\u7535\u8111'],\n                    'token_ids': [[5011, 6381, 3315], [4510, 5554]]\n                },\n                ...\n            ]\n        '''\n        return [self.find_sub_labels(l) for l in label]\n\n    def get_common_sub_str(self,\n                           str1: str,\n                           str2: str\n                           ):\n        '''\n        \u5bfb\u627e\u6700\u5927\u516c\u5171\u5b50\u4e32(\u8fde\u7eed\u5b50\u5e8f\u5217)\u3002\n        :param str1: abcd\n        :param str2: abadbcdba\n        :return:\n        '''\n        # \u521d\u59cb\u5316\u4e24\u4e2a\u5b57\u7b26\u4e32\u7684\u957f\u5ea6\n        lstr1, lstr2 = len(str1), len(str2)\n        # \u751f\u62100\u77e9\u9635\uff0c\u4e3a\u65b9\u4fbf\u540e\u7eed\u8ba1\u7b97\uff0c\u6bd4\u5b57\u7b26\u4e32\u957f\u5ea6\u591a\u4e86\u4e00\u5217\uff0c\u751f\u6210\u4e00\u4e2a lstr1+1 * lstr2+1 \u7684\u4e8c\u7ef4\u77e9\u9635\n        record = [[0 for i in range(lstr2 + 1)] for j in range(lstr1 + 1)]\n        # \u521d\u59cb\u5316\u6700\u957f\u5339\u914d\u5bf9\u5e94\u5728str1\u4e2d\u7684\u6700\u540e\u4e00\u4f4d\n        p = 0\n        # \u521d\u59cb\u5316\u6700\u957f\u5339\u914d\u957f\u5ea6\n        maxNum = 0\n        # \u904d\u5386\u4e24\u4e2a\u5b57\u7b26\u4e32\uff0c\u5bfb\u627e\u6700\u957f\u516c\u5171\u5b50\u4e32\n        for i in range(1, lstr1 + 1):\n            for j in range(1, lstr2 + 1):\n                # \u5f53\u53d1\u73b0\u76f8\u540c\u5b57\u7b26\u65f6\n                if str1[i - 1] == str2[j - 1]:\n                    # \u5728record\u77e9\u9635\u4e2d\u8bb0\u5f55\u5339\u914d\u957f\u5ea6\n                    record[i][j] = record[i - 1][j - 1] + 1\n                    # \u66f4\u65b0\u6700\u957f\u5339\u914d\u957f\u5ea6\u548c\u5bf9\u5e94\u5728str1\u4e2d\u7684\u6700\u540e\u4e00\u4f4d\n                    if record[i][j] &gt; maxNum:\n                        maxNum = record[i][j]\n                        p = i\n\n        # \u8fd4\u56de\u6700\u957f\u516c\u5171\u5b50\u4e32\u548c\u5176\u957f\u5ea6\n        return str1[p - maxNum:p], maxNum\n\n    def hard_mapping(self, sub_label: str):\n        '''\n        \u5f3a\u5339\u914d\u51fd\u6570\uff0c\u5f53\u6a21\u578b\u751f\u6210\u7684\u5b50label\u4e0d\u5b58\u5728\u65f6\uff0c\u901a\u8fc7\u6700\u5927\u516c\u5171\u5b50\u4e32\u627e\u5230\u91cd\u5408\u5ea6\u6700\u9ad8\u7684\u4e3blabel\u3002\n        :param sub_label: \u5b50label\n        :return: \u4e3blabel\n        '''\n        # \u521d\u59cb\u5316\u53d8\u91cflabel\u548cmax_overlap_str\uff0c\u7528\u4e8e\u8bb0\u5f55\u6700\u5927\u91cd\u53e0\u5ea6\u7684\u6807\u7b7e\u548c\u5bf9\u5e94\u7684\u91cd\u53e0\u5ea6\u503c\n        label, max_overlap_str = '', 0\n\n        # \u904d\u5386\u6807\u7b7e\u5b57\u5178\uff0c\u5176\u4e2dmain_label\u662f\u4e3b\u6807\u7b7e\uff0csub_labels\u662f\u4e0e\u4e3b\u6807\u7b7e\u76f8\u5173\u7684\u5b50\u6807\u7b7e\u5217\u8868\n        for main_label, sub_labels in self.label_dict.items():\n            overlap_num = 0\n            # \u5bf9\u4e8e\u6bcf\u4e2a\u5b50\u6807\u7b7e\uff0c\u8ba1\u7b97\u5b83\u4e0e\u5f53\u524d\u63a8\u7406\u6807\u7b7e\u4e4b\u95f4\u7684\u6700\u957f\u516c\u5171\u5b50\u4e32\u957f\u5ea6\u603b\u548c\n            for s_label in sub_labels:\n                # \u7d2f\u52a0\u6bcf\u4e2a\u5b50\u6807\u7b7e\u4e0e\u5f53\u524d\u63a8\u7406\u6807\u7b7e\u4e4b\u95f4\u7684\u6700\u957f\u516c\u5171\u5b50\u4e32\u957f\u5ea6\n                overlap_num += self.get_common_sub_str(sub_label, s_label)[1]\n\n            # \u5982\u679c\u5f53\u524d\u7684\u91cd\u53e0\u5ea6\u5927\u4e8e\u6216\u7b49\u4e8e\u4e4b\u524d\u7684\u6700\u5927\u91cd\u53e0\u5ea6\uff0c\u5219\u66f4\u65b0\u6700\u5927\u91cd\u53e0\u5ea6\u548c\u5bf9\u5e94\u7684\u6807\u7b7e\n            if overlap_num &gt;= max_overlap_str:\n                max_overlap_str = overlap_num\n                label = main_label\n\n        return label\n\n    def find_main_label(self,\n                        sub_label: Union[list, str],\n                        hard_mapping=True\n                        ):\n        '''\n        \u901a\u8fc7\u5b50\u6807\u7b7e\u627e\u5230\u7236\u6807\u7b7e\u3002\n        :param sub_label: \u5b50\u6807\u7b7e, \u6587\u672c\u578b \u6216 id_list, e.g. -&gt; '\u82f9\u679c' or [5741, 3362]\n        :param hard_mapping: \u5f53\u751f\u6210\u7684\u8bcd\u8bed\u4e0d\u5b58\u5728\u65f6\uff0c\u662f\u5426\u4e00\u5b9a\u8981\u5339\u914d\u5230\u4e00\u4e2a\u6700\u76f8\u4f3c\u7684label\u3002\n        :return:\n        dict -&gt; {\n            'label': '\u6c34\u679c',\n            'token_ids': [3717, 3362]\n        }\n        '''\n        # \u5982\u679c\u4f20\u5165\u7684sub_label\u4e3aid\u5217\u8868\uff0c\u5219\u901a\u8fc7tokenizer\u8f6c\u6362\u56de\u5b57\u7b26\u4e32\n        if type(sub_label) == list:\n            pad_token_id = self.tokenizer.pad_token_id\n            # \u79fb\u9664\u5217\u8868\u4e2d\u7684[PAD]token\uff0c\u907f\u514d\u5f71\u54cd\u540e\u7eed\u5904\u7406\n            while pad_token_id in sub_label:\n                sub_label.remove(pad_token_id)\n            # \u5c06id\u5217\u8868\u8f6c\u6362\u4e3a\u5bf9\u5e94\u7684\u5b57\u7b26\u4e32\n            sub_label = ''.join(self.tokenizer.convert_ids_to_tokens(sub_label))\n        # print(f'sub_label--&gt;{sub_label}')\n        # \u521d\u59cb\u5316\u4e3b\u6807\u7b7e\u4e3a'\u65e0'\uff0c\u4f5c\u4e3a\u672a\u627e\u5230\u7279\u5b9a\u5b50\u6807\u7b7e\u65f6\u7684\u9ed8\u8ba4\u503c\n        main_label = '\u65e0'\n\n        # \u904d\u5386\u6807\u7b7e\u5b57\u5178\uff0c\u5bfb\u627e\u4e0e\u5b50\u6807\u7b7e\u5339\u914d\u7684\u4e3b\u6807\u7b7e\n        for label, sub_labels in self.label_dict.items():\n            # \u68c0\u67e5\u5f53\u524d\u5b50\u6807\u7b7e\u662f\u5426\u5728\u5b57\u5178\u4e2d\u5bf9\u5e94\u7684\u5b50\u6807\u7b7e\u5217\u8868\u4e2d\n            if sub_label in sub_labels:\n                # \u5f53\u627e\u5230\u5339\u914d\u65f6\uff0c\u66f4\u65b0\u4e3b\u6807\u7b7e\u5e76\u7ec8\u6b62\u5faa\u73af\n                main_label = label\n                break\n        # print(f'main_label--&gt;{main_label}')\n        # \u5982\u679c\u4e3b\u6807\u7b7e\u4e3a'\u65e0'\u4e14\u542f\u7528\u4e86\u5f3a\u5339\u914d\u529f\u80fd\uff0c\u5219\u4f7f\u7528\u5f3a\u5339\u914d\u65b9\u6cd5\u66f4\u65b0\u4e3b\u6807\u7b7e\n        if main_label == '\u65e0' and hard_mapping:\n            main_label = self.hard_mapping(sub_label)\n        # print('\u5f3a\u5339\u914d', main_label)\n        ret = {\n            'label': main_label,\n            'token_ids': self.tokenizer(main_label, add_special_tokens=False)['input_ids']\n        }\n        return ret\n\n    def batch_find_main_label(self,\n                              sub_label: List[Union[list, str]],\n                              hard_mapping=True\n                              ):\n        '''\n        \u6279\u91cf\u901a\u8fc7\u5b50\u6807\u7b7e\u627e\u7236\u6807\u7b7e\u3002\n        :param sub_label: \u5b50\u6807\u7b7e\u5217\u8868, ['\u82f9\u679c', ...] or [[5741, 3362], ...]\n        :param hard_mapping:\n        :return:\n        list: [\n                {\n                'label': '\u6c34\u679c',\n                'token_ids': [3717, 3362]\n                },\n                ...\n        ]\n        '''\n        return [self.find_main_label(l, hard_mapping) for l in sub_label]\n\n\nif __name__ == '__main__':\n    tokenizer = AutoTokenizer.from_pretrained(pc.pre_model)\n    verbalizer = Verbalizer(\n        verbalizer_file=pc.verbalizer,\n        tokenizer=tokenizer,\n        max_label_len=2)\n    print(f'label_dict--&gt;{verbalizer.label_dict}')\n\n    # \u67e5\u627e\u5355\u4e2a\u5b50\u6807\u7b7e\n    label = '\u7535\u8111'\n    ret = verbalizer.find_sub_labels(label)\n    print(f'ret--&gt;{ret}')\n    print('*' * 80)\n\n    # \u67e5\u627e\u591a\u4e2a\u5b50\u6807\u7b7e\n    labels = ['\u7535\u8111', '\u8863\u670d']\n    # labels = [[4510, 5554], [6132, 3302]]\n    result = verbalizer.batch_find_sub_labels(labels)\n    print(f'result--&gt;{result}')\n    print('*' * 80)\n\n    # \u67e5\u627e\u5355\u4e2a\u5b50\u6807\u7b7e\u5bf9\u5e94\u7684\u7236\u6807\u7b7e\n    # sub_label = [4510, 5554]\n    sub_label = '\u8863\u7535'\n    ret = verbalizer.find_main_label(sub_label)\n    print(f'ret--&gt;{ret}')\n    print('*' * 80)\n\n    # \u67e5\u627e\u591a\u4e2a\u5b50\u6807\u7b7e\u5bf9\u5e94\u7684\u7236\u6807\u7b7e\n    # sub_label = ['\u8863\u670d', '\u725b\u5976']\n    sub_label = [[6132, 3302], [5885, 4281]]\n    ret = verbalizer.batch_find_main_label(sub_label, hard_mapping=True)\n    print(f'ret--&gt;{ret}')\n</code></pre> <p>print\u7ed3\u679c\u663e\u793a:</p> <pre><code>label_dict--&gt;{'\u7535\u8111': ['\u7535\u8111'], '\u6c34\u679c': ['\u6c34\u679c'], '\u5e73\u677f': ['\u5e73\u677f'], '\u8863\u670d': ['\u8863\u670d'], '\u9152\u5e97': ['\u9152\u5e97'], '\u6d17\u6d74': ['\u6d17\u6d74'], '\u4e66\u7c4d': ['\u4e66\u7c4d'], '\u8499\u725b': ['\u8499\u725b'], '\u624b\u673a': ['\u624b\u673a'], '\u7535\u5668': ['\u7535\u5668']}\nret--&gt;{'sub_labels': ['\u7535\u8111'], 'token_ids': [[4510, 5554]]}\n********************************************************************************\nresult--&gt;[{'sub_labels': ['\u7535\u8111'], 'token_ids': [[4510, 5554]]}, {'sub_labels': ['\u8863\u670d'], 'token_ids': [[6132, 3302]]}]\n********************************************************************************\nret--&gt;{'label': '\u7535\u5668', 'token_ids': [4510, 1690]}\n********************************************************************************\nret--&gt;[{'label': '\u8863\u670d', 'token_ids': [6132, 3302]}, {'label': '\u8499\u725b', 'token_ids': [5885, 4281]}]\n</code></pre>"},{"location":"03-%E6%96%B0%E9%9B%B6%E5%94%AE%E8%A1%8C%E4%B8%9A%E8%AF%84%E4%BB%B7%E5%86%B3%E7%AD%96%E7%B3%BB%E7%BB%9F/07-BERT%2BP-Tuning%E6%96%B9%E5%BC%8F%E6%A8%A1%E5%9E%8B%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0%E4%B8%8E%E8%AE%AD%E7%BB%83.html#12-common_utilspy","title":"1.2 common_utils.py","text":"<ul> <li>\u76ee\u7684\uff1a\u5b9a\u4e49\u635f\u5931\u51fd\u6570\u3001\u5c06mask_position\u4f4d\u7f6e\u7684token logits\u8f6c\u6362\u4e3atoken\u7684id\u3002</li> <li>\u811a\u672c\u91cc\u9762\u5305\u542b\u4e24\u4e2a\u51fd\u6570\uff1amlm_loss()\u4ee5\u53caconvert_logits_to_ids()</li> <li>\u4ee3\u7801\u5982\u4e0b\uff1a</li> </ul> <pre><code>import torch\n\ndef mlm_loss(logits,\n             mask_positions,\n             sub_mask_labels,\n             cross_entropy_criterion,\n             device):\n    '''\n    \u8ba1\u7b97\u6307\u5b9a\u4f4d\u7f6e\u7684mask token\u7684output\u4e0elabel\u4e4b\u95f4\u7684cross entropy loss\u3002\n    :param logits: (torch.tensor): \u6a21\u578b\u539f\u59cb\u8f93\u51fa -&gt; (batch_size, seq_len, vocab_size)\n    :param mask_positions: (torch.tensor): mask token\u7684\u4f4d\u7f6e  -&gt; (batch_size, mask_label_num)\n    :param sub_mask_labels: (list): mask token\u7684sub label, \u7531\u4e8e\u6bcf\u4e2alabel\u7684sub_label\u6570\u76ee\u4e0d\u540c\uff0c\u6240\u4ee5\u8fd9\u91cc\u662f\u4e2a\u53d8\u957f\u7684list,\n                                    e.g. -&gt; [\n                                        [[2398, 3352]],\n                                        [[2398, 3352], [3819, 3861]]\n                                    ]\n    :param cross_entropy_criterion: (CrossEntropyLoss): CE Loss\u8ba1\u7b97\u5668\n    :param device: (str): cpu\u8fd8\u662fgpu\n    :return: CE Loss\n    '''\n    '''\n    \u83b7\u53d6logits\u7684\u5c3a\u5bf8\u4fe1\u606f\uff0c\u4e3a\u540e\u7eed\u8ba1\u7b97\u505a\u51c6\u5907\n    logits.size()\u8fd4\u56de\u4e00\u4e2a\u5305\u542b\u4e09\u4e2a\u7ef4\u5ea6\u7684\u5143\u7ec4\n    \u7b2c\u4e00\u4e2a\u7ef4\u5ea6(batch_size)\u4ee3\u8868\u6279\u6b21\u5927\u5c0f\uff0c\u5373\u4e00\u6b21\u5904\u7406\u7684\u6570\u636e\u6279\u6b21\u5305\u542b\u7684\u6837\u672c\u6570\u91cf\n    \u7b2c\u4e8c\u4e2a\u7ef4\u5ea6(seq_len)\u4ee3\u8868\u5e8f\u5217\u957f\u5ea6\uff0c\u5373\u6bcf\u4e2a\u6837\u672c\u4e2d\u5305\u542b\u7684\u5e8f\u5217\u5143\u7d20\u6570\u91cf\n    \u7b2c\u4e09\u4e2a\u7ef4\u5ea6(vocab_size)\u4ee3\u8868\u8bcd\u6c47\u8868\u5927\u5c0f\uff0c\u5373\u6bcf\u4e2a\u5e8f\u5217\u5143\u7d20\u53ef\u80fd\u7684\u7c7b\u522b\u6570\u91cf\n    '''\n    batch_size, seq_len, vocab_size = logits.size()\n    # print(f'\u6a21\u578b\u9884\u6d4b\u7ed3\u679clogits--&gt;{logits.size()}')\n    # print(f'mask_positions--&gt;{mask_positions.shape}')\n    # print(f'sub_mask_labels--&gt;{sub_mask_labels}')\n\n    # \u521d\u59cb\u5316loss\u53d8\u91cf\u4e3aNone\uff0c\u7528\u4e8e\u540e\u7eed\u53ef\u80fd\u7684\u635f\u5931\u8ba1\u7b97\n    loss = None\n    # \u904d\u5386 logits\u3001sub_mask_labels \u548c mask_positions \u7684\u5143\u7d20\n    for single_value in zip(logits, sub_mask_labels, mask_positions):\n        # \u83b7\u53d6\u5f53\u524dtoken\u7684 logits\n        single_logits = single_value[0]\n        # print(f'single_logits--&gt;{single_logits.shape}')  # \u5f62\u72b6[512, 21128]\n        # \u83b7\u53d6\u5f53\u524dtoken\u7684 sub_mask_labels\n        single_sub_mask_labels = single_value[1]\n        # print(f'single_sub_mask_labels--&gt;{single_sub_mask_labels}')\n        # \u83b7\u53d6\u5f53\u524dtoken\u7684 mask_positions\n        single_mask_positions = single_value[2]\n        # print(f'single_mask_positions--&gt;{single_mask_positions}')  # \u5f62\u72b6size[2]--&gt;\u5177\u4f53\u503c([5, 6])\n\n        # \u4ece\u5355\u4e2a\u5e8f\u5217\u7684logits\u4e2d\uff0c\u63d0\u53d6\u51fa\u88ab\u63a9\u7801\u4f4d\u7f6e\u7684logits\n        single_mask_logits = single_logits[single_mask_positions]  # (mask_label_num, vocab_size)\n        # \u6253\u5370\u88ab\u63a9\u7801\u4f4d\u7f6elogits\u7684\u5f62\u72b6\uff0c\u4ee5\u9a8c\u8bc1\u5176\u662f\u5426\u7b26\u5408\u9884\u671f\n        # print(f'single_mask_logits--&gt;{single_mask_logits.shape}')\n\n        # \u6a21\u578b\u8bad\u7ec3\u65f6\u4e3b\u6807\u7b7e\u5bf9\u5e94\u7684\u6240\u6709\u5b50\u6807\u7b7e\u90fd\u6709\u76f8\u4f3c\u7684\u7279\u5f81\u503c, \u5728\u8ba1\u7b97CE Loss\u65f6\uff0c\u9700\u8981\u5c06\u6bcf\u4e2a\u5b50\u6807\u7b7e\u7684\u5bf9\u5e94\u7684\u635f\u5931\u6c42\u5e73\u5747\uff0c\u56e0\u6b64\u9700\u8981\u5c06\u9884\u6d4b\u7684\u6982\u7387\u503c\u8fdb\u884c\u6269\u5c55\n        # \u5bf9\u5355\u4e2a single_mask_logits \u8fdb\u884c\u6269\u5c55\uff0c\u4f7f\u5176\u5728\u7b2c\u4e00\u4e2a\u7ef4\u5ea6\u4e0a\u91cd\u590d\uff0c\u4ee5\u5339\u914d single_sub_mask_labels \u7684\u6570\u91cf\n        # \u4f7f\u7528repeat\u8bbe\u7f6e\u91cd\u590d\u7684\u500d\u6570 (sub_label_num, mask_label_num, vocab_size)\n        single_mask_logits = single_mask_logits.repeat(len(single_sub_mask_labels), 1, 1)\n        # \u6253\u5370\u91cd\u590d\u540e\u7684single_mask_logits\u7684\u5f62\u72b6\uff0c\u4ee5\u4fbf\u8c03\u8bd5\u548c\u9a8c\u8bc1\u91cd\u590d\u64cd\u4f5c\u7684\u6548\u679c\n        # print(f'\u91cd\u590d\u540e\u7684single_mask_logits--&gt;{single_mask_logits.shape}')\n\n        # \u5c06\u4e09\u7ef4\u5f20\u91cf\u8c03\u6574\u4e3a\u4e8c\u7ef4\uff0c\u4ee5\u4fbf\u8ba1\u7b97\u635f\u5931\n        single_mask_logits = single_mask_logits.reshape(-1, vocab_size)  # (sub_label_num * mask_label_num, vocab_size)\n        # print(f'\u8c03\u6574\u6210\u4e8c\u7ef4\u540e\u7684single_mask_logits--&gt;{single_mask_logits.shape}')\n\n        # \u5c06\u5b50\u6807\u7b7e\u8f6c\u6362\u4e3a\u5f20\u91cf\uff0c\u5e76\u8c03\u6574\u5f62\u72b6\u4ee5\u5339\u914d\u6a21\u578b\u9884\u6d4b\u7684\u7ed3\u679c\n        single_sub_mask_labels = torch.LongTensor(single_sub_mask_labels).to(device)  # (sub_label_num, mask_label_num)\n        # \u8ba1\u7b97\u635f\u5931\u503c\u65f6\u771f\u5b9e\u5b50\u6807\u7b7e\u7ef4\u5ea6\u4e3a1\u7ef4\uff0c\u56e0\u6b64\u9700\u8981\u5c06\u5176\u5c55\u5e73\u4ee5\u5339\u914d\u6a21\u578b\u9884\u6d4b\u7684\u7ed3\u679c\n        single_sub_mask_labels = single_sub_mask_labels.reshape(-1, 1).squeeze()  # (sub_label_num * mask_label_num)\n        # print(f'\u771f\u5b9e\u5b50\u6807\u7b7emask\u503c\uff1asingle_sub_mask_labels--&gt;{single_sub_mask_labels.shape}')\n        # print(f'\u771f\u5b9e\u5b50\u6807\u7b7emask\u503c\uff1asingle_sub_mask_labels--&gt;{single_sub_mask_labels}')\n\n        # \u8ba1\u7b97\u5f53\u524d\u6279\u6b21\u6240\u6709\u5b50\u6807\u7b7e\u7684\u635f\u5931\n        cur_loss = cross_entropy_criterion(single_mask_logits, single_sub_mask_labels)\n        # \u8ba1\u7b97\u5f53\u524d\u6279\u6b21\u6240\u6709\u5b50\u6807\u7b7e\u7684\u5e73\u5747\u635f\u5931\n        cur_loss = cur_loss / len(single_sub_mask_labels)\n\n        # \u5982\u679c\u5f53\u524d\u635f\u5931loss\u672a\u88ab\u521d\u59cb\u5316\uff08\u5373\u4e3aNone\uff09\uff0c\u5219\u5c06\u5176\u8bbe\u7f6e\u4e3a\u5f53\u524d\u6279\u6b21\u7684\u635f\u5931cur_loss\n        if not loss:\n            loss = cur_loss\n        # \u5982\u679c\u5f53\u524d\u635f\u5931loss\u5df2\u7ecf\u5b58\u5728\uff0c\u5219\u5c06\u5f53\u524d\u6279\u6b21\u7684\u635f\u5931cur_loss\u7d2f\u52a0\u5230loss\u4e2d\n        else:\n            loss += cur_loss\n\n    # \u8ba1\u7b97\u5e73\u5747\u635f\u5931\uff1a\u5c06\u7d2f\u8ba1\u7684\u635f\u5931loss\u9664\u4ee5\u6279\u6b21\u5927\u5c0fbatch_size\n    loss = loss / batch_size  # (1,)\n    return loss\n\ndef convert_logits_to_ids(\n        logits: torch.tensor,\n        mask_positions: torch.tensor):\n    '''\n    \u8f93\u5165Language Model\u7684\u8bcd\u8868\u6982\u7387\u5206\u5e03\uff08LMModel\u7684logits\uff09\uff0c\u5c06mask_position\u4f4d\u7f6e\u7684token logits\u8f6c\u6362\u4e3atoken\u7684id\u3002\n    :param logits: (torch.tensor): model output -&gt; (batch, seq_len, vocab_size) [8, 512, 21128]\n    :param mask_positions: (torch.tensor): mask token\u7684\u4f4d\u7f6e -&gt; (batch, mask_label_num) [8, 2]\n    :return: \u5bf9\u5e94mask position\u4e0a\u6700\u5927\u6982\u7387\u7684\u63a8\u7406token -&gt; (batch, mask_label_num) [8, 2]\n    '''\n    # \u83b7\u53d6\u6807\u7b7e\u7684\u957f\u5ea6\uff0cmask_positions.size()\u8fd4\u56de\u7684\u662f\u4e00\u4e2a\u5305\u542b\u7ef4\u5ea6\u7684\u5143\u7ec4\uff0c[1]\u8868\u793a\u83b7\u53d6\u7b2c\u4e8c\u4e2a\u7ef4\u5ea6\u7684\u5927\u5c0f\n    label_length = mask_positions.size()[1]\n    # print(f'label_length--&gt;{label_length}')\n\n    # \u83b7\u53d6\u6279\u6b21\u5927\u5c0f\u3001\u5e8f\u5217\u957f\u5ea6\u548c\u8bcd\u6c47\u8868\u5927\u5c0f\uff0clogits.size()\u8fd4\u56de\u7684\u662f\u4e00\u4e2a\u5305\u542b\u7ef4\u5ea6\u7684\u5143\u7ec4\n    batch_size, seq_len, vocab_size = logits.size()\n\n    # \u521d\u59cb\u5316\u4e00\u4e2a\u7a7a\u5217\u8868\uff0c\u7528\u4e8e\u5b58\u50a8\u91cd\u5851\u540e\u7684 mask_positions\n    mask_positions_after_reshaped = []\n\n    # print(f'mask_positions.detach().cpu().numpy().tolist()--&gt;{mask_positions.detach().cpu().numpy().tolist()}')\n    # \u904d\u5386\u6bcf\u4e2a\u6279\u6b21\u7684mask_positions\n    for batch, mask_pos in enumerate(mask_positions.detach().cpu().numpy().tolist()):\n        # \u904d\u5386\u6bcf\u4e2amask\u4f4d\u7f6e\n        for pos in mask_pos:\n            # \u5c06\u6279\u6b21\u53f7\u548c\u5e8f\u5217\u4e2d\u7684mask\u4f4d\u7f6e\u7ed3\u5408\u8d77\u6765\uff0c\u5f97\u5230\u91cd\u5851\u540e\u7684mask_positions\n            mask_positions_after_reshaped.append(batch * seq_len + pos)\n    # print(f'mask_positions_after_reshaped--&gt;{mask_positions_after_reshaped}')\n    # print(f'\u539f\u59cb\u7684logits--&gt;{logits.shape}')\n\n    # \u5c06\u539f\u59cb\u7684logits\u91cd\u5851\u4e3a(batch_size * seq_len, vocab_size)\u7684\u5f62\u72b6\n    logits = logits.reshape(batch_size * seq_len, -1)  # (batch_size * seq_len, vocab_size)\n    # print(f'\u6539\u53d8\u539f\u59cb\u6a21\u578b\u8f93\u51fa\u7684\u7ed3\u679c\u5f62\u72b6--&gt;{logits.shape}')\n\n    # \u4ece\u91cd\u5851\u540e\u7684logits\u4e2d\uff0c\u9009\u62e9\u51fa\u88ab\u63a9\u7801\u4f4d\u7f6e\u7684logits\n    mask_logits = logits[mask_positions_after_reshaped]\n    # print(f'\u88ab\u63a9\u7801\u4f4d\u7f6e\u7684logits--&gt;{mask_logits.shape}')\n\n    # \u83b7\u53d6\u6bcf\u4e2a\u6837\u672cmask\u4f4d\u7f6e\u6240\u9884\u6d4b\u7684tokens\n    predict_tokens = mask_logits.argmax(dim=-1)  # (batch * label_num)\n    # print(f'\u83b7\u53d6\u6bcf\u4e2a\u6837\u672cmask\u4f4d\u7f6e\u9884\u6d4b\u7684tokens', predict_tokens)\n\n    # \u5c06\u6bcf\u4e2a\u6837\u672cmask\u4f4d\u7f6e\u9884\u6d4b\u7684tokens\u91cd\u5851\u4e3a(batch, label_num)\u7684\u5f62\u72b6\n    predict_tokens = predict_tokens.reshape(-1, label_length)  # (batch, label_num)\n    # print(f'predict_tokens--&gt;{predict_tokens}')\n\n    return predict_tokens\n</code></pre>"},{"location":"03-%E6%96%B0%E9%9B%B6%E5%94%AE%E8%A1%8C%E4%B8%9A%E8%AF%84%E4%BB%B7%E5%86%B3%E7%AD%96%E7%B3%BB%E7%BB%9F/07-BERT%2BP-Tuning%E6%96%B9%E5%BC%8F%E6%A8%A1%E5%9E%8B%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0%E4%B8%8E%E8%AE%AD%E7%BB%83.html#13-metirc_utilspy","title":"1.3 metirc_utils.py","text":"<ul> <li>\u76ee\u7684\uff1a\u5b9a\u4e49\uff08\u591a\uff09\u5206\u7c7b\u95ee\u9898\u4e0b\u7684\u6307\u6807\u8bc4\u4f30\uff08acc, precision, recall, f1\uff09\u3002</li> <li>\u4ee3\u7801\u5982\u4e0b\uff1a</li> </ul> <pre><code>from typing import List\nimport numpy as np\nfrom sklearn.metrics import accuracy_score, precision_score, f1_score\nfrom sklearn.metrics import recall_score, confusion_matrix\n\n\nclass ClassEvaluator(object):\n    def __init__(self):\n        # \u521d\u59cb\u5316\u771f\u5b9e\u7ed3\u679c\u548c\u9884\u6d4b\u7ed3\u679c\u7684\u5217\u8868\n        self.goldens = []  # \u5b58\u50a8\u771f\u5b9e\u7ed3\u679c\u6570\u636e\n        self.predictions = []  # \u5b58\u50a8\u9884\u6d4b\u7ed3\u679c\u6570\u636e\n\n    def add_batch(self,\n                  pred_batch: List[List],\n                  gold_batch: List[List]):\n        '''\n        \u6dfb\u52a0\u4e00\u4e2abatch\u4e2d\u7684prediction\u548cgold\u5217\u8868\uff0c\u7528\u4e8e\u540e\u7eed\u7edf\u4e00\u8ba1\u7b97\u3002\n        :param pred_batch: (list): \u6a21\u578b\u9884\u6d4b\u6807\u7b7e\u5217\u8868, e.g. -&gt;  [['\u4f53', '\u80b2'], ['\u8d22', '\u7ecf'], ...]\n        :param gold_batch: (list): \u771f\u5b9e\u6807\u7b7e\u6807\u7b7e\u5217\u8868, e.g. -&gt;  [['\u4f53', '\u80b2'], ['\u8d22', '\u7ecf'], ...]\n        :return:\n        '''\n        # \u786e\u4fdd\u9884\u6d4b\u6279\u6b21\u548c\u771f\u5b9e\u6279\u6b21\u957f\u5ea6\u4e00\u81f4\uff0c\u8fd9\u662f\u540e\u7eed\u5904\u7406\u7684\u524d\u63d0\u6761\u4ef6\n        assert len(pred_batch) == len(gold_batch)\n        # print(f'pred_batch0--&gt;{pred_batch}')\n        # print(f'gold_batch0--&gt;{gold_batch}')\n\n        # \u82e5\u9047\u5230\u591a\u4e2a\u5b50\u6807\u7b7e\u6784\u6210\u4e00\u4e2a\u6807\u7b7e\u7684\u60c5\u51b5\n        # \u5224\u65adgold_batch\u7684\u7b2c\u4e00\u4e2a\u5143\u7d20\u662f\u5426\u4e3a\u5217\u8868\u6216\u5143\u7ec4\u7c7b\u578b\n        if type(gold_batch[0]) in [list, tuple]:\n            # \u5982\u679c\u662f\uff0c\u5219\u5c06pred_batch\u4e2d\u7684\u6bcf\u4e2a\u5143\u7d20\u8f6c\u6362\u4e3a\u5b57\u7b26\u4e32\u540e\u62fc\u63a5\u8d77\u6765\n            pred_batch = [''.join([str(e) for e in ele]) for ele in pred_batch]\n            # \u540c\u6837\u5730\uff0c\u4e5f\u5c06gold_batch\u4e2d\u7684\u6bcf\u4e2a\u5143\u7d20\u8f6c\u6362\u4e3a\u5b57\u7b26\u4e32\u540e\u62fc\u63a5\u8d77\u6765\n            gold_batch = [''.join([str(e) for e in ele]) for ele in gold_batch]\n        # print(f'pred_batch--&gt;{pred_batch}')\n        # print(f'gold_batch--&gt;{gold_batch}')\n\n        # \u5c06\u771f\u5b9e\u7ed3\u679c\u7684\u6279\u6b21\u6570\u636e\u6dfb\u52a0\u5230self.goldens\u5217\u8868\u4e2d\n        self.goldens.extend(gold_batch)\n        # print(f'self.goldens--&gt;{self.goldens}')\n        # \u5c06\u9884\u6d4b\u7ed3\u679c\u7684\u6279\u6b21\u6570\u636e\u6dfb\u52a0\u5230self.predictions\u5217\u8868\u4e2d\n        self.predictions.extend(pred_batch)\n        # print(f'self.predictions--&gt;{self.predictions}')\n\n    def compute(self, round_num=2) -&gt; dict:\n        '''\n        \u6839\u636e\u5f53\u524d\u7c7b\u4e2d\u7d2f\u79ef\u7684\u53d8\u91cf\u503c\uff0c\u8ba1\u7b97\u5f53\u524d\u7684P, R, F1\u3002\n        :param round_num: (int): \u8ba1\u7b97\u7ed3\u679c\u4fdd\u7559\u5c0f\u6570\u70b9\u540e\u51e0\u4f4d, \u9ed8\u8ba4\u5c0f\u6570\u70b9\u540e2\u4f4d\u3002\n        :return:\n        dict -&gt; {\n            'accuracy': \u51c6\u786e\u7387,\n            'precision': \u7cbe\u51c6\u7387,\n            'recall': \u53ec\u56de\u7387,\n            'f1': f1\u503c,\n            'class_metrics': {\n                '0': {\n                        'precision': \u8be5\u7c7b\u522b\u4e0b\u7684precision,\n                        'recall': \u8be5\u7c7b\u522b\u4e0b\u7684recall,\n                        'f1': \u8be5\u7c7b\u522b\u4e0b\u7684f1\n                    },\n                ...\n            }\n        }\n        '''\n        # print(f'self.goldens--&gt;{self.goldens}')\n        # print(f'self.predictions--&gt;{self.predictions}')\n        # \u521d\u59cb\u5316\u7c7b\u522b\u96c6\u5408\u3001\u7c7b\u522b\u6307\u6807\u5b57\u5178\u548c\u7ed3\u679c\u5b57\u5178\uff0c\u7528\u4e8e\u5b58\u50a8\u5168\u5c40\u6307\u6807\n        # \u5c06 self.goldens \u548c self.predictions \u7684\u96c6\u5408\u5408\u5e76\uff0c\u5e76\u8fdb\u884c\u6392\u5e8f\uff0c\u7ed3\u679c\u5b58\u50a8\u5728\u53d8\u91cf classes \u4e2d\u3002\n        classes = sorted(list(set(self.goldens) | set(self.predictions)))\n        class_metrics = {}\n        res = {}\n        # print(f'classes--&gt;{classes}')\n\n        # \u6784\u5efa\u5168\u5c40\u6307\u6807\n        # \u8ba1\u7b97\u5e76\u5b58\u50a8\u5168\u5c40\u51c6\u786e\u7387\n        res['accuracy'] = round(accuracy_score(self.goldens, self.predictions), round_num)\n        # \u8ba1\u7b97\u5e76\u5b58\u50a8\u5168\u5c40\u7cbe\u786e\u7387\n        res['precision'] = round(precision_score(self.goldens, self.predictions, average='weighted'), round_num)\n        # \u8ba1\u7b97\u5e76\u5b58\u50a8\u5168\u5c40\u53ec\u56de\u7387\n        res['recall'] = round(recall_score(self.goldens, self.predictions, average='weighted'), round_num)\n        # \u8ba1\u7b97\u5e76\u5b58\u50a8\u5168\u5c40F1\u5206\u6570\n        res['f1'] = round(f1_score(self.goldens, self.predictions, average='weighted'), round_num)\n        # print(f'res--&gt;{res}')\n\n        try:\n            # \u8ba1\u7b97\u6df7\u6dc6\u77e9\u9635\uff0c\u5e76\u5c06\u5176\u8f6c\u6362\u4e3anumpy\u6570\u7ec4\uff0c\u5f62\u72b6\u4e3a(n_class, n_class)\n            conf_matrix = np.array(confusion_matrix(self.goldens, self.predictions))\n            # print(f'conf_matrix--&gt;{conf_matrix}')\n            # \u786e\u4fdd\u6df7\u6dc6\u77e9\u9635\u7684\u7ef4\u5ea6\u4e0e\u7c7b\u522b\u6570\u91cf\u5339\u914d\n            assert conf_matrix.shape[0] == len(classes)\n            # \u904d\u5386\u6bcf\u4e2a\u7c7b\u522b\uff0c\u8ba1\u7b97\u7cbe\u786e\u5ea6(precision)\u3001\u53ec\u56de\u7387(recall)\u548cF1\u5206\u6570(f1)\n            for i in range(conf_matrix.shape[0]):\n                # \u8ba1\u7b97\u5f53\u524d\u7c7b\u522b\u7684\u7cbe\u786e\u5ea6\n                precision = 0 if sum(conf_matrix[:, i]) == 0 else (conf_matrix[i, i] / sum(conf_matrix[:, i]))\n                # \u8ba1\u7b97\u5f53\u524d\u7c7b\u522b\u7684\u53ec\u56de\u7387\n                recall = 0 if sum(conf_matrix[i, :]) == 0 else (conf_matrix[i, i] / sum(conf_matrix[i, :]))\n                # \u8ba1\u7b97\u5f53\u524d\u7c7b\u522b\u7684F1\u5206\u6570\n                f1 = 0 if (precision + recall) == 0 else (2 * precision * recall / (precision + recall))\n                # \u5c06\u5f53\u524d\u7c7b\u522b\u7684\u7cbe\u786e\u5ea6\u3001\u53ec\u56de\u7387\u548cF1\u5206\u6570\u4fdd\u5b58\u5230\u5b57\u5178\u4e2d\n                class_metrics[classes[i]] = {\n                    'precision': round(precision, round_num),\n                    'recall': round(recall, round_num),\n                    'f1': round(f1, round_num)\n                }\n            # \u5c06\u6240\u6709\u7c7b\u522b\u7684\u6307\u6807\u4fdd\u5b58\u5230\u7ed3\u679c\u5b57\u5178\u4e2d\n            res['class_metrics'] = class_metrics\n        except Exception as e:\n            # \u5f02\u5e38\u5904\u7406\uff1a\u5f53\u8ba1\u7b97\u7c7b\u522b\u6307\u6807\u65f6\u53d1\u751f\u5f02\u5e38\uff0c\u6253\u5370\u8b66\u544a\u4fe1\u606f\u548c\u76f8\u5173\u6570\u636e\n            print(f'[Warning] Something wrong when calculate class_metrics: {e}')\n            print(f'--&gt; goldens: {set(self.goldens)}')\n            print(f'--&gt; predictions: {set(self.predictions)}')\n            print(f'--&gt; diff elements: {set(self.predictions) - set(self.goldens)}')\n            # \u5c06\u7ed3\u679c\u5b57\u5178\u4e2d\u7684\u7c7b\u522b\u6307\u6807\u8bbe\u7f6e\u4e3a\u7a7a\u5b57\u5178\n            res['class_metrics'] = {}\n\n        return res\n\n    def reset(self):\n        \"\"\"\n        \u91cd\u7f6e\u79ef\u7d2f\u7684\u6570\u503c\u3002\n        \"\"\"\n        self.goldens = []\n        self.predictions = []\n\n\nif __name__ == '__main__':\n    metric = ClassEvaluator()\n    metric.add_batch(\n        [['\u8d22', '\u7ecf'], ['\u8d22', '\u7ecf'], ['\u4f53', '\u80b2'], ['\u4f53', '\u80b2'], ['\u8ba1', '\u7b97', '\u673a']],\n        [['\u4f53', '\u80b2'], ['\u8d22', '\u7ecf'], ['\u4f53', '\u80b2'], ['\u8ba1', '\u7b97', '\u673a'], ['\u8ba1', '\u7b97', '\u673a']],\n    )\n    # metric.add_batch(\n    #     [0, 0, 1, 1, 0],\n    #     [1, 1, 1, 0, 0]\n    # )\n    res = metric.compute()\n    print(res)\n</code></pre> <p>print\u4ee3\u7801\u7ed3\u679c\uff1a</p> <p><code>python {'accuracy': 0.6,      'precision': 0.7,      'recall': 0.6,      'f1': 0.6,      'class_metrics':      {'\u4f53\u80b2': {'precision': np.float64(0.5), 'recall': np.float64(0.5), 'f1': np.float64(0.5)},       '\u8ba1\u7b97\u673a': {'precision': np.float64(1.0), 'recall': np.float64(0.5), 'f1': np.float64(0.67)},       '\u8d22\u7ecf': {'precision': np.float64(0.5), 'recall': np.float64(1.0), 'f1': np.float64(0.67)}     }}</code></p>"},{"location":"03-%E6%96%B0%E9%9B%B6%E5%94%AE%E8%A1%8C%E4%B8%9A%E8%AF%84%E4%BB%B7%E5%86%B3%E7%AD%96%E7%B3%BB%E7%BB%9F/07-BERT%2BP-Tuning%E6%96%B9%E5%BC%8F%E6%A8%A1%E5%9E%8B%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0%E4%B8%8E%E8%AE%AD%E7%BB%83.html#2","title":"2 \u5b9e\u73b0\u6a21\u578b\u8bad\u7ec3\u51fd\u6570,\u9a8c\u8bc1\u51fd\u6570","text":"<ul> <li> <p>\u76ee\u7684\uff1a\u5b9e\u73b0\u6a21\u578b\u7684\u8bad\u7ec3\u548c\u9a8c\u8bc1\uff0c\u811a\u672c\u91cc\u9762\u5305\u542b\u4e24\u4e2a\u51fd\u6570\uff1amodel2train()\u548cevaluate_model()</p> </li> <li> <p>\u4ee3\u7801\u8def\u5f84\uff1allm_tuning/prompt_tasks/P_Tuning/train.py</p> <p>\u4ee3\u7801\u5982\u4e0b\uff1a</p> </li> </ul> <pre><code>import os\nimport time\n\nimport torch\nfrom tqdm import tqdm\nfrom transformers import AutoModelForMaskedLM, AutoTokenizer, get_scheduler\n\nfrom prompt_tasks.PET.data_handle.data_loader import get_data\nfrom prompt_tasks.PET.pet_config import ProjectConfig\nfrom prompt_tasks.PET.utils.common_utils import mlm_loss, convert_logits_to_ids\nfrom prompt_tasks.PET.utils.metirc_utils import ClassEvaluator\nfrom prompt_tasks.PET.utils.verbalizer import Verbalizer\n\npc = ProjectConfig()\n\n\ndef model2train():\n    # \u52a0\u8f7d\u8bad\u7ec3\u6570\u636e\u548c\u9a8c\u8bc1\u6570\u636e\n    train_dataloader, dev_dataloader = get_data()\n\n    # \u52a0\u8f7d\u9884\u8bad\u7ec3\u6a21\u578b\n    model = AutoModelForMaskedLM.from_pretrained(pc.pre_model).to(pc.device)\n    # print(f'\u9884\u8bad\u7ec3\u6a21\u578b\u5e26MLM\u5934\u7684--&gt;{model}')\n    # \u52a0\u8f7d\u5206\u8bcd\u5668\n    tokenizer = AutoTokenizer.from_pretrained(pc.pre_model)\n    # \u52a0\u8f7d\u6620\u5c04\u8bcd\u8868\n    verbalizer = Verbalizer(verbalizer_file=pc.verbalizer,\n                            tokenizer=tokenizer,\n                            max_label_len=pc.max_label_len)\n    # print(f'verbalizer--&gt;{verbalizer.label_dict}')\n\n    # \u4e0d\u9700\u8981\u6743\u91cd\u8870\u51cf\u7684\u53c2\u6570\n    no_decay = [\"bias\", \"LayerNorm.weight\"]\n    # print(type(model.parameters()))\n    # \u5b9a\u4e49\u4f18\u5316\u5668\u7684\u53c2\u6570\u7ec4\uff0c\u4ee5\u4fbf\u5bf9\u6a21\u578b\u7684\u4e0d\u540c\u90e8\u5206\u5e94\u7528\u4e0d\u540c\u7684\u6743\u91cd\u8870\u51cf\n    optimizer_grouped_parameters = [\n        # \u7b2c\u4e00\u7ec4\u53c2\u6570\uff1a\u5305\u542b\u6240\u6709\u9002\u7528\u6743\u91cd\u8870\u51cf\u7684\u6a21\u578b\u53c2\u6570\n        {\n            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n            \"weight_decay\": pc.weight_decay,\n        },\n        # \u7b2c\u4e8c\u7ec4\u53c2\u6570\uff1a\u5305\u542b\u6240\u6709\u4e0d\u9002\u7528\u6743\u91cd\u8870\u51cf\u7684\u6a21\u578b\u53c2\u6570\n        {\n            \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n            \"weight_decay\": 0.0,\n        },\n    ]\n    # \u521d\u59cb\u5316AdamW\u4f18\u5316\u5668\uff0c\u7528\u4e8e\u6a21\u578b\u53c2\u6570\u7684\u4f18\u5316\n    # AdamW\u662fAdam\u7b97\u6cd5\u7684\u53d8\u4f53\uff0c\u52a0\u5165\u4e86\u6743\u91cd\u8870\u51cf\uff08L2\u6b63\u5219\u5316\uff09\uff0c\u6709\u52a9\u4e8e\u9632\u6b62\u8fc7\u62df\u5408\n    # \u53c2\u6570optimizer_grouped_parameters\u662f\u5206\u7ec4\u7684\u6a21\u578b\u53c2\u6570\uff0c\u5141\u8bb8\u5bf9\u4e0d\u540c\u7684\u53c2\u6570\u5e94\u7528\u4e0d\u540c\u7684\u5b66\u4e60\u7387\u6216\u6b63\u5219\u5316\u5f3a\u5ea6\n    optimizer = torch.optim.AdamW(optimizer_grouped_parameters, lr=pc.learning_rate)\n\n    # \u6839\u636e\u8bad\u7ec3\u8f6e\u6570\u8ba1\u7b97\u6700\u5927\u8bad\u7ec3\u6b65\u6570\uff0c\u4ee5\u4fbf\u4e8escheduler\u52a8\u6001\u8c03\u6574lr\n    num_update_steps_per_epoch = len(train_dataloader)\n    # \u6307\u5b9a\u603b\u7684\u8bad\u7ec3\u6b65\u6570\uff0c\u5b83\u4f1a\u88ab\u5b66\u4e60\u7387\u8c03\u5ea6\u5668\u7528\u6765\u786e\u5b9a\u5b66\u4e60\u7387\u7684\u53d8\u5316\u89c4\u5f8b\uff0c\u786e\u4fdd\u5b66\u4e60\u7387\u5728\u6574\u4e2a\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u5f97\u4ee5\u5408\u7406\u5730\u8c03\u8282\n    max_train_steps = pc.epochs * num_update_steps_per_epoch\n    # \u8ba1\u7b97\u9884\u70ed\u9636\u6bb5\u7684\u8bad\u7ec3\u6b65\u6570\uff0c\u7528\u4e8e\u521d\u59cb\u5316\u5b66\u4e60\u7387\u8c03\u5ea6\n    warm_steps = int(pc.warmup_ratio * max_train_steps)  # \u9884\u70ed\u9636\u6bb5\u7684\u8bad\u7ec3\u6b65\u6570\n    # \u521b\u5efa\u5b66\u4e60\u7387\u8c03\u5ea6\u5668\uff0c\u4f7f\u7528\u7ebf\u6027\u8c03\u5ea6\u7b56\u7565\uff0c\u6839\u636e\u8bad\u7ec3\u7684\u8fdb\u884c\u9010\u6b65\u8c03\u6574\u5b66\u4e60\u7387\n    lr_scheduler = get_scheduler(\n        name='linear',\n        optimizer=optimizer,\n        num_warmup_steps=warm_steps,\n        num_training_steps=max_train_steps)\n\n    # \u521d\u59cb\u5316\u635f\u5931\u5217\u8868\uff0c\u7528\u4e8e\u8bb0\u5f55\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7684\u635f\u5931\u503c\n    loss_list = []\n    # \u8bb0\u5f55\u8bad\u7ec3\u5f00\u59cb\u7684\u65f6\u95f4\uff0c\u7528\u4e8e\u8ba1\u7b97\u8bad\u7ec3\u65f6\u957f\n    tic_train = time.time()\n    # \u521b\u5efa\u5206\u7c7b\u8bc4\u4f30\u5668\uff0c\u7528\u4e8e\u8bc4\u4f30\u6a21\u578b\u6027\u80fd\n    metric = ClassEvaluator()\n    # \u5b9a\u4e49\u635f\u5931\u51fd\u6570\uff0c\u7528\u4e8e\u8ba1\u7b97\u6a21\u578b\u9884\u6d4b\u503c\u4e0e\u771f\u5b9e\u6807\u7b7e\u4e4b\u95f4\u7684\u5dee\u5f02\n    criterion = torch.nn.CrossEntropyLoss()\n    # \u521d\u59cb\u5316\u8bad\u7ec3\u6b21\u6570\u548c\u6700\u4f73F1\u5206\u6570\uff0c\u7528\u4e8e\u8ddf\u8e2a\u8bad\u7ec3\u8fdb\u5ea6\u548c\u6a21\u578b\u6027\u80fd\n    global_step, best_f1 = 0, 0\n\n    print('\u5f00\u59cb\u8bad\u7ec3\uff1a')\n    for epoch in range(pc.epochs):\n        for batch in tqdm(train_dataloader, desc='\u6a21\u578b\u8bad\u7ec3'):\n            # print(f'batch--&gt;{batch}')\n            # \u5c06\u6279\u6b21\u6570\u636e\u8f93\u5165\u6a21\u578b\uff0c\u83b7\u53d6logits\n            logits = model(input_ids=batch['input_ids'].to(pc.device),\n                           token_type_ids=batch['token_type_ids'].to(pc.device),\n                           attention_mask=batch['attention_mask'].to(pc.device)).logits\n            # print(f'logits-&gt;{logits.shape}')\n\n            # \u771f\u5b9e\u6807\u7b7e\n            mask_labels = batch['mask_labels'].numpy().tolist()\n            # print(f'mask_labels---&gt;{mask_labels}')\n            # \u63d0\u53d6\u5b50\u6807\u7b7e\n            sub_labels = verbalizer.batch_find_sub_labels(mask_labels)\n            # print(f'sub_labels---&gt;{sub_labels}')\n            # \u83b7\u53d6\u5b50\u6807\u7b7e\u7684token_ids\n            sub_labels = [ele['token_ids'] for ele in sub_labels]\n            # print(f'sub_labels_token_ids---&gt;{sub_labels}')\n\n            # \u8ba1\u7b97\u63a9\u7801\u8bed\u8a00\u6a21\u578b\u7684\u635f\u5931\u503c\n            loss = mlm_loss(logits,\n                            batch['mask_positions'].to(pc.device),\n                            sub_labels,\n                            criterion,\n                            pc.device)\n            # print(f'\u8ba1\u7b97\u635f\u5931\u503c--&gt;{loss}')\n            # \u6e05\u96f6\u4f18\u5316\u5668\u7684\u68af\u5ea6\n            optimizer.zero_grad()\n            # \u53cd\u5411\u4f20\u64ad\u8ba1\u7b97\u68af\u5ea6\n            loss.backward()\n            # \u66f4\u65b0\u6a21\u578b\u53c2\u6570\n            optimizer.step()\n            # \u66f4\u65b0\u5b66\u4e60\u7387\u8c03\u5ea6\u5668\n            lr_scheduler.step()\n\n            # \u5c06\u635f\u5931\u503c\u6dfb\u52a0\u5230\u635f\u5931\u5217\u8868\u4e2d\n            loss_list.append(loss)\n            # \u8bad\u7ec3\u6b21\u6570\u589e\u52a01\n            global_step += 1\n            # \u6253\u5370\u8bad\u7ec3\u65e5\u5fd7\n            if global_step % pc.logging_steps == 0:\n                time_diff = time.time() - tic_train\n                loss_avg = sum(loss_list) / len(loss_list)\n                print(\"global step %d, epoch: %d, loss: %.5f, speed: %.2f step/s\"\n                      % (global_step, epoch, loss_avg, pc.logging_steps / time_diff))\n                tic_train = time.time()\n        # \u6a21\u578b\u9a8c\u8bc1\n        # \u4f7f\u7528\u7ed9\u5b9a\u7684\u6a21\u578b\u3001\u8bc4\u4f30\u6307\u6807\u3001\u6570\u636e\u52a0\u8f7d\u5668\u3001\u5206\u8bcd\u5668\u548c\u6807\u8bb0\u5316\u5668\u8fdb\u884c\u6a21\u578b\u8bc4\u4f30\n        acc, precision, recall, f1, class_metrics = evaluate_model(model,\n                                                                   metric,\n                                                                   dev_dataloader,\n                                                                   tokenizer,\n                                                                   verbalizer)\n\n        # \u6253\u5370\u8bc4\u4f30\u7ed3\u679c\u4e2d\u7684\u7cbe\u786e\u5ea6\u3001\u53ec\u56de\u7387\u548cF1\u5206\u6570\n        print(\"\u9a8c\u8bc1\u96c6\u7684 precision: %.5f, recall: %.5f, F1: %.5f\" % (precision, recall, f1))\n        # \u5982\u679c\u5f53\u524dF1\u5206\u6570\u9ad8\u4e8e\u6700\u4f73F1\u5206\u6570\uff0c\u5219\u66f4\u65b0\u6700\u4f73F1\u5206\u6570\u548c\u76f8\u5173\u6a21\u578b\u53ca\u5206\u8bcd\u5668\n        if f1 &gt; best_f1:\n            print(\n                f\"\u6700\u597d\u7684f1\u5206\u6570\u88ab\u66f4\u65b0: {best_f1:.5f} --&gt; {f1:.5f}\"\n            )\n            print(f'\u6bcf\u79cd\u7c7b\u578b\u7684Metrics\u4e3a: {class_metrics}')\n            # \u66f4\u65b0\u5f53\u524d\u6700\u4f73\u7684F1\u5206\u6570\n            best_f1 = f1\n            # \u5b9a\u4e49\u5f53\u524d\u4fdd\u5b58\u6a21\u578b\u548c\u5206\u8bcd\u5668\u7684\u76ee\u5f55\n            cur_save_dir = os.path.join(pc.save_dir, \"model_best\")\n            print(cur_save_dir)\n            # \u68c0\u67e5\u5e76\u521b\u5efa\u4fdd\u5b58\u76ee\u5f55\uff08\u5982\u679c\u4e0d\u5b58\u5728\uff09\n            if not os.path.exists(cur_save_dir):\n                os.makedirs(cur_save_dir)\n            # \u4fdd\u5b58\u6a21\u578b\u5230\u6307\u5b9a\u76ee\u5f55\n            model.save_pretrained(cur_save_dir)\n            # \u4fdd\u5b58\u5206\u8bcd\u5668\u5230\u6307\u5b9a\u76ee\u5f55\n            tokenizer.save_pretrained(cur_save_dir)\n        tic_train = time.time()\n\n    print('\u8bad\u7ec3\u7ed3\u675f')\n\n\ndef evaluate_model(model,\n                   metric,\n                   data_loader,\n                   tokenizer,\n                   verbalizer):\n    '''\n    \u5728\u6d4b\u8bd5\u96c6\u4e0a\u8bc4\u4f30\u5f53\u524d\u6a21\u578b\u7684\u8bad\u7ec3\u6548\u679c\u3002\n    :param model: \u5f53\u524d\u6a21\u578b\n    :param metric: \u8bc4\u4f30\u6307\u6807\u7c7b(metric)\n    :param data_loader: \u6d4b\u8bd5\u96c6\u7684dataloader\n    :param tokenizer: \u5206\u8bcd\u5668\n    :param verbalizer: \u6620\u5c04\u8868\n    :return:\n    '''\n    model.eval()\n    metric.reset()\n\n    with torch.no_grad():\n        for step, batch in enumerate(tqdm(data_loader, desc='\u6a21\u578b\u9a8c\u8bc1')):\n            # print(f'batch--&gt;{batch}')\n            logits = model(input_ids=batch['input_ids'].to(pc.device),\n                           token_type_ids=batch['token_type_ids'].to(pc.device),\n                           attention_mask=batch['attention_mask'].to(pc.device)).logits\n            # print(f'\u9a8c\u8bc1\u96c6\u6a21\u578b\u9884\u6d4b\u7684\u7ed3\u679c\u2014\u2014\u2014\u2014&gt;{logits.shape}')\n\n            mask_labels = batch['mask_labels'].numpy().tolist()  # (batch, label_num)\n            # print(f\"mask_labels-0--&gt;{mask_labels}\")\n\n            for i in range(len(mask_labels)):  # \u53bb\u6389label\u4e2d\u7684[PAD] token\n                while tokenizer.pad_token_id in mask_labels[i]:\n                    mask_labels[i].remove(tokenizer.pad_token_id)\n            # print(f'mask_labels-1--&gt;{mask_labels}')\n            # \u5c06mask_labels id\u8f6c\u6362\u4e3a\u6587\u5b57\n            mask_labels = [''.join(tokenizer.convert_ids_to_tokens(t)) for t in mask_labels]\n            # print(f'\u771f\u5b9e\u7684\u7ed3\u679c\u4e3b\u6807\u7b7e\uff1amask_labels_str--&gt;{mask_labels}')\n\n            # \u83b7\u53d6\u6a21\u578b\u9884\u6d4b\u7684\u5b50\u6807\u7b7e\n            predictions = convert_logits_to_ids(logits,\n                                                batch['mask_positions']).cpu().numpy().tolist()  # (batch, label_num)\n            # print(f'\u6a21\u578b\u9884\u6d4b\u7684\u5b50\u6807\u7b7e\u7684\u7ed3\u679c--&gt;{predictions}')\n\n            # \u6839\u636e\u6a21\u578b\u9884\u6d4b\u7684\u5b50\u6807\u7b7e\uff0c\u627e\u5230\u5b50label\u5c5e\u4e8e\u7684\u4e3blabel\n            predictions = verbalizer.batch_find_main_label(predictions)  # \u627e\u5230\u5b50label\u5c5e\u4e8e\u7684\u4e3blabel\n            # print(f\"\u627e\u5230\u6a21\u578b\u9884\u6d4b\u7684\u5b50\u6807\u7b7e\u5bf9\u5e94\u7684\u4e3b\u6807\u7b7e\u7684\u7ed3\u679c--&gt;{predictions}')\")\n\n            # \u83b7\u5f97\u9884\u6d4b\u7684\u4e3b\u6807\u7b7e\u540d\n            predictions = [ele['label'] for ele in predictions]\n            # print(f\"\u53ea\u83b7\u5f97\u9884\u6d4b\u7684\u4e3b\u6807\u7b7e\u7684\u7ed3\u679cstring--&gt;{predictions}')\")\n\n            # \u8c03\u7528add_batch\u65b9\u6cd5, \u5c06\u6a21\u578b\u9884\u6d4b\u7684\u4e3b\u6807\u7b7e\u4e0e\u771f\u5b9e\u4e3b\u6807\u7b7e\u4fdd\u5b58\u5230metric\u5c5e\u6027\u4e2d\n            metric.add_batch(pred_batch=predictions, gold_batch=mask_labels)\n    eval_metric = metric.compute()\n    model.train()\n\n    return eval_metric['accuracy'], eval_metric['precision'], \\\n        eval_metric['recall'], eval_metric['f1'], \\\n        eval_metric['class_metrics']\n\n\nif __name__ == '__main__':\n    model2train()\n</code></pre> <ul> <li>\u8f93\u51fa\u7ed3\u679c:</li> </ul> <pre><code>...\nglobal step 350, epoch: 43, loss: 0.10804, speed: 1.20 step/s\nglobal step 360, epoch: 44, loss: 0.10504, speed: 1.22 step/s\nglobal step 370, epoch: 46, loss: 0.10220, speed: 1.21 step/s\nglobal step 380, epoch: 47, loss: 0.09951, speed: 1.20 step/s\nglobal step 390, epoch: 48, loss: 0.09696, speed: 1.20 step/s\nglobal step 400, epoch: 49, loss: 0.09454, speed: 1.22 step/s\nEvaluation precision: 0.76000, recall: 0.70000, F1: 0.70000\n</code></pre> <ul> <li>\u7ed3\u8bba: BERT+P-Tuning\u6a21\u578b\u5728\u8bad\u7ec3\u96c6\u4e0a\u7684\u8868\u73b0\u662fPrecion: 76%</li> <li>\u6ce8\u610f\uff1a\u672c\u9879\u76ee\u4e2d\u53ea\u7528\u4e8660\u6761\u6837\u672c\uff0c\u5728\u63a5\u8fd1400\u6761\u6837\u672c\u4e0a\u7cbe\u786e\u7387\u5c31\u5df2\u7ecf\u8fbe\u5230\u4e8676%\uff0c\u5982\u679c\u60f3\u8ba9\u6307\u6807\u66f4\u9ad8\uff0c\u53ef\u4ee5\u6269\u589e\u6837\u672c\u3002</li> </ul> <p>\u63d0\u5347\u6a21\u578b\u6027\u80fd\uff1a</p> <p>\u589e\u52a0\u8bad\u7ec3\u6570\u636e\u96c6\uff08100\u6761\u5de6\u53f3\u7684\u6570\u636e\uff09\uff1a</p> <pre><code>\u624b\u673a  \u5916\u89c2\u65f6\u5c1a\u65b0\u6f6e\uff0c\u9002\u5408\u5e74\u8f7b\u4eba\u5c55\u73b0\u4e2a\u6027\u3002\n\u624b\u673a  \u5c4f\u5e55\u663e\u793a\u6548\u679c\u975e\u5e38\u51fa\u8272\uff0c\u89c2\u770b\u89c6\u9891\u548c\u6d4f\u89c8\u7f51\u9875\u5f88\u8212\u9002\u3002\n\u7535\u8111  \u4f7f\u7528\u4e86\u4e00\u6bb5\u65f6\u95f4\u7684\u8fd9\u6b3e\u7535\u8111\uff0c\u786c\u76d8\u91c7\u7528WD\uff0c\u8fd0\u884c\u6d41\u7545\u65e0\u5361\u987f\uff0c\u6e29\u5ea6\u63a7\u5236\u8f83\u597d\uff0c\u6027\u4ef7\u6bd4\u4ee4\u4eba\u6ee1\u610f\u3002\n\u624b\u673a  \u624b\u673a\u53cd\u5e94\u7075\u654f\uff0c\u64cd\u4f5c\u754c\u9762\u7b80\u6d01\u6613\u7528\uff0c\u975e\u5e38\u6ee1\u610f\u3002\n\u7535\u5668  \u4ea7\u54c1\u6027\u80fd\u7a33\u5b9a\uff0c\u5f88\u4e0d\u9519\u54e6\uff01\u8d2d\u4e70\u65f6\u6709\u70b9\u62c5\u5fc3\uff0c\u4f46\u6536\u5230\u8d27\u540e\u53d1\u73b0\u662f\u6b63\u54c1\uff0c\u5927\u5bb6\u53ef\u4ee5\u653e\u5fc3\u8d2d\u4e70\u3002\n</code></pre> <p>\u4fee\u6539\u9a8c\u8bc1\u96c6\u810f\u6570\u636e</p> <pre><code># \u539f\u59cb\u6807\u7b7e\u548c\u8bc4\u8bba\u6587\u672c\u5185\u5bb9\u4e0d\u7b26\n\u5e73\u677f  \u624b\u673a\u5f88\u597d\uff0c\u5c31\u662f\u5ba2\u670d\u5783\u573e\u7279\u522b\u662f\u5143\u8c46\n# \u4fee\u6539\u540e\n\u624b\u673a  \u624b\u673a\u5f88\u597d\uff0c\u5c31\u662f\u5ba2\u670d\u5783\u573e\u7279\u522b\u662f\u5143\u8c46\n</code></pre> <p>\u6a21\u578b\u8868\u73b0\uff1a</p> <p>Evaluation precision: 0.79000, recall: 0.70000, F1: 0.71000</p>"},{"location":"03-%E6%96%B0%E9%9B%B6%E5%94%AE%E8%A1%8C%E4%B8%9A%E8%AF%84%E4%BB%B7%E5%86%B3%E7%AD%96%E7%B3%BB%E7%BB%9F/07-BERT%2BP-Tuning%E6%96%B9%E5%BC%8F%E6%A8%A1%E5%9E%8B%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0%E4%B8%8E%E8%AE%AD%E7%BB%83.html#3","title":"3 \u5b9e\u73b0\u6a21\u578b\u9884\u6d4b\u51fd\u6570","text":"<ul> <li>\u76ee\u7684\uff1a\u52a0\u8f7d\u8bad\u7ec3\u597d\u7684\u6a21\u578b\u5e76\u6d4b\u8bd5\u6548\u679c</li> <li>\u4ee3\u7801\u8def\u5f84\uff1allm_tuning/prompt_tasks/P_Tuning/inference.py</li> </ul> <p>\u200b       \u4ee3\u7801\u5982\u4e0b\uff1a</p> <pre><code>import os\nimport time\nfrom typing import List\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForMaskedLM\n\nfrom prompt_tasks.P_Tuning.data_handle.data_preprocess import convert_example\nfrom prompt_tasks.P_Tuning.ptune_config import ProjectConfig\nfrom prompt_tasks.P_Tuning.utils.common_utils import convert_logits_to_ids\nfrom prompt_tasks.P_Tuning.utils.verbalizer import Verbalizer\n\npc = ProjectConfig()\n\nmodel_path = os.path.join(pc.save_dir, 'model_best')\ntokenizer = AutoTokenizer.from_pretrained(model_path)\nmodel = AutoModelForMaskedLM.from_pretrained(model_path).to(pc.device)\nmodel.eval()\n\nmax_label_len = 2  # \u6807\u7b7e\u6700\u5927\u957f\u5ea6\np_embedding_num = 6\nverbalizer = Verbalizer(\n    verbalizer_file=pc.verbalizer,\n    tokenizer=tokenizer,\n    max_label_len=max_label_len)\n\ndef inference(contents: List[str]):\n    '''\n    \u63a8\u7406\u51fd\u6570\uff0c\u8f93\u5165\u539f\u59cb\u53e5\u5b50\uff0c\u8f93\u51famask label\u7684\u9884\u6d4b\u503c\u3002\n    :param contents:\n    :return: \u63cf\u539f\u59cb\u53e5\u5b50\u5217\u8868\u3002\n    '''\n    with (torch.no_grad()):\n        start_time = time.time()\n\n        # \u5c06\u5185\u5bb9\u5c01\u88c5\u4e3a\u793a\u4f8b\u5b57\u5178\uff0c\u51c6\u5907\u8fdb\u884c\u6807\u8bb0\u5316\u5904\u7406\n        examples = {'text': contents}\n\n        # \u5bf9\u793a\u4f8b\u8fdb\u884c\u6807\u8bb0\u5316\u5904\u7406\uff0c\u8fd4\u56de\u6807\u8bb0\u5316\u8f93\u51fa\n        tokenized_output = convert_example(\n            examples,\n            tokenizer,\n            max_seq_len=128,\n            max_label_len=max_label_len,\n            p_embedding_num=p_embedding_num,\n            train_mode=False,\n            return_tensor=True)\n\n        # \u4f7f\u7528\u6a21\u578b\u8fdb\u884c\u9884\u6d4b\uff0c\u83b7\u53d6logits\n        logits = model(input_ids=tokenized_output['input_ids'].to(pc.device),\n                       token_type_ids=tokenized_output['token_type_ids'].to(pc.device),\n                       attention_mask=tokenized_output['attention_mask'].to(pc.device)).logits\n\n        # \u5c06logits\u8f6c\u6362\u4e3a\u9884\u6d4b\u6807\u7b7e\n        predictions = convert_logits_to_ids(logits, tokenized_output['mask_positions']\n                                            ).cpu().numpy().tolist()  # (batch, label_num)\n\n        # \u627e\u5230\u5b50label\u5c5e\u4e8e\u7684\u4e3blabel\n        predictions = verbalizer.batch_find_main_label(predictions)\n\n        # \u63d0\u53d6\u9884\u6d4b\u7684\u6807\u7b7e\n        predictions = [ele['label'] for ele in predictions]\n\n        used = time.time() - start_time\n        print(f'\u8017\u65f6 {used} \u79d2\u3002')\n        return predictions\n\n\nif __name__ == '__main__':\n    contents = [\n        '\u5929\u53f0\u5f88\u597d\u770b\uff0c\u8eba\u5728\u8eba\u6905\u4e0a\u5f88\u60a0\u95f2\uff0c\u56e0\u4e3a\u6d3b\u52a8\u6240\u4ee5\u6211\u89c9\u5f97\u6027\u4ef7\u6bd4\u8fd8\u4e0d\u9519\uff0c\u9002\u5408\u4e00\u5bb6\u51fa\u884c\uff0c\u7279\u522b\u662f\u53bb\u8fea\u58eb\u5c3c\u4e5f\u86ee\u8fd1\u7684\uff0c\u4e0b\u6b21\u6709\u673a\u4f1a\u80af\u5b9a\u8fd8\u4f1a\u518d\u6765\u7684\uff0c\u503c\u5f97\u63a8\u8350',\n        '\u73af\u5883\uff0c\u8bbe\u65bd\uff0c\u5f88\u68d2\uff0c\u5468\u8fb9\u914d\u5957\u8bbe\u65bd\u9f50\u5168\uff0c\u524d\u53f0\u5c0f\u59d0\u59d0\u8d85\u7ea7\u6f02\u4eae\uff01\u9152\u5e97\u5f88\u8d5e\uff0c\u65e9\u9910\u4e0d\u9519\uff0c\u670d\u52a1\u6001\u5ea6\u5f88\u597d\uff0c\u524d\u53f0\u7f8e\u7709\u5f88\u6f02\u4eae\u3002\u6027\u4ef7\u6bd4\u8d85\u9ad8\u7684\u4e00\u5bb6\u9152\u5e97\u3002\u5f3a\u70c8\u63a8\u8350',\n        \"\u7269\u6d41\u8d85\u5feb\uff0c\u9694\u5929\u5c31\u5230\u4e86\uff0c\u8fd8\u6ca1\u7528\uff0c\u5c6f\u7740\u51fa\u6e38\u7684\u65f6\u5019\u7528\u7684\uff0c\u542c\u65b9\u4fbf\u7684\uff0c\u5360\u5730\u5c0f\",\n        \"\u798f\u884c\u5e02\u6765\u5230\u65e0\u65e9\u96c6\u5e02\uff0c\u56e0\u4e3a\u662f\u559c\u6b22\u7684\u9762\u5305\u5e97\uff0c\u6240\u4ee5\u8dd1\u6765\u96c6\u5e02\u770b\u770b\u3002\u7b2c\u4e00\u773c\u5c31\u770b\u5230\u4e86\uff0c\u4e4b\u524d\u5728\u5fae\u5e97\u4e70\u4e86\u5c0f\u5218\uff0c\u8fd9\u6b21\u4e70\u4e86\u8001\u5218\uff0c\u8fd8\u6709\u4e00\u76f4\u559c\u6b22\u7684\u5de7\u514b\u529b\u78c5\u86cb\u7cd5\u3002\u597d\u5947\u8001\u677f\u4e3a\u5565\u4e0d\u505a\u67e0\u6aac\u78c5\u86cb\u7cd5\u4e86\uff0c\u5fae\u5e97\u4e00\u76f4\u90fd\u662f\u4e70\u4e0d\u5230\u7684\u72b6\u6001\u3002\u56e0\u4e3a\u4e0d\u7231\u78b1\u6c34\u786c\u6b27\u4e4b\u7c7b\u7684\uff0c\u6240\u4ee5\u671f\u5f85\u8001\u677f\u591a\u6765\u70b9\u5176\u4ed6\u5c0f\u70b9\uff0c\u997c\u5e72\u4e00\u76f4\u4e5f\u662f\u5927\u7231\uff0c\u90a3\u5929\u597d\u50cf\u4e5f\u6ca1\u770b\u5230\",\n        \"\u670d\u52a1\u5f88\u7528\u5fc3\uff0c\u623f\u578b\u4e5f\u5f88\u8212\u670d\uff0c\u5c0f\u670b\u53cb\u5f88\u559c\u6b22\uff0c\u4e0b\u6b21\u53bb\u5609\u5b9a\u8fd8\u4f1a\u518d\u9009\u62e9\u3002\u5e8a\u94fa\u67d4\u8f6f\u8212\u9002\uff0c\u665a\u4e0a\u4f11\u606f\u5f88\u5b89\u9038\uff0c\u9694\u97f3\u6548\u679c\u4e0d\u9519\u8d5e\uff0c\u4e0b\u6b21\u8fd8\u4f1a\u6765\"\n    ]\n    print(\"\u9488\u5bf9\u4e0b\u9762\u7684\u6587\u672c\u8bc4\u8bba\uff0c\u8bf7\u5206\u522b\u7ed9\u51fa\u5bf9\u5e94\u6240\u5c5e\u7c7b\u522b\uff1a\")\n    res = inference(contents)\n    print('\u63a8\u65ad\u7684\u7c7b\u522b\u4e3a\uff1a', res)\n    new_dict = {}\n    for i in range(len(contents)):\n        new_dict[contents[i]] = res[i]\n    print(f'new_dict--&gt;{new_dict}')\n</code></pre> <ul> <li>\u7ed3\u679c\u5c55\u793a</li> </ul> <pre><code>{\n    '\u5929\u53f0\u5f88\u597d\u770b\uff0c\u8eba\u5728\u8eba\u6905\u4e0a\u5f88\u60a0\u95f2\uff0c\u56e0\u4e3a\u6d3b\u52a8\u6240\u4ee5\u6211\u89c9\u5f97\u6027\u4ef7\u6bd4\u8fd8\u4e0d\u9519\uff0c\u9002\u5408\u4e00\u5bb6\u51fa\n\u884c\uff0c\u7279\u522b\u662f\u53bb\u8fea\u58eb\u5c3c\u4e5f\u86ee\u8fd1\u7684\uff0c\u4e0b\u6b21\u6709\u673a\u4f1a\u80af\u5b9a\u8fd8\u4f1a\u518d\u6765\u7684\uff0c\u503c\u5f97\u63a8\u8350': '\u9152\u5e97',\n    '\u73af\u5883\uff0c\u8bbe\u65bd\uff0c\u5f88\u68d2\uff0c\u5468\u8fb9\u914d\u5957\u8bbe\u65bd\u9f50\u5168\uff0c\u524d\u53f0\u5c0f\u59d0\u59d0\u8d85\u7ea7\u6f02\u4eae\uff01\u9152\u5e97\u5f88\u8d5e\uff0c\u65e9\u9910\u4e0d\n\u9519\uff0c\u670d\u52a1\u6001\u5ea6\u5f88\u597d\uff0c\u524d\u53f0\u7f8e\u7709\u5f88\u6f02\u4eae\u3002\u6027\u4ef7\u6bd4\u8d85\u9ad8\u7684\u4e00\u5bb6\u9152\u5e97\u3002\u5f3a\u70c8\u63a8\u8350': '\u9152\u5e97',\n    '\u7269\u6d41\u8d85\u5feb\uff0c\u9694\u5929\u5c31\u5230\u4e86\uff0c\u8fd8\u6ca1\u7528\uff0c\u5c6f\u7740\u51fa\u6e38\u7684\u65f6\u5019\u7528\u7684\uff0c\u542c\u65b9\u4fbf\u7684\uff0c\u5360\u5730\u5c0f': '\u8863\u670d',\n    '\u798f\u884c\u5e02\u6765\u5230\u65e0\u65e9\u96c6\u5e02\uff0c\u56e0\u4e3a\u662f\u559c\u6b22\u7684\u9762\u5305\u5e97\uff0c\u6240\u4ee5\u8dd1\u6765\u96c6\u5e02\u770b\u770b\u3002\u7b2c\u4e00\u773c\u5c31\u770b\u5230\u4e86\n\uff0c\u4e4b\u524d\u5728\u5fae\u5e97\u4e70\u4e86\u5c0f\u5218\uff0c\u8fd9\u6b21\u4e70\u4e86\u8001\u5218\uff0c\u8fd8\u6709\u4e00\u76f4\u559c\u6b22\u7684\u5de7\u514b\u529b\u78c5\u86cb\u7cd5\u3002\u597d\u5947\u8001\u677f\u4e3a\u5565\u4e0d\u505a\n\u67e0\u6aac\u78c5\u86cb\u7cd5\u4e86\uff0c\u5fae\u5e97\u4e00\u76f4\u90fd\u662f\u4e70\u4e0d\u5230\u7684\u72b6\u6001\u3002\u56e0\u4e3a\u4e0d\u7231\u78b1\u6c34\u786c\u6b27\u4e4b\u7c7b\u7684\uff0c\u6240\u4ee5\u671f\u5f85\u8001\u677f\u591a\u6765\n\u70b9\u5176\u4ed6\u5c0f\u70b9\uff0c\u997c\u5e72\u4e00\u76f4\u4e5f\u662f\u5927\u7231\uff0c\u90a3\u5929\u597d\u50cf\u4e5f\u6ca1\u770b\u5230': '\u5e73\u677f',\n    '\u670d\u52a1\u5f88\u7528\u5fc3\uff0c\u623f\u578b\u4e5f\u5f88\u8212\u670d\uff0c\u5c0f\u670b\u53cb\u5f88\u559c\u6b22\uff0c\u4e0b\u6b21\u53bb\u5609\u5b9a\u8fd8\u4f1a\u518d\u9009\u62e9\u3002\u5e8a\u94fa\u67d4\u8f6f\u8212\n\u9002\uff0c\u665a\u4e0a\u4f11\u606f\u5f88\u5b89\u9038\uff0c\u9694\u97f3\u6548\u679c\u4e0d\u9519\u8d5e\uff0c\u4e0b\u6b21\u8fd8\u4f1a\u6765': '\u9152\u5e97'\n}\n</code></pre>"},{"location":"03-%E6%96%B0%E9%9B%B6%E5%94%AE%E8%A1%8C%E4%B8%9A%E8%AF%84%E4%BB%B7%E5%86%B3%E7%AD%96%E7%B3%BB%E7%BB%9F/07-BERT%2BP-Tuning%E6%96%B9%E5%BC%8F%E6%A8%A1%E5%9E%8B%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0%E4%B8%8E%E8%AE%AD%E7%BB%83.html#_3","title":"\u5c0f\u8282\u603b\u7ed3","text":"<ul> <li>\u672c\u5c0f\u8282\u5b9e\u73b0\u4e86\u57fa\u4e8eBERT+P-Tuning\u6a21\u578b\u7684\u6784\u5efa, \u5e76\u5b8c\u6210\u4e86\u8bad\u7ec3\u548c\u6d4b\u8bd5\u8bc4\u4f30\u3002</li> </ul>"},{"location":"04-%E5%9F%BA%E4%BA%8EChatGLM%E5%BE%AE%E8%B0%83%E5%AE%9E%E7%8E%B0%E4%BF%A1%E6%81%AF%E6%8A%BD%E5%8F%96%2B%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E7%9A%84%E5%A4%9A%E4%BB%BB%E5%8A%A1%E5%AE%9E%E6%88%98/01-%E9%A1%B9%E7%9B%AE%E6%95%B4%E4%BD%93%E4%BB%8B%E7%BB%8D.html","title":"4.1 \u9879\u76ee\u6574\u4f53\u7b80\u4ecb","text":""},{"location":"04-%E5%9F%BA%E4%BA%8EChatGLM%E5%BE%AE%E8%B0%83%E5%AE%9E%E7%8E%B0%E4%BF%A1%E6%81%AF%E6%8A%BD%E5%8F%96%2B%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E7%9A%84%E5%A4%9A%E4%BB%BB%E5%8A%A1%E5%AE%9E%E6%88%98/01-%E9%A1%B9%E7%9B%AE%E6%95%B4%E4%BD%93%E4%BB%8B%E7%BB%8D.html#chatglm-6b","title":"\u57fa\u4e8eChatGLM-6B\u5b8c\u6210\u591a\u4efb\u52a1\u9879\u76ee\u4ecb\u7ecd","text":""},{"location":"04-%E5%9F%BA%E4%BA%8EChatGLM%E5%BE%AE%E8%B0%83%E5%AE%9E%E7%8E%B0%E4%BF%A1%E6%81%AF%E6%8A%BD%E5%8F%96%2B%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E7%9A%84%E5%A4%9A%E4%BB%BB%E5%8A%A1%E5%AE%9E%E6%88%98/01-%E9%A1%B9%E7%9B%AE%E6%95%B4%E4%BD%93%E4%BB%8B%E7%BB%8D.html#_1","title":"\u5b66\u4e60\u76ee\u6807","text":"<ul> <li>\u4e86\u89e3\u9879\u76ee\u80cc\u666f</li> <li>\u7406\u89e3ChatGLM-6B\u6a21\u578b\u67b6\u6784\u539f\u7406</li> <li>\u5b89\u88c5\u9879\u76ee\u5fc5\u5907\u7684\u5de5\u5177\u5305</li> <li>\u4e86\u89e3\u6574\u4f53\u9879\u76ee\u67b6\u6784</li> </ul>"},{"location":"04-%E5%9F%BA%E4%BA%8EChatGLM%E5%BE%AE%E8%B0%83%E5%AE%9E%E7%8E%B0%E4%BF%A1%E6%81%AF%E6%8A%BD%E5%8F%96%2B%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E7%9A%84%E5%A4%9A%E4%BB%BB%E5%8A%A1%E5%AE%9E%E6%88%98/01-%E9%A1%B9%E7%9B%AE%E6%95%B4%E4%BD%93%E4%BB%8B%E7%BB%8D.html#1","title":"1. \u9879\u76ee\u7b80\u4ecb","text":"<p>LLM\uff08Large Language Model\uff09\u901a\u5e38\u62e5\u6709\u5927\u91cf\u7684\u5148\u9a8c\u77e5\u8bc6\uff0c\u4f7f\u5f97\u5176\u5728\u8bb8\u591a\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4efb\u52a1\u4e0a\u90fd\u6709\u7740\u4e0d\u9519\u7684\u6027\u80fd\u3002\u4f46\uff0c\u60f3\u8981\u76f4\u63a5\u5229\u7528 LLM \u5b8c\u6210\u4e00\u4e9b\u4efb\u52a1\u4f1a\u5b58\u5728\u4e00\u4e9b\u7b54\u6848\u89e3\u6790\u4e0a\u7684\u56f0\u96be\uff0c\u5982\u89c4\u8303\u5316\u8f93\u51fa\u683c\u5f0f\uff0c\u4e25\u683c\u670d\u4ece\u8f93\u5165\u4fe1\u606f\u7b49\u3002\u56e0\u6b64\uff0c\u5728\u8fd9\u4e2a\u9879\u76ee\u4e2d\u6211\u4eec\u5bf9\u5927\u6a21\u578b ChatGLM-6B \u8fdb\u884c Finetune\uff0c\u4f7f\u5176\u80fd\u591f\u66f4\u597d\u7684\u5bf9\u9f50\u6211\u4eec\u6240\u9700\u8981\u7684\u8f93\u51fa\u683c\u5f0f\u3002</p>"},{"location":"04-%E5%9F%BA%E4%BA%8EChatGLM%E5%BE%AE%E8%B0%83%E5%AE%9E%E7%8E%B0%E4%BF%A1%E6%81%AF%E6%8A%BD%E5%8F%96%2B%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E7%9A%84%E5%A4%9A%E4%BB%BB%E5%8A%A1%E5%AE%9E%E6%88%98/01-%E9%A1%B9%E7%9B%AE%E6%95%B4%E4%BD%93%E4%BB%8B%E7%BB%8D.html#2-chatglm-6b","title":"2. ChatGLM-6B\u6a21\u578b","text":"<p>ChatGLM-6B \u662f\u6e05\u534e\u5927\u5b66\u63d0\u51fa\u7684\u4e00\u4e2a\u5f00\u6e90\u3001\u652f\u6301\u4e2d\u82f1\u53cc\u8bed\u7684\u5bf9\u8bdd\u8bed\u8a00\u6a21\u578b\uff0c\u57fa\u4e8e General Language Model (GLM) \u67b6\u6784\uff0c\u5177\u6709 62 \u4ebf\u53c2\u6570\u3002\u8be5\u6a21\u578b\u4f7f\u7528\u4e86\u548c ChatGPT \u76f8\u4f3c\u7684\u6280\u672f\uff0c\u7ecf\u8fc7\u7ea6 1T \u6807\u8bc6\u7b26\u7684\u4e2d\u82f1\u53cc\u8bed\u8bad\u7ec3(\u4e2d\u82f1\u6587\u6bd4\u4f8b\u4e3a 1:1)\uff0c\u8f85\u4ee5\u76d1\u7763\u5fae\u8c03\u3001\u53cd\u9988\u81ea\u52a9\u3001\u4eba\u7c7b\u53cd\u9988\u5f3a\u5316\u5b66\u4e60\u7b49\u6280\u672f\u7684\u52a0\u6301\uff0c62 \u4ebf\u53c2\u6570\u7684 ChatGLM-6B \u5df2\u7ecf\u80fd\u751f\u6210\u76f8\u5f53\u7b26\u5408\u4eba\u7c7b\u504f\u597d\u7684\u56de\u7b54\uff08\u76ee\u524d\u4e2d\u6587\u652f\u6301\u6700\u597d\uff09\u3002</p> <p>\u76f8\u6bd4\u539f\u59cbDecoder\u6a21\u5757\uff0cChatGLM-6B\u6a21\u578b\u7ed3\u6784\u6709\u5982\u4e0b\u6539\u52a8\u70b9\uff1a</p> <ul> <li>embedding \u5c42\u68af\u5ea6\u7f29\u51cf\uff1a\u4e3a\u4e86\u63d0\u5347\u8bad\u7ec3\u7a33\u5b9a\u6027\uff0c\u51cf\u5c0f\u4e86 embedding \u5c42\u7684\u68af\u5ea6\u3002\u68af\u5ea6\u7f29\u51cf\u7684\u6548\u679c\u76f8\u5f53\u4e8e\u628a embedding \u5c42\u7684\u68af\u5ea6\u7f29\u5c0f\u4e86 10 \u500d\uff0c\u51cf\u5c0f\u4e86\u68af\u5ea6\u7684\u8303\u6570\u3002</li> <li>layer normalization\uff1a\u91c7\u7528\u4e86\u57fa\u4e8e Deep Norm \u7684 post layer norm\u3002</li> <li>\u6fc0\u6d3b\u51fd\u6570\uff1a\u66ff\u6362ReLU\u6fc0\u6d3b\u51fd\u6570\u91c7\u7528\u4e86 GeGLU \u6fc0\u6d3b\u51fd\u6570\u3002</li> <li>\u4f4d\u7f6e\u7f16\u7801\uff1a\u53bb\u9664\u4e86\u7edd\u5bf9\u4f4d\u7f6e\u7f16\u7801\uff0c\u91c7\u7528\u4e86\u65cb\u8f6c\u4f4d\u7f6e\u7f16\u7801 RoPE\u3002</li> </ul>"},{"location":"04-%E5%9F%BA%E4%BA%8EChatGLM%E5%BE%AE%E8%B0%83%E5%AE%9E%E7%8E%B0%E4%BF%A1%E6%81%AF%E6%8A%BD%E5%8F%96%2B%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E7%9A%84%E5%A4%9A%E4%BB%BB%E5%8A%A1%E5%AE%9E%E6%88%98/01-%E9%A1%B9%E7%9B%AE%E6%95%B4%E4%BD%93%E4%BB%8B%E7%BB%8D.html#22-6b","title":"2.2 \u6a21\u578b\u914d\u7f6e(6B)","text":"\u914d\u7f6e \u6570\u636e \u53c2\u6570 6.2B \u9690\u85cf\u5c42\u7ef4\u5ea6 4096 \u5c42\u6570 28 \u6ce8\u610f\u529b\u5934\u6570 32 \u8bad\u7ec3\u6570\u636e 1T \u8bcd\u8868\u5927\u5c0f 130528 \u6700\u5927\u957f\u5ea6 2048"},{"location":"04-%E5%9F%BA%E4%BA%8EChatGLM%E5%BE%AE%E8%B0%83%E5%AE%9E%E7%8E%B0%E4%BF%A1%E6%81%AF%E6%8A%BD%E5%8F%96%2B%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E7%9A%84%E5%A4%9A%E4%BB%BB%E5%8A%A1%E5%AE%9E%E6%88%98/01-%E9%A1%B9%E7%9B%AE%E6%95%B4%E4%BD%93%E4%BB%8B%E7%BB%8D.html#23","title":"2.3 \u786c\u4ef6\u8981\u6c42(\u5b98\u7f51\u4ecb\u7ecd)","text":"\u91cf\u5316\u7b49\u7ea7 \u6700\u4f4eGPU\u663e\u5b58\uff08\u63a8\u7406\uff09 \u6700\u4f4eGPU\u663e\u5b58\uff08\u9ad8\u6548\u53c2\u6570\u5fae\u8c03\uff09 FP16(\u65e0\u91cf\u5316) 13GB 14GB INT8 10GB 9GB INT4 6GB 7GB <p>\u6ce8\u610f\uff1a\u663e\u5b58\u7684\u5360\u7528\u9664\u4e86\u8ddf\u6a21\u578b\u53c2\u6570\u5927\u5c0f\u6709\u5173\u7cfb\u5916\uff0c\u8fd8\u548c\u6587\u672c\u652f\u6301\u6700\u5927\u957f\u5ea6\u6709\u5173</p>"},{"location":"04-%E5%9F%BA%E4%BA%8EChatGLM%E5%BE%AE%E8%B0%83%E5%AE%9E%E7%8E%B0%E4%BF%A1%E6%81%AF%E6%8A%BD%E5%8F%96%2B%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E7%9A%84%E5%A4%9A%E4%BB%BB%E5%8A%A1%E5%AE%9E%E6%88%98/01-%E9%A1%B9%E7%9B%AE%E6%95%B4%E4%BD%93%E4%BB%8B%E7%BB%8D.html#24","title":"2.4 \u6a21\u578b\u7279\u70b9","text":"<ul> <li>\u4f18\u70b9</li> <li>1.\u8f83\u4f4e\u7684\u90e8\u7f72\u95e8\u69db\uff1a INT4 \u7cbe\u5ea6\u4e0b\uff0c\u53ea\u97006GB\u663e\u5b58\uff0c\u4f7f\u5f97 ChatGLM-6B \u53ef\u4ee5\u90e8\u7f72\u5728\u6d88\u8d39\u7ea7\u663e\u5361\u4e0a\u8fdb\u884c\u63a8\u7406\u3002</li> <li>2.\u66f4\u957f\u7684\u5e8f\u5217\u957f\u5ea6\uff1a \u76f8\u6bd4 GLM-10B\uff08\u5e8f\u5217\u957f\u5ea61024\uff09\uff0cChatGLM2-6B \u5e8f\u5217\u957f\u5ea6\u8fbe32K\uff0c\u652f\u6301\u66f4\u957f\u5bf9\u8bdd\u548c\u5e94\u7528\u3002</li> <li>\u4eba\u7c7b\u7c7b\u610f\u56fe\u5bf9\u9f50\u8bad\u7ec3</li> <li>\u7f3a\u70b9\uff1a</li> <li>\u6a21\u578b\u5bb9\u91cf\u5c0f\uff0c\u76f8\u5bf9\u8f83\u5f31\u7684\u6a21\u578b\u8bb0\u5fc6\u548c\u8bed\u8a00\u80fd\u529b\u3002</li> <li>\u8f83\u5f31\u7684\u591a\u8f6e\u5bf9\u8bdd\u80fd\u529b\u3002</li> </ul>"},{"location":"04-%E5%9F%BA%E4%BA%8EChatGLM%E5%BE%AE%E8%B0%83%E5%AE%9E%E7%8E%B0%E4%BF%A1%E6%81%AF%E6%8A%BD%E5%8F%96%2B%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E7%9A%84%E5%A4%9A%E4%BB%BB%E5%8A%A1%E5%AE%9E%E6%88%98/01-%E9%A1%B9%E7%9B%AE%E6%95%B4%E4%BD%93%E4%BB%8B%E7%BB%8D.html#3","title":"3. \u73af\u5883\u914d\u7f6e","text":""},{"location":"04-%E5%9F%BA%E4%BA%8EChatGLM%E5%BE%AE%E8%B0%83%E5%AE%9E%E7%8E%B0%E4%BF%A1%E6%81%AF%E6%8A%BD%E5%8F%96%2B%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E7%9A%84%E5%A4%9A%E4%BB%BB%E5%8A%A1%E5%AE%9E%E6%88%98/01-%E9%A1%B9%E7%9B%AE%E6%95%B4%E4%BD%93%E4%BB%8B%E7%BB%8D.html#31","title":"3.1 \u57fa\u7840\u73af\u5883\u914d\u7f6e\uff1a","text":"<p>\u672c\u6b21\u73af\u5883\u4f9d\u8d56\u4e8e\u8d8b\u52a8\u4e91https://platform.virtaicloud.com/\u7b97\u529b</p> <ul> <li>\u64cd\u4f5c\u7cfb\u7edf: CentOS 7</li> <li>CPUs: 8 core(s)\uff0c\u5185\u5b58\uff1a48G</li> <li>GPUs: 1\u5361\uff0c A800\uff0c 80GB GPUs</li> <li>Python: 3.10</li> <li>Pytorh: 1.11.0</li> <li>Cuda: 11.3.1</li> <li>\u4ef7\u683c\uff1a13.58\u5143/\u5c0f\u65f6</li> </ul>"},{"location":"04-%E5%9F%BA%E4%BA%8EChatGLM%E5%BE%AE%E8%B0%83%E5%AE%9E%E7%8E%B0%E4%BF%A1%E6%81%AF%E6%8A%BD%E5%8F%96%2B%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E7%9A%84%E5%A4%9A%E4%BB%BB%E5%8A%A1%E5%AE%9E%E6%88%98/01-%E9%A1%B9%E7%9B%AE%E6%95%B4%E4%BD%93%E4%BB%8B%E7%BB%8D.html#32","title":"3.2 \u5b89\u88c5\u4f9d\u8d56\u5305\uff1a","text":"<ol> <li>\u521b\u5efa\u4e00\u4e2a\u865a\u62df\u73af\u5883\uff0c\u60a8\u53ef\u4ee5\u628a <code>llm_env</code> \u4fee\u6539\u4e3a\u4efb\u610f\u4f60\u60f3\u8981\u65b0\u5efa\u7684\u73af\u5883\u540d\u79f0\uff1a</li> </ol> <pre><code>conda create -n llm_env python=3.10\n</code></pre> <ol> <li>\u6fc0\u6d3b\u65b0\u5efa\u865a\u62df\u73af\u5883\u5e76\u5b89\u88c5\u54cd\u5e94\u7684\u4f9d\u8d56\u5305\uff1a</li> </ol> <pre><code>conda activate llm_env\npip install -r requirements.txt\n</code></pre> <p><pre><code>protobuf&gt;=3.19.5,&lt;3.20.1\ntransformers==4.33\nicetk\ncpm_kernels\nstreamlit==1.18.0\nmatplotlib\ndatasets\naccelerate&gt;=0.20.3\npackaging&gt;=20.0\npsutil\npyyaml\npeft==0.3.0\n</code></pre> requirements.txt\u6587\u4ef6\u5185\u5bb9\u5982\u4e0a\u6240\u793a </p>"},{"location":"04-%E5%9F%BA%E4%BA%8EChatGLM%E5%BE%AE%E8%B0%83%E5%AE%9E%E7%8E%B0%E4%BF%A1%E6%81%AF%E6%8A%BD%E5%8F%96%2B%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E7%9A%84%E5%A4%9A%E4%BB%BB%E5%8A%A1%E5%AE%9E%E6%88%98/01-%E9%A1%B9%E7%9B%AE%E6%95%B4%E4%BD%93%E4%BB%8B%E7%BB%8D.html#4","title":"4. \u9879\u76ee\u67b6\u6784","text":"<p>\u9879\u76ee\u67b6\u6784\u6d41\u7a0b\u56fe\uff1a</p> <p>\u9879\u76ee\u4ee3\u7801\u67b6\u6784\u56fe\uff1a</p>"},{"location":"04-%E5%9F%BA%E4%BA%8EChatGLM%E5%BE%AE%E8%B0%83%E5%AE%9E%E7%8E%B0%E4%BF%A1%E6%81%AF%E6%8A%BD%E5%8F%96%2B%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E7%9A%84%E5%A4%9A%E4%BB%BB%E5%8A%A1%E5%AE%9E%E6%88%98/01-%E9%A1%B9%E7%9B%AE%E6%95%B4%E4%BD%93%E4%BB%8B%E7%BB%8D.html#_2","title":"\u5c0f\u7ed3\u603b\u7ed3","text":"<p>\u672c\u7ae0\u8282\u4e3b\u8981\u4ecb\u7ecd\u4e86\u9879\u76ee\u7684\u80cc\u666f\u3001\u9879\u76ee\u7684\u9002\u914d\u73af\u5883\u4ee5\u53ca\u9879\u76ee\u7684\u6574\u4f53\u67b6\u6784\u6d41\u7a0b\uff0c\u4ee5\u6b64\u6765\u65b9\u4fbf\u6211\u4eec\u6574\u4e2a\u9879\u76ee\u7684\u5f00\u53d1\u3002</p>"},{"location":"04-%E5%9F%BA%E4%BA%8EChatGLM%E5%BE%AE%E8%B0%83%E5%AE%9E%E7%8E%B0%E4%BF%A1%E6%81%AF%E6%8A%BD%E5%8F%96%2B%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E7%9A%84%E5%A4%9A%E4%BB%BB%E5%8A%A1%E5%AE%9E%E6%88%98/02-%E5%A4%9A%E4%BB%BB%E5%8A%A1%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86.html","title":"4.2 \u591a\u4efb\u52a1\u6570\u636e\u9884\u5904\u7406\u65b9\u5f0f","text":""},{"location":"04-%E5%9F%BA%E4%BA%8EChatGLM%E5%BE%AE%E8%B0%83%E5%AE%9E%E7%8E%B0%E4%BF%A1%E6%81%AF%E6%8A%BD%E5%8F%96%2B%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E7%9A%84%E5%A4%9A%E4%BB%BB%E5%8A%A1%E5%AE%9E%E6%88%98/02-%E5%A4%9A%E4%BB%BB%E5%8A%A1%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86.html#_1","title":"\u591a\u4efb\u52a1\u6570\u636e\u9884\u5904\u7406\u4ecb\u7ecd","text":""},{"location":"04-%E5%9F%BA%E4%BA%8EChatGLM%E5%BE%AE%E8%B0%83%E5%AE%9E%E7%8E%B0%E4%BF%A1%E6%81%AF%E6%8A%BD%E5%8F%96%2B%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E7%9A%84%E5%A4%9A%E4%BB%BB%E5%8A%A1%E5%AE%9E%E6%88%98/02-%E5%A4%9A%E4%BB%BB%E5%8A%A1%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86.html#_2","title":"\u5b66\u4e60\u76ee\u6807","text":"<ul> <li>\u4e86\u89e3\u672c\u9879\u76ee\u4e2d\u591a\u4efb\u52a1\u6570\u636e\u96c6\u7684\u683c\u5f0f</li> <li>\u638c\u63e1\u5b9e\u73b0\u6570\u636e\u9884\u5904\u7406\u7684\u51fd\u6570\u4ee3\u7801</li> </ul>"},{"location":"04-%E5%9F%BA%E4%BA%8EChatGLM%E5%BE%AE%E8%B0%83%E5%AE%9E%E7%8E%B0%E4%BF%A1%E6%81%AF%E6%8A%BD%E5%8F%96%2B%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E7%9A%84%E5%A4%9A%E4%BB%BB%E5%8A%A1%E5%AE%9E%E6%88%98/02-%E5%A4%9A%E4%BB%BB%E5%8A%A1%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86.html#_3","title":"\u6570\u636e\u9884\u5904\u7406\u8fc7\u7a0b","text":"<ul> <li>\u672c\u9879\u76ee\u4e2d\u5bf9\u6570\u636e\u90e8\u5206\u7684\u9884\u5904\u7406\u6b65\u9aa4\u5982\u4e0b:</li> <li>\u67e5\u770b\u9879\u76ee\u6570\u636e\u96c6</li> <li>\u7f16\u5199Config\u7c7b\u9879\u76ee\u6587\u4ef6\u914d\u7f6e\u4ee3\u7801</li> <li>\u7f16\u5199\u6570\u636e\u5904\u7406\u76f8\u5173\u4ee3\u7801</li> </ul>"},{"location":"04-%E5%9F%BA%E4%BA%8EChatGLM%E5%BE%AE%E8%B0%83%E5%AE%9E%E7%8E%B0%E4%BF%A1%E6%81%AF%E6%8A%BD%E5%8F%96%2B%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E7%9A%84%E5%A4%9A%E4%BB%BB%E5%8A%A1%E5%AE%9E%E6%88%98/02-%E5%A4%9A%E4%BB%BB%E5%8A%A1%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86.html#1","title":"1 \u67e5\u770b\u9879\u76ee\u6570\u636e\u96c6","text":"<ul> <li> <p>\u6570\u636e\u5b58\u653e\u4f4d\u7f6e\uff1allm_tuning/ptune_chatglm/data</p> </li> <li> <p>data\u6587\u4ef6\u5939\u91cc\u9762\u5305\u542b3\u4e2ajsonl\u6587\u6863\uff0c\u5206\u522b\u4e3a\uff1amixed_train_dataset.jsonl\u3001mixed_dev_dataset.jsonl\u3001dataset.jsonl</p> </li> </ul>"},{"location":"04-%E5%9F%BA%E4%BA%8EChatGLM%E5%BE%AE%E8%B0%83%E5%AE%9E%E7%8E%B0%E4%BF%A1%E6%81%AF%E6%8A%BD%E5%8F%96%2B%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E7%9A%84%E5%A4%9A%E4%BB%BB%E5%8A%A1%E5%AE%9E%E6%88%98/02-%E5%A4%9A%E4%BB%BB%E5%8A%A1%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86.html#11-trainjsonl","title":"1.1 train.jsonl","text":"<ul> <li> <p>mixed_train_dataset.jsonl\u4e3a\u8bad\u7ec3\u6570\u636e\u96c6\uff0c\u56e0\u4e3a\u6211\u4eec\u672c\u6b21\u9879\u76ee\u540c\u65f6\u8fdb\u884c\u300c\u4fe1\u606f\u62bd\u53d6+\u6587\u672c\u5206\u7c7b\u300d\u4e24\u9879\u4efb\u52a1\uff0c\u56e0\u6b64\u6570\u636e\u4e2d\u6df7\u5408\u4e86\u4e24\u79cd\u4efb\u52a1\u6570\u636e\u7c7b\u578b\u3002\u4e3e\u4f8b\u5c55\u793a\u5982\u4e0b\uff1a</p> </li> <li> <p>\u4fe1\u606f\u62bd\u53d6\u6570\u636e\u793a\u4f8b</p> </li> <li>Instruction \u90e8\u5206\u544a\u8bc9\u6a21\u578b\u73b0\u5728\u9700\u8981\u505a\u300c\u9605\u8bfb\u7406\u89e3\u300d\u4efb\u52a1\uff0cInput \u90e8\u5206\u544a\u77e5\u6a21\u578b\u8981\u62bd\u53d6\u7684\u53e5\u5b50\u4ee5\u53ca\u8f93\u51fa\u7684\u683c\u5f0f\u3002</li> </ul> <pre><code>{\n    \"context\": \"Instruction: \u4f60\u73b0\u5728\u662f\u4e00\u4e2a\u5f88\u5389\u5bb3\u7684\u9605\u8bfb\u7406\u89e3\u5668\uff0c\u4e25\u683c\u6309\u7167\u4eba\u7c7b\u6307\u4ee4\u8fdb\u884c\u56de\u7b54\u3002\\nInput: \u627e\u5230\u53e5\u5b50\u4e2d\u7684\u4e09\u5143\u7ec4\u4fe1\u606f\u5e76\u8f93\u51fa\u6210json\u7ed9\u6211:\\n\\n\u4e5d\u7384\u73e0\u662f\u5728\u7eb5\u6a2a\u4e2d\u6587\u7f51\u8fde\u8f7d\u7684\u4e00\u90e8\u5c0f\u8bf4\uff0c\u4f5c\u8005\u662f\u9f99\u9a6c\u3002\\nAnswer: \", \n    \"target\": \"```json\\n[{\\\"predicate\\\": \\\"\u8fde\u8f7d\u7f51\u7ad9\\\", \\\"object_type\\\": \\\"\u7f51\u7ad9\\\", \\\"subject_type\\\": \\\"\u7f51\u7edc\u5c0f\u8bf4\\\", \\\"object\\\": \\\"\u7eb5\u6a2a\u4e2d\u6587\u7f51\\\", \\\"subject\\\": \\\"\u4e5d\u7384\u73e0\\\"}, {\\\"predicate\\\": \\\"\u4f5c\u8005\\\", \\\"object_type\\\": \\\"\u4eba\u7269\\\", \\\"subject_type\\\": \\\"\u56fe\u4e66\u4f5c\u54c1\\\", \\\"object\\\": \\\"\u9f99\u9a6c\\\", \\\"subject\\\": \\\"\u4e5d\u7384\u73e0\\\"}]\\n```\"\n}\n</code></pre> <ul> <li>\u6587\u672c\u6570\u636e\u793a\u4f8b</li> <li>Instruction \u90e8\u5206\u544a\u8bc9\u6a21\u578b\u73b0\u5728\u9700\u8981\u505a\u300c\u9605\u8bfb\u7406\u89e3\u300d\u4efb\u52a1\uff0cInput \u90e8\u5206\u544a\u77e5\u6a21\u578b\u8981\u62bd\u53d6\u7684\u53e5\u5b50\u4ee5\u53ca\u8f93\u51fa\u7684\u683c\u5f0f\u3002</li> </ul> <pre><code>{\n    \"context\": \"Instruction: \u4f60\u73b0\u5728\u662f\u4e00\u4e2a\u5f88\u5389\u5bb3\u7684\u9605\u8bfb\u7406\u89e3\u5668\uff0c\u4e25\u683c\u6309\u7167\u4eba\u7c7b\u6307\u4ee4\u8fdb\u884c\u56de\u7b54\u3002\\nInput: \u4e0b\u9762\u53e5\u5b50\u53ef\u80fd\u662f\u4e00\u6761\u5173\u4e8e\u4ec0\u4e48\u7684\u8bc4\u8bba\uff0c\u7528\u5217\u8868\u5f62\u5f0f\u56de\u7b54\uff1a\\n\\n\u5f88\u4e0d\u9519\uff0c\u5f88\u65b0\u9c9c\uff0c\u5feb\u9012\u5c0f\u54e5\u670d\u52a1\u5f88\u597d\uff0c\u6c34\u679c\u4e5f\u633a\u751c\u633a\u8106\u7684\\nAnswer: \", \n    \"target\": \"[\\\"\u6c34\u679c\\\"]\"\n}\n</code></pre> <p>\u8bad\u7ec3\u96c6\u4e2d\u4e00\u5171\u5305\u542b902\u6761\u6570\u636e\uff0c\u6bcf\u4e00\u6761\u6570\u636e\u90fd\u5206\u4e3a <code>context</code> \u548c <code>target</code> \u4e24\u90e8\u5206\uff1a</p> <ol> <li><code>context</code> \u90e8\u5206\u662f\u63a5\u53d7\u7528\u6237\u7684\u8f93\u5165\u30022.<code>target</code> \u90e8\u5206\u7528\u4e8e\u6307\u5b9a\u6a21\u578b\u7684\u8f93\u51fa\u3002</li> </ol> <p>\u5728 <code>context</code> \u4e2d\u53c8\u5305\u62ec 2 \u4e2a\u90e8\u5206\uff1a</p> <ol> <li>Instruction\uff1a\u7528\u4e8e\u544a\u77e5\u6a21\u578b\u7684\u5177\u4f53\u6307\u4ee4\uff0c\u5f53\u9700\u8981\u4e00\u4e2a\u6a21\u578b\u540c\u65f6\u89e3\u51b3\u591a\u4e2a\u4efb\u52a1\u65f6\u53ef\u4ee5\u8bbe\u5b9a\u4e0d\u540c\u7684 Instruction \u6765\u5e2e\u52a9\u6a21\u578b\u5224\u522b\u5f53\u524d\u5e94\u5f53\u505a\u4ec0\u4e48\u4efb\u52a1\u3002</li> <li>Input\uff1a\u5f53\u524d\u7528\u6237\u7684\u8f93\u5165\u3002</li> </ol>"},{"location":"04-%E5%9F%BA%E4%BA%8EChatGLM%E5%BE%AE%E8%B0%83%E5%AE%9E%E7%8E%B0%E4%BF%A1%E6%81%AF%E6%8A%BD%E5%8F%96%2B%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E7%9A%84%E5%A4%9A%E4%BB%BB%E5%8A%A1%E5%AE%9E%E6%88%98/02-%E5%A4%9A%E4%BB%BB%E5%8A%A1%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86.html#12-devjsonl","title":"1.2 dev.jsonl","text":"<ul> <li> <p>mixed_dev_dataset.jsonl\u4e3a\u8bad\u7ec3\u6570\u636e\u96c6\uff0c\u56e0\u4e3a\u6211\u4eec\u672c\u6b21\u9879\u76ee\u540c\u65f6\u8fdb\u884c\u300c\u4fe1\u606f\u62bd\u53d6+\u6587\u672c\u5206\u7c7b\u300d\u4e24\u9879\u4efb\u52a1\uff0c\u56e0\u6b64\u6570\u636e\u4e2d\u6df7\u5408\u4e86\u4e24\u79cd\u4efb\u52a1\u6570\u636e\u7c7b\u578b\u3002\u4e3e\u4f8b\u5c55\u793a\u5982\u4e0b\uff1a</p> </li> <li> <p>\u4fe1\u606f\u62bd\u53d6\u6570\u636e\u793a\u4f8b</p> </li> <li>Instruction \u90e8\u5206\u544a\u8bc9\u6a21\u578b\u73b0\u5728\u9700\u8981\u505a\u300c\u9605\u8bfb\u7406\u89e3\u300d\u4efb\u52a1\uff0cInput \u90e8\u5206\u544a\u77e5\u6a21\u578b\u8981\u62bd\u53d6\u7684\u53e5\u5b50\u4ee5\u53ca\u8f93\u51fa\u7684\u683c\u5f0f\u3002</li> </ul> <pre><code>{\n    \"context\": \"Instruction: \u4f60\u73b0\u5728\u662f\u4e00\u4e2a\u5f88\u5389\u5bb3\u7684\u9605\u8bfb\u7406\u89e3\u5668\uff0c\u4e25\u683c\u6309\u7167\u4eba\u7c7b\u6307\u4ee4\u8fdb\u884c\u56de\u7b54\u3002\\nInput: \u4e0b\u9762\u53e5\u5b50\u5305\u542b\u4e86\u54ea\u4e9b\u4e09\u5143\u7ec4\uff0c\u53ea\u7528json\u7684\u683c\u5f0f\u56de\u7b54\uff1a\\n\\n\u300a\u5168\u56fd\u516c\u5171\u82f1\u8bed\u7b49\u7ea7\u8003\u8bd5\u56db\u7ea7\u8bcd\u6c47\u79d1\u5b66\u8bb0\u5fc6\uff08\u78c1\u5e261--5\uff09\u300b\u662f\u7531\u4eba\u6c11\u5927\u5b66\u51fa\u7248\u793e\u51fa\u7248\u7684\u4e00\u90e8\u6559\u80b2\u4f5c\u54c1\uff0c\u4f5c\u8005\u662f\u949f\u9053\u9686\u3002\\nAnswer: \", \n    \"target\": \"```json\\n[{\\\"predicate\\\": \\\"\u51fa\u7248\u793e\\\", \\\"object_type\\\": \\\"\u51fa\u7248\u793e\\\", \\\"subject_type\\\": \\\"\u4e66\u7c4d\\\", \\\"object\\\": \\\"\u4eba\u6c11\u5927\u5b66\\\", \\\"subject\\\": \\\"\u5168\u56fd\u516c\u5171\u82f1\u8bed\u7b49\u7ea7\u8003\u8bd5\u56db\u7ea7\u8bcd\u6c47\u79d1\u5b66\u8bb0\u5fc6\uff08\u78c1\u5e261--5\uff09\\\"}, {\\\"predicate\\\": \\\"\u4f5c\u8005\\\", \\\"object_type\\\": \\\"\u4eba\u7269\\\", \\\"subject_type\\\": \\\"\u56fe\u4e66\u4f5c\u54c1\\\", \\\"object\\\": \\\"\u949f\u9053\u9686\\\", \\\"subject\\\": \\\"\u5168\u56fd\u516c\u5171\u82f1\u8bed\u7b49\u7ea7\u8003\u8bd5\u56db\u7ea7\u8bcd\u6c47\u79d1\u5b66\u8bb0\u5fc6\uff08\u78c1\u5e261--5\uff09\\\"}]\\n```\"\n}\n</code></pre> <ul> <li>\u6587\u672c\u6570\u636e\u793a\u4f8b</li> <li>Instruction \u90e8\u5206\u544a\u8bc9\u6a21\u578b\u73b0\u5728\u9700\u8981\u505a\u300c\u9605\u8bfb\u7406\u89e3\u300d\u4efb\u52a1\uff0cInput \u90e8\u5206\u544a\u77e5\u6a21\u578b\u8981\u62bd\u53d6\u7684\u53e5\u5b50\u4ee5\u53ca\u8f93\u51fa\u7684\u683c\u5f0f\u3002</li> </ul> <pre><code>{\n    \"context\": \"Instruction: \u4f60\u73b0\u5728\u662f\u4e00\u4e2a\u5f88\u5389\u5bb3\u7684\u9605\u8bfb\u7406\u89e3\u5668\uff0c\u4e25\u683c\u6309\u7167\u4eba\u7c7b\u6307\u4ee4\u8fdb\u884c\u56de\u7b54\u3002\\nInput: \u4e0b\u9762\u53e5\u5b50\u4e2d\u63cf\u8ff0\u7684\u662f\u4e00\u4e2a\u4ec0\u4e48\uff1f\u7528\u5217\u8868\u7684\u65b9\u5f0f\u56de\u7b54\u3002\\n\\n\u4ec0\u4e48\u82f9\u679c\u554a\uff0c\u90fd\u6ca1\u6709\u82f9\u679c\u5473\uff0c\u602a\u602a\u7684\u5473\u9053\uff0c\u800c\u4e14\u4e00\u70b9\u90fd\u4e0d\u751c\uff0c\u8d85\u7ea7\u96be\u5403\uff01\\nAnswer: \", \n    \"target\": \"[\\\"\u6c34\u679c\\\"]\"\n}\n</code></pre> <p>\u8bad\u7ec3\u96c6\u4e2d\u4e00\u5171\u5305\u542b122\u6761\u6570\u636e\uff0c\u6bcf\u4e00\u6761\u6570\u636e\u90fd\u5206\u4e3a <code>context</code> \u548c <code>target</code> \u4e24\u90e8\u5206\uff1a</p> <ol> <li><code>context</code> \u90e8\u5206\u662f\u63a5\u53d7\u7528\u6237\u7684\u8f93\u5165\u30022.<code>target</code> \u90e8\u5206\u7528\u4e8e\u6307\u5b9a\u6a21\u578b\u7684\u8f93\u51fa\u3002</li> </ol> <p>\u5728 <code>context</code> \u4e2d\u53c8\u5305\u62ec 2 \u4e2a\u90e8\u5206\uff1a</p> <ol> <li>Instruction\uff1a\u7528\u4e8e\u544a\u77e5\u6a21\u578b\u7684\u5177\u4f53\u6307\u4ee4\uff0c\u5f53\u9700\u8981\u4e00\u4e2a\u6a21\u578b\u540c\u65f6\u89e3\u51b3\u591a\u4e2a\u4efb\u52a1\u65f6\u53ef\u4ee5\u8bbe\u5b9a\u4e0d\u540c\u7684 Instruction \u6765\u5e2e\u52a9\u6a21\u578b\u5224\u522b\u5f53\u524d\u5e94\u5f53\u505a\u4ec0\u4e48\u4efb\u52a1\u3002</li> <li>Input\uff1a\u5f53\u524d\u7528\u6237\u7684\u8f93\u5165\u3002</li> </ol> <p>\u5982\u679c\u60f3\u4f7f\u7528\u81ea\u5b9a\u4e49\u6570\u636e\u8bad\u7ec3\uff0c\u53ea\u9700\u8981\u4eff\u7167\u4e0a\u8ff0\u793a\u4f8b\u6570\u636e\u6784\u5efa\u6570\u636e\u96c6\u5373\u53ef\u3002</p>"},{"location":"04-%E5%9F%BA%E4%BA%8EChatGLM%E5%BE%AE%E8%B0%83%E5%AE%9E%E7%8E%B0%E4%BF%A1%E6%81%AF%E6%8A%BD%E5%8F%96%2B%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E7%9A%84%E5%A4%9A%E4%BB%BB%E5%8A%A1%E5%AE%9E%E6%88%98/02-%E5%A4%9A%E4%BB%BB%E5%8A%A1%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86.html#2-config","title":"2 \u7f16\u5199\u9879\u76eeConfig\u7c7b\u914d\u7f6e\u6587\u4ef6","text":"<ul> <li> <p>\u4ee3\u7801\u8def\u5f84\uff1allm_tuning/ptune_chatglm/glm_config.py</p> </li> <li> <p>config\u6587\u4ef6\u76ee\u7684\uff1a\u914d\u7f6e\u9879\u76ee\u5e38\u7528\u53d8\u91cf\uff0c\u4e00\u822c\u8fd9\u4e9b\u53d8\u91cf\u5c5e\u4e8e\u4e0d\u7ecf\u5e38\u6539\u53d8\u7684\uff0c\u6bd4\u5982\uff1a\u8bad\u7ec3\u6587\u4ef6\u8def\u5f84\u3001\u6a21\u578b\u8bad\u7ec3\u6b21\u6570\u3001\u6a21\u578b\u8d85\u53c2\u6570\u7b49\u7b49</p> </li> </ul> <p>\u5177\u4f53\u4ee3\u7801\u5b9e\u73b0\uff1a</p> <pre><code># -*- coding:utf-8 -*-\nimport torch\n\n\nclass ProjectConfig(object):\n    def __init__(self):\n        self.device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n        self.pre_model = './llm/ChatGLM-6B/THUDM/chatglm-6b'\n        self.train_path = './llm/ptune_chatglm/data/mixed_train_dataset.jsonl'\n        self.dev_path = './llm/ptune_chatglm/data/mixed_dev_dataset.jsonl'\n        self.use_lora = True\n        self.use_ptuning = False\n        # \u4f4e\u79e9\u77e9\u9635\u7684\u79e9\u662f8\n        self.lora_rank = 8\n        self.batch_size = 1\n        self.epochs = 2\n        self.learning_rate = 3e-5\n        self.weight_decay = 0\n        self.warmup_ratio = 0.06\n        self.max_source_seq_len = 400\n        self.max_target_seq_len = 300\n        self.logging_steps = 10\n        self.save_freq = 200\n        self.pre_seq_len = 128\n        self.prefix_projection = False # \u9ed8\u8ba4\u4e3aFalse,\u5373p-tuning,\u5982\u679c\u4e3aTrue\uff0c\u5373p-tuning-v2\n        self.save_dir = './llm/ptune_chatglm/checkpoints/ptune'\n\n\nif __name__ == '__main__':\n    pc = ProjectConfig()\n    print(pc.save_dir)\n</code></pre>"},{"location":"04-%E5%9F%BA%E4%BA%8EChatGLM%E5%BE%AE%E8%B0%83%E5%AE%9E%E7%8E%B0%E4%BF%A1%E6%81%AF%E6%8A%BD%E5%8F%96%2B%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E7%9A%84%E5%A4%9A%E4%BB%BB%E5%8A%A1%E5%AE%9E%E6%88%98/02-%E5%A4%9A%E4%BB%BB%E5%8A%A1%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86.html#3","title":"3 \u7f16\u5199\u6570\u636e\u5904\u7406\u76f8\u5173\u4ee3\u7801","text":"<ul> <li>\u4ee3\u7801\u8def\u5f84\uff1allm_tuning/ptune_chatglm/data_handle.</li> <li>data_handle\u6587\u4ef6\u5939\u4e2d\u4e00\u5171\u5305\u542b\u4e24\u4e2apy\u811a\u672c\uff1adata_preprocess.py\u3001data_loader.py</li> </ul>"},{"location":"04-%E5%9F%BA%E4%BA%8EChatGLM%E5%BE%AE%E8%B0%83%E5%AE%9E%E7%8E%B0%E4%BF%A1%E6%81%AF%E6%8A%BD%E5%8F%96%2B%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E7%9A%84%E5%A4%9A%E4%BB%BB%E5%8A%A1%E5%AE%9E%E6%88%98/02-%E5%A4%9A%E4%BB%BB%E5%8A%A1%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86.html#31-data_preprocesspy","title":"3.1 data_preprocess.py","text":"<ul> <li>\u76ee\u7684: \u5c06\u6837\u672c\u6570\u636e\u8f6c\u6362\u4e3a\u6a21\u578b\u63a5\u53d7\u7684\u8f93\u5165\u6570\u636e</li> <li>\u5bfc\u5165\u5fc5\u5907\u7684\u5de5\u5177\u5305</li> </ul> <pre><code>import json\n# \u8fd4\u56de\u7684\u5b57\u7b26\u4e32\u5305\u542b\u6709\u5173\u5f02\u5e38\u7684\u8be6\u7ec6\u4fe1\nimport traceback\nimport numpy as np\nfrom tqdm import tqdm\nfrom datasets import load_dataset\nfrom transformers import AutoTokenizer\nfrom functools import partial\nimport sys\nsys.path.append('..')\n\nfrom glm_config import *\n</code></pre> <ul> <li>\u5b9a\u4e49\u6570\u636e\u8f6c\u6362\u65b9\u6cd5convert_example()</li> </ul> <pre><code>def convert_example(\n        examples: dict,\n        tokenizer,\n        max_source_seq_len: int,\n        max_target_seq_len: int,\n    ):\n    \"\"\"\n    \u5c06\u6837\u672c\u6570\u636e\u8f6c\u6362\u4e3aPtuning\u6a21\u578b\u63a5\u6536\u7684\u8f93\u5165\u6570\u636e\u3002\n\n    Args:\n        examples (dict): \u8bad\u7ec3\u6570\u636e\u6837\u672c, e.g. -&gt; {\n                                                \"text\": [\n                                                            '{\"context\": \"\u5e74\u57fa\u51c6\u5229\u73874.35%\u3002\u4ece\u5b9e\u9645\u770b...\", \"target\": \"2017\u5e74\u94f6\u884c\u8d37\u6b3e\u57fa\u51c6\u5229\u7387\"}',\n                                                            ...\n                                                ]\n                                            }\n        max_source_seq_len (int): prompt\u6700\u5927\u957f\u5ea6\n        max_target_seq_len (int): \u7b54\u6848\u6700\u5927\u957f\u5ea6\n\n    Returns:\n        dict (str: np.array) -&gt; tokenized_output = {\n                            'input_ids': [[1525, 10, ...], [758, 2345, ...]],\n                            'labels': [[822, 10, ...], [125, 58...]]\n                        }\n    \"\"\"\n    tokenized_output = {\n        'input_ids': [],\n        'labels': []\n    }\n\n    max_seq_length = max_source_seq_len + max_target_seq_len\n\n    for example in examples['text']:\n        try:\n            example = json.loads(example)\n            context = example[\"context\"]\n            target = example[\"target\"]\n            # print(f'context--&gt;{context}')\n            # print(f'target--&gt;{target}')\n            prompts_ids = tokenizer.encode(\n                text=context,\n                add_special_tokens=False\n            )\n            # print(f'prompts_ids--\u300b{prompts_ids}\\n{len(prompts_ids)}')\n\n            target_ids = tokenizer.encode(\n                text=target,\n                add_special_tokens=False\n            )\n            # print(f'target_ids--\u300b{target_ids}\\n{len(target_ids)}')\n\n            if len(prompts_ids) &gt;= max_source_seq_len:\n                # source \u9700\u8981\u7559\u4e00\u4e2a [gMASK] token \u5728\u7ed3\u5c3e\n                prompts_ids = prompts_ids[:max_source_seq_len - 1]\n\n            if len(target_ids) &gt;= max_target_seq_len - 1: \n              # target \u9700\u8981\u7559\u4e00\u4e2a &lt;sop&gt; \u5728\u5f00\u5934\u548c\u4e00\u4e2a &lt;eop&gt; token \u5728\u7ed3\u5c3e\n                target_ids = target_ids[:max_target_seq_len - 2]\n\n                        # source_ids + [gMASK] + &lt;sop&gt; + target_ids + &lt;eop&gt;\n            input_ids = tokenizer.build_inputs_with_special_tokens(prompts_ids, target_ids)     \n            # print(f'input_ids--&gt;{input_ids}')\n\n            # bos \u5728 target \u7684\u7b2c\u4e00\u4f4d\n            context_length = input_ids.index(tokenizer.bos_token_id) \n            # print(f'context_length--&gt;{context_length}')\n                        # [gMASK] \u5728 source \u7684\u6700\u540e\u4e00\u4f4d\n            mask_position = context_length - 1\n\n            # \u4ece bos \u5f00\u59cb\u5230\u540e\u9762\u6240\u6709\u7684 target \u5230 eos \u90fd\u4e3a label\n            labels = [-100] * context_length + input_ids[mask_position + 1:]                    \n            # print(f'labels--&gt;{labels}')\n\n            pad_len = max_seq_length - len(input_ids)\n            # print(f'pad_len--&gt;{pad_len}')\n\n            input_ids = input_ids + [tokenizer.pad_token_id] * pad_len\n            # print(f'input_ids--&gt;{input_ids}\\n{len(input_ids)}')\n            labels = labels + [-100] * pad_len\n            # print(f'labels--&gt;{labels}\\n{len(labels)}')\n\n\n            tokenized_output['input_ids'].append(input_ids)\n            tokenized_output['labels'].append(labels)\n        except:\n            print(f'\"{example}\" -&gt; {traceback.format_exc()}')\n            continue\n\n    for k, v in tokenized_output.items():\n        tokenized_output[k] = np.array(v)\n\n    return tokenized_output\n</code></pre> <ul> <li>\u5b9a\u4e49\u83b7\u53d6\u8bad\u7ec3\u6216\u9a8c\u8bc1\u6570\u636e\u6700\u5927\u957f\u5ea6\u65b9\u6cd5get_max_length()</li> </ul> <pre><code>def get_max_length(\n        tokenizer,\n        dataset_file: str\n    ):\n    \"\"\"\n    \u6d4b\u8bd5\u6570\u636e\u96c6\u6700\u5927\u7684\u8f93\u5165/\u8f93\u51fatokens\u662f\u591a\u5c11\u3002\n\n    Args:\n        dataset_file (str): _description_\n    \"\"\"\n    source_seq_len_list = []\n    target_seq_len_list = []\n    with open(dataset_file, 'r') as f:\n        for line in tqdm(f.readlines()):\n            line = json.loads(line)\n\n            source_len = tokenizer.encode(line['context'])\n            source_seq_len_list.append(len(source_len))\n\n            target_len = tokenizer.encode(line['target'])\n            target_seq_len_list.append(len(target_len))\n\n    print(dataset_file)\n    print(f\"\u3010Source Sequence\u3011 Max: {max(source_seq_len_list)}, Avg: {int(sum(source_seq_len_list) / len(source_seq_len_list))}, Middle: {sorted(source_seq_len_list)[int(len(source_seq_len_list) / 2)]}.\")\n    print(f\"\u3010Target Sequence\u3011 Max: {max(target_seq_len_list)}, Avg: {int(sum(target_seq_len_list) / len(target_seq_len_list))}, Middle: {sorted(target_seq_len_list)[int(len(target_seq_len_list) / 2)]}.\")\n</code></pre>"},{"location":"04-%E5%9F%BA%E4%BA%8EChatGLM%E5%BE%AE%E8%B0%83%E5%AE%9E%E7%8E%B0%E4%BF%A1%E6%81%AF%E6%8A%BD%E5%8F%96%2B%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E7%9A%84%E5%A4%9A%E4%BB%BB%E5%8A%A1%E5%AE%9E%E6%88%98/02-%E5%A4%9A%E4%BB%BB%E5%8A%A1%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86.html#33-data_loaderpy","title":"3.3 data_loader.py","text":"<ul> <li>\u76ee\u7684\uff1a\u5b9a\u4e49\u6570\u636e\u52a0\u8f7d\u5668</li> <li>\u5bfc\u5165\u5fc5\u5907\u7684\u5de5\u5177\u5305</li> </ul> <pre><code># coding:utf-8\nfrom torch.utils.data import DataLoader\nfrom transformers import default_data_collator, AutoTokenizer\nfrom data_handle.data_preprocess import *\nfrom glm_config import *\n\npc = ProjectConfig() # \u5b9e\u4f8b\u5316\u9879\u76ee\u914d\u7f6e\u6587\u4ef6\n\ntokenizer = AutoTokenizer.from_pretrained(pc.pre_model, trust_remote_code=True)\n</code></pre> <ul> <li>\u5b9a\u4e49\u83b7\u53d6\u6570\u636e\u52a0\u8f7d\u5668\u7684\u65b9\u6cd5get_data()</li> </ul> <pre><code>def get_data():\n    dataset = load_dataset('text', data_files={'train': pc.train_path,\n                                               'dev': pc.dev_path})\n\n\n    new_func = partial(convert_example,\n                       tokenizer=tokenizer,\n                       max_source_seq_len=100,\n                       max_target_seq_len=100)\n\n    dataset = dataset.map(new_func, batched=True)\n    train_dataset = dataset[\"train\"]\n    dev_dataset = dataset[\"dev\"]\n    train_dataloader = DataLoader(train_dataset,\n                                  shuffle=True,\n                                  collate_fn=default_data_collator,\n                                  batch_size=pc.batch_size)\n    dev_dataloader = DataLoader(dev_dataset,\n                                collate_fn=default_data_collator,\n                                batch_size=pc.batch_size)\n    return train_dataloader, dev_dataloader\nif __name__ == '__main__':\n    train_dataloader, dev_dataloader = get_data()\n    print(len(train_dataloader))\n    print(len(dev_dataloader))\n    for i, value in enumerate(train_dataloader):\n        print(value)\n        print(value['input_ids'].shape)\n        print(value['labels'].shape)\n        break\n</code></pre> <ul> <li>\u6253\u5370\u7ed3\u679c\uff1a</li> </ul> <pre><code>902\n122\n{\n    'input_ids': tensor([[ 37010,     12,      5,  76331,  83362,  92831, \n103593,  64464,      6,\n          77115,  65077,  72863,  63891,  66207,  63823,      4,   3430,     12,\n          68327,  74351,  77756,  66263,  81577,  64536,      6,  82145,   2031,\n          63825,  69574,  66207,     12,      4,      4,  64590,  67748,  69958,\n          66152,  63923,  65024,  64676,  65102,  66089,  64101,  73127,  64025,\n          64236,      6,  72996,  73518,  64236,  82273,  63823,      4,  13049,\n             12, 130001, 130004,      5, 125827,   2031,      4, 127903,  38861,\n             83,     28,  66845,  67541,     57,     28,   1932,     24,    317,\n             83,     28,  64069,     57,     28,   9832,     24,    317,     83,\n             28,  65210,     57,     28,   1932,     83,     28,  73127,  64025,\n          64236,     57,     28,   9832,     83,     28,  64590,  67748,  69958,\n          66152, 127731,      4, 125827, 130005,      3,      3,      3,      3,\n              3,      3,      3,      3,      3,      3,      3,      3,      3,\n              3,      3,      3,      3,      3,      3,      3,      3,      3,\n              3,      3,      3,      3,      3,      3,      3,      3,      3,\n              3,      3,      3,      3,      3,      3,      3,      3,      3,\n              3,      3,      3,      3,      3,      3,      3,      3,      3,\n              3,      3,      3,      3,      3,      3,      3,      3,      3,\n              3,      3,      3,      3,      3,      3,      3,      3,      3,\n              3,      3,      3,      3,      3,      3,      3,      3,      3,\n              3,      3,      3,      3,      3,      3,      3,      3,      3,\n              3,      3,      3,      3,      3,      3,      3,      3,      3,\n              3,      3]]),\n    'labels': tensor([[  -100,   -100,   -100,   -100,   -100,   -100,   -100,  \n-100,   -100,\n           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n           -100,   -100, 130004,      5, 125827,   2031,      4, 127903,  38861,\n             83,     28,  66845,  67541,     57,     28,   1932,     24,    317,\n             83,     28,  64069,     57,     28,   9832,     24,    317,     83,\n             28,  65210,     57,     28,   1932,     83,     28,  73127,  64025,\n          64236,     57,     28,   9832,     83,     28,  64590,  67748,  69958,\n          66152, 127731,      4, 125827, 130005,   -100,   -100,   -100,   -100,\n           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n           -100,   -100]])\n}\ntorch.Size([1, 200])\ntorch.Size([1, 200])\n</code></pre>"},{"location":"04-%E5%9F%BA%E4%BA%8EChatGLM%E5%BE%AE%E8%B0%83%E5%AE%9E%E7%8E%B0%E4%BF%A1%E6%81%AF%E6%8A%BD%E5%8F%96%2B%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E7%9A%84%E5%A4%9A%E4%BB%BB%E5%8A%A1%E5%AE%9E%E6%88%98/03-LoRA%E6%96%B9%E5%BC%8F%E6%A8%A1%E5%9E%8B%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0%E4%B8%8E%E8%AE%AD%E7%BB%83.html","title":"4.3 LoRA\u65b9\u5f0f\u5fae\u8c03ChatGLM\u6a21\u578b\u4ee3\u7801\u5b9e\u73b0\u548c\u8bad\u7ec3","text":""},{"location":"04-%E5%9F%BA%E4%BA%8EChatGLM%E5%BE%AE%E8%B0%83%E5%AE%9E%E7%8E%B0%E4%BF%A1%E6%81%AF%E6%8A%BD%E5%8F%96%2B%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E7%9A%84%E5%A4%9A%E4%BB%BB%E5%8A%A1%E5%AE%9E%E6%88%98/03-LoRA%E6%96%B9%E5%BC%8F%E6%A8%A1%E5%9E%8B%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0%E4%B8%8E%E8%AE%AD%E7%BB%83.html#chatglmlora","title":"\u57fa\u4e8eChatGLM+LoRA\u65b9\u5f0f\u6a21\u578b\u642d\u5efa","text":""},{"location":"04-%E5%9F%BA%E4%BA%8EChatGLM%E5%BE%AE%E8%B0%83%E5%AE%9E%E7%8E%B0%E4%BF%A1%E6%81%AF%E6%8A%BD%E5%8F%96%2B%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E7%9A%84%E5%A4%9A%E4%BB%BB%E5%8A%A1%E5%AE%9E%E6%88%98/03-LoRA%E6%96%B9%E5%BC%8F%E6%A8%A1%E5%9E%8B%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0%E4%B8%8E%E8%AE%AD%E7%BB%83.html#_1","title":"\u5b66\u4e60\u76ee\u6807","text":"<ul> <li>\u638c\u63e1\u57fa\u4e8eChatGLM+LoRA\u65b9\u5f0f\u6a21\u578b\u642d\u5efa\u4ee3\u7801\u7684\u5b9e\u73b0.</li> <li>\u638c\u63e1\u6a21\u578b\u7684\u8bad\u7ec3,\u9a8c\u8bc1\u53ca\u76f8\u5173\u5de5\u5177\u4ee3\u7801\u7684\u5b9e\u73b0.</li> <li>\u638c\u63e1\u4f7f\u7528\u6a21\u578b\u9884\u6d4b\u4ee3\u7801\u7684\u5b9e\u73b0.</li> </ul>"},{"location":"04-%E5%9F%BA%E4%BA%8EChatGLM%E5%BE%AE%E8%B0%83%E5%AE%9E%E7%8E%B0%E4%BF%A1%E6%81%AF%E6%8A%BD%E5%8F%96%2B%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E7%9A%84%E5%A4%9A%E4%BB%BB%E5%8A%A1%E5%AE%9E%E6%88%98/03-LoRA%E6%96%B9%E5%BC%8F%E6%A8%A1%E5%9E%8B%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0%E4%B8%8E%E8%AE%AD%E7%BB%83.html#_2","title":"\u6a21\u578b\u642d\u5efa","text":"<ul> <li>\u672c\u9879\u76ee\u4e2d\u5b8c\u6210ChatGLM+LoRA\u6a21\u578b\u642d\u5efa\u3001\u8bad\u7ec3\u53ca\u5e94\u7528\u7684\u6b65\u9aa4\u5982\u4e0b\uff08\u6ce8\u610f\uff1a\u56e0\u4e3a\u672c\u9879\u76ee\u4e2d\u4f7f\u7528\u7684\u662fChatGLM\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u6240\u4ee5\u76f4\u63a5\u52a0\u8f7d\u5373\u53ef\uff0c\u65e0\u9700\u91cd\u590d\u642d\u5efa\u6a21\u578b\u67b6\u6784\uff09:</li> <li>\u4e00\u3001\u5b9e\u73b0\u6a21\u578b\u5de5\u5177\u7c7b\u51fd\u6570</li> <li>\u4e8c\u3001\u5b9e\u73b0\u6a21\u578b\u8bad\u7ec3\u51fd\u6570,\u9a8c\u8bc1\u51fd\u6570</li> <li>\u4e09\u3001\u5b9e\u73b0\u6a21\u578b\u9884\u6d4b\u51fd\u6570</li> </ul>"},{"location":"04-%E5%9F%BA%E4%BA%8EChatGLM%E5%BE%AE%E8%B0%83%E5%AE%9E%E7%8E%B0%E4%BF%A1%E6%81%AF%E6%8A%BD%E5%8F%96%2B%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E7%9A%84%E5%A4%9A%E4%BB%BB%E5%8A%A1%E5%AE%9E%E6%88%98/03-LoRA%E6%96%B9%E5%BC%8F%E6%A8%A1%E5%9E%8B%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0%E4%B8%8E%E8%AE%AD%E7%BB%83.html#_3","title":"\u4e00\u3001\u5b9e\u73b0\u6a21\u578b\u5de5\u5177\u7c7b\u51fd\u6570","text":"<ul> <li>\u76ee\u7684\uff1a\u6a21\u578b\u5728\u8bad\u7ec3\u3001\u9a8c\u8bc1\u3001\u9884\u6d4b\u65f6\u9700\u8981\u7684\u51fd\u6570</li> <li>\u4ee3\u7801\u8def\u5f84\uff1a/Users/**/PycharmProjects/llm/ptune_chatglm/utils</li> <li>utils\u6587\u4ef6\u5939\u5171\u5305\u542b1\u4e2apy\u811a\u672c\uff1acommon_utils.py</li> </ul>"},{"location":"04-%E5%9F%BA%E4%BA%8EChatGLM%E5%BE%AE%E8%B0%83%E5%AE%9E%E7%8E%B0%E4%BF%A1%E6%81%AF%E6%8A%BD%E5%8F%96%2B%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E7%9A%84%E5%A4%9A%E4%BB%BB%E5%8A%A1%E5%AE%9E%E6%88%98/03-LoRA%E6%96%B9%E5%BC%8F%E6%A8%A1%E5%9E%8B%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0%E4%B8%8E%E8%AE%AD%E7%BB%83.html#1-common_utilspy","title":"1. common_utils.py","text":"<ul> <li>\u76ee\u7684\uff1a\u5b9a\u4e49\u6570\u636e\u7c7b\u578b\u8f6c\u6362\u7c7b\u3001\u5206\u79d2\u65f6\u4e4b\u95f4\u8f6c\u6362\u4ee5\u53ca\u6a21\u578b\u4fdd\u5b58\u51fd\u6570\u3002</li> <li>\u811a\u672c\u91cc\u9762\u5305\u542b\u4e00\u4e2a\u7c7b\u4ee5\u53ca\u4e24\u4e2a\u51fd\u6570\uff1aCastOutputToFloat\u3001second2time()\u4ee5\u53casave_model()</li> <li>\u5bfc\u5165\u5fc5\u5907\u7684\u5de5\u5177\u5305\uff1a</li> </ul> <pre><code># coding:utf-8\n# \u5bfc\u5165\u5fc5\u5907\u5de5\u5177\u5305\nimport torch\nimport torch.nn as nn\nfrom glm_config import *\nimport copy\npc = ProjectConfig()\n</code></pre> <ul> <li>\u5b9a\u4e49CastOutputToFloat\u7c7b</li> </ul> <pre><code>class CastOutputToFloat(nn.Sequential):\n    def forward(self, x):\n        return super().forward(x).to(torch.float32)\n</code></pre> <ul> <li>\u5b9a\u4e49second2time()\u51fd\u6570</li> </ul> <pre><code>def second2time(seconds: int):\n    \"\"\"\n    \u5c06\u79d2\u8f6c\u6362\u6210\u65f6\u5206\u79d2\u3002\n\n    Args:\n        seconds (int): _description_\n    \"\"\"\n    m, s = divmod(seconds, 60)\n    h, m = divmod(m, 60)\n    return \"%02d:%02d:%02d\" % (h, m, s)\n</code></pre> <ul> <li>\u5b9a\u4e49save_model()</li> </ul> <pre><code>def save_model(\n        model,\n        cur_save_dir: str\n    ):\n    \"\"\"\n    \u5b58\u50a8\u5f53\u524d\u6a21\u578b\u3002\n\n    Args:\n        cur_save_path (str): \u5b58\u50a8\u8def\u5f84\u3002\n    \"\"\"\n    if pc.use_lora:   \n        # merge lora params with origin model\n        merged_model = copy.deepcopy(model)\n        merged_model = merged_model.merge_and_unload()\n        merged_model.save_pretrained(cur_save_dir)\n    else:\n        model.save_pretrained(cur_save_dir)\n</code></pre>"},{"location":"04-%E5%9F%BA%E4%BA%8EChatGLM%E5%BE%AE%E8%B0%83%E5%AE%9E%E7%8E%B0%E4%BF%A1%E6%81%AF%E6%8A%BD%E5%8F%96%2B%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E7%9A%84%E5%A4%9A%E4%BB%BB%E5%8A%A1%E5%AE%9E%E6%88%98/03-LoRA%E6%96%B9%E5%BC%8F%E6%A8%A1%E5%9E%8B%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0%E4%B8%8E%E8%AE%AD%E7%BB%83.html#_4","title":"\u4e8c\u3001\u5b9e\u73b0\u6a21\u578b\u8bad\u7ec3\u51fd\u6570,\u9a8c\u8bc1\u51fd\u6570","text":"<ul> <li>\u76ee\u7684\uff1a\u5b9e\u73b0\u6a21\u578b\u7684\u8bad\u7ec3\u548c\u9a8c\u8bc1</li> <li>\u4ee3\u7801\u8def\u5f84\uff1a/Users/**/PycharmProjects/llm/ptune_chatglm/train.py</li> <li>\u811a\u672c\u91cc\u9762\u5305\u542b\u4e24\u4e2a\u51fd\u6570\uff1amodel2train()\u548cevaluate_model()</li> <li>\u5bfc\u5165\u5fc5\u5907\u7684\u5de5\u5177\u5305</li> </ul> <pre><code>import os\nimport time\nimport copy\nimport argparse\nfrom functools import partial\nimport peft\n# autocast\u662fPyTorch\u4e2d\u4e00\u79cd\u6df7\u5408\u7cbe\u5ea6\u7684\u6280\u672f\uff0c\u53ef\u5728\u4fdd\u6301\u6570\u503c\u7cbe\u5ea6\u7684\u60c5\u51b5\u4e0b\u63d0\u9ad8\u8bad\u7ec3\u901f\u5ea6\u548c\u51cf\u5c11\u663e\u5b58\u5360\u7528\u3002\n# \u8be5\u65b9\u6cd5\u6df7\u5408\u7cbe\u5ea6\u8bad\u7ec3\uff0c\u5982\u679c\u5728CPU\u73af\u5883\u4e2d\u4e0d\u8d77\u4efb\u4f55\u4f5c\u7528\nfrom torch.cuda.amp import autocast as autocast\nfrom transformers import AutoTokenizer, AutoConfig, AutoModel, get_scheduler\nfrom utils.common_utils import *\nfrom data_handle.data_loader import *\nfrom glm_config import *\n\npc = ProjectConfig()\n</code></pre> <ul> <li>\u5b9a\u4e49model2train()\u51fd\u6570</li> </ul> <pre><code>def model2train():\n    tokenizer = AutoTokenizer.from_pretrained(pc.pre_model, trust_remote_code=True)\n\n    config = AutoConfig.from_pretrained(pc.pre_model, trust_remote_code=True)\n\n    if pc.use_ptuning:\n        config.pre_seq_len = pc.pre_seq_len\n        config.prefix_projection = pc.prefix_projection\n    model = AutoModel.from_pretrained(pc.pre_model,\n                                      config=config,\n                                      trust_remote_code=True)\n\n    #model.half()\u5c06\u6a21\u578b\u6570\u636e\u7c7b\u578b\u4ece\u9ed8\u8ba4\u7684float32\u7cbe\u5ea6\u8f6c\u6362\u4e3a\u66f4\u4f4e\u7684float16\u7cbe\u5ea6\uff0c\u51cf\u5c11\u5185\u5b58\n    model = model.float()\n    # \u68af\u5ea6\u68c0\u67e5\u70b9\u662f\u4e00\u79cd\u4f18\u5316\u6280\u672f\uff0c\u7528\u4e8e\u5728\u53cd\u5411\u4f20\u64ad\u8fc7\u7a0b\u4e2d\u964d\u4f4e\u5185\u5b58\u4f7f\u7528\n    model.gradient_checkpointing_enable()\n    model.enable_input_require_grads()\n    # \u4e0d\u8fdb\u884c\u7f13\u5b58\uff0c\u51cf\u5c11\u5185\u5b58\n    model.config.use_cache = False\n    if pc.use_ptuning:\n        model.transformer.prefix_encoder.float()\n    if pc.use_lora:\n        model.lm_head = CastOutputToFloat(model.lm_head)\n        peft_config = peft.LoraConfig(\n            task_type=peft.TaskType.CAUSAL_LM,\n            inference_mode=False, # \u63a8\u7406\u65f6\u4e3aTrue\uff0c\u6bd4\u5982\u7edd\u5b9a\u662f\u5426\u4f7f\u7528dropout\n            r=pc.lora_rank, # \u4f4e\u79e9\u77e9\u9635\u7ef4\u5ea6\n            lora_alpha=32, # \u7f29\u653e\u7cfb\u6570\n            lora_dropout=0.1,\n        )\n        model = peft.get_peft_model(model, peft_config)\n\n    model = model.to(pc.device)\n\n\n    no_decay = [\"bias\", \"LayerNorm.weight\"]\n    optimizer_grouped_parameters = [\n        {\n            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n            \"weight_decay\": pc.weight_decay,\n        },\n        {\n            \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n            \"weight_decay\": 0.0,\n        },\n    ]\n    optimizer = torch.optim.AdamW(optimizer_grouped_parameters, lr=pc.learning_rate)\n    # model.to(pc.device)\n    #\n    train_dataloader, dev_dataloader = get_data()\n    # \u6839\u636e\u8bad\u7ec3\u8f6e\u6570\u8ba1\u7b97\u6700\u5927\u8bad\u7ec3\u6b65\u6570\uff0c\u4ee5\u4fbf\u4e8escheduler\u52a8\u6001\u8c03\u6574lr\n    num_update_steps_per_epoch = len(train_dataloader)\n    #\u6307\u5b9a\u603b\u7684\u8bad\u7ec3\u6b65\u6570\uff0c\u5b83\u4f1a\u88ab\u5b66\u4e60\u7387\u8c03\u5ea6\u5668\u7528\u6765\u786e\u5b9a\u5b66\u4e60\u7387\u7684\u53d8\u5316\u89c4\u5f8b\uff0c\u786e\u4fdd\u5b66\u4e60\u7387\u5728\u6574\u4e2a\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u5f97\u4ee5\u5408\u7406\u5730\u8c03\u8282\n    max_train_steps = pc.epochs * num_update_steps_per_epoch\n    warm_steps = int(pc.warmup_ratio * max_train_steps) # \u9884\u70ed\u9636\u6bb5\u7684\u8bad\u7ec3\u6b65\u6570\n    lr_scheduler = get_scheduler(\n        name='linear',\n        optimizer=optimizer,\n        num_warmup_steps=warm_steps,\n        num_training_steps=max_train_steps,\n    )\n    #\n    loss_list = []\n    tic_train = time.time()\n    global_step, best_eval_loss = 0, float('inf')\n    for epoch in range(1, pc.epochs + 1):\n        for batch in train_dataloader:\n            if pc.use_lora:\n                with autocast():\n                    loss = model(\n                        input_ids=batch['input_ids'].to(dtype=torch.long, device=pc.device),\n                        labels=batch['labels'].to(dtype=torch.long, device=pc.device)\n                    ).loss\n            else:\n                loss = model(\n                    input_ids=batch['input_ids'].to(dtype=torch.long, device=pc.device),\n                    labels=batch['labels'].to(dtype=torch.long, device=pc.device)\n                ).loss\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            lr_scheduler.step()\n\n            loss_list.append(float(loss.cpu().detach()))\n\n            global_step += 1\n            if global_step % pc.logging_steps == 0:\n                time_diff = time.time() - tic_train\n                loss_avg = sum(loss_list) / len(loss_list)\n                print(\"global step %d ( %02.2f%% ) , epoch: %d, loss: %.5f, speed: %.2f step/s, ETA: %s\"% (global_step,\n                   global_step / max_train_steps * 100,\n                   epoch,\n                   loss_avg,\n                   pc.logging_steps / time_diff,\n                   second2time(int(max_train_steps - global_step) / (pc.logging_steps / time_diff))))\n                tic_train = time.time()\n\n            if global_step % pc.save_freq == 0:\n                cur_save_dir = os.path.join(pc.save_dir, \"model_%d\" % global_step)\n                save_model(model, cur_save_dir)\n                tokenizer.save_pretrained(cur_save_dir)\n                print(f'Model has saved at {cur_save_dir}.')\n\n                eval_loss = evaluate_model(model, dev_dataloader)\n\n                print(\"Evaluation Loss: %.5f\" % (eval_loss))\n                if eval_loss &lt; best_eval_loss:\n                    print(f\"Min eval loss has been updated: {best_eval_loss:.5f} --&gt; {eval_loss:.5f}\")\n                    best_eval_loss = eval_loss\n                    cur_save_dir = os.path.join(pc.save_dir, \"model_best\")\n                    save_model(model, cur_save_dir)\n                    tokenizer.save_pretrained(cur_save_dir)\n                    print(f'Best model has saved at {cur_save_dir}.')\n                tic_train = time.time()\n</code></pre> <ul> <li>\u5b9a\u4e49evaluate_model()\u51fd\u6570</li> </ul> <pre><code>def evaluate_model(model, dev_dataloader):\n    \"\"\"\n    \u5728\u6d4b\u8bd5\u96c6\u4e0a\u8bc4\u4f30\u5f53\u524d\u6a21\u578b\u7684\u8bad\u7ec3\u6548\u679c\u3002\n\n    Args:\n        model: \u5f53\u524d\u6a21\u578b\n        data_loader: \u6d4b\u8bd5\u96c6\u7684dataloader\n    \"\"\"\n    model.eval()\n    loss_list = []\n    with torch.no_grad():\n        for batch in dev_dataloader:\n            if pc.use_lora:\n                with autocast():\n                    loss = model(\n                        input_ids=batch['input_ids'].to(dtype=torch.long, device=pc.device),\n                        labels=batch['labels'].to(dtype=torch.long, device=pc.device)\n                    ).loss\n            else:\n                loss = model(\n                    input_ids=batch['input_ids'].to(dtype=torch.long, device=pc.device),\n                    labels=batch['labels'].to(dtype=torch.long, device=pc.device)\n                ).loss\n            loss_list.append(float(loss.cpu().detach()))\n    model.train()\n    return sum(loss_list) / len(loss_list)\n</code></pre> <ul> <li>\u8c03\u7528:</li> </ul> <pre><code>cd /Users/**/PycharmProjects/llm/ptune_chatglm\n# \u5b9e\u73b0\u6a21\u578b\u8bad\u7ec3\npython train.py\n</code></pre> <ul> <li>\u8f93\u51fa\u7ed3\u679c:</li> </ul>"},{"location":"04-%E5%9F%BA%E4%BA%8EChatGLM%E5%BE%AE%E8%B0%83%E5%AE%9E%E7%8E%B0%E4%BF%A1%E6%81%AF%E6%8A%BD%E5%8F%96%2B%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E7%9A%84%E5%A4%9A%E4%BB%BB%E5%8A%A1%E5%AE%9E%E6%88%98/03-LoRA%E6%96%B9%E5%BC%8F%E6%A8%A1%E5%9E%8B%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0%E4%B8%8E%E8%AE%AD%E7%BB%83.html#_5","title":"\u4e09\u3001\u5b9e\u73b0\u6a21\u578b\u9884\u6d4b\u51fd\u6570","text":"<ul> <li>\u76ee\u7684\uff1a\u52a0\u8f7d\u8bad\u7ec3\u597d\u7684\u6a21\u578b\u5e76\u6d4b\u8bd5\u6548\u679c</li> <li>\u4ee3\u7801\u8def\u5f84\uff1a/Users/**/PycharmProjects/llm/prompt_tasks/ptune_chatglm/inference.py</li> <li>\u5bfc\u5165\u5fc5\u5907\u7684\u5de5\u5177\u5305</li> </ul> <pre><code>import time\nimport torch\n\nfrom transformers import AutoTokenizer, AutoModel\n# torch.set_default_tensor_type(torch.cuda.HalfTensor)\n</code></pre> <ul> <li>\u9884\u6d4b\u4ee3\u7801\u5177\u4f53\u5b9e\u73b0</li> </ul> <pre><code>def inference(\n        model,\n        tokenizer,\n        instuction: str,\n        sentence: str\n    ):\n    \"\"\"\n    \u6a21\u578b inference \u51fd\u6570\u3002\n\n    Args:\n        instuction (str): _description_\n        sentence (str): _description_\n\n    Returns:\n        _type_: _description_\n    \"\"\"\n    with torch.no_grad():\n        input_text = f\"Instruction: {instuction}\\n\"\n        if sentence:\n            input_text += f\"Input: {sentence}\\n\"\n        input_text += f\"Answer: \"\n        batch = tokenizer(input_text, return_tensors=\"pt\")\n        out = model.generate(\n            input_ids=batch[\"input_ids\"].to(device),\n            max_new_tokens=max_new_tokens,\n            temperature=0\n        )\n        out_text = tokenizer.decode(out[0])\n        answer = out_text.split('Answer: ')[-1]\n        return answer\n\n\nif __name__ == '__main__':\n    from rich import print\n\n    device = 'mps:0'\n    max_new_tokens = 300\n    model_path = \"./llm/ptune_chatglm/checkpoints/model_1800\"\n    tokenizer = AutoTokenizer.from_pretrained(\n        model_path,\n        trust_remote_code=True\n    )\n\n    model = AutoModel.from_pretrained(\n        model_path,\n        trust_remote_code=True\n    ).half().to(device)\n\n    samples = [\n        {\n            'instruction': \"\u73b0\u5728\u4f60\u662f\u4e00\u4e2a\u975e\u5e38\u5389\u5bb3\u7684SPO\u62bd\u53d6\u5668\u3002\",\n            \"input\": \"\u4e0b\u9762\u8fd9\u53e5\u4e2d\u5305\u542b\u4e86\u54ea\u4e9b\u4e09\u5143\u7ec4\uff0c\u7528json\u5217\u8868\u7684\u5f62\u5f0f\u56de\u7b54\uff0c\u4e0d\u8981\u8f93\u51fa\u9664json\u5916\u7684\u5176\u4ed6\u7b54\u6848\u3002\\n\\n73\u83b7\u5956\u8bb0\u5f55\u4eba\u7269\u8bc4\u4ef7\uff1a\u9ec4\u78ca\u662f\u4e00\u4e2a\u7279\u522b\u5e78\u8fd0\u7684\u6f14\u5458\uff0c\u62cd\u7b2c\u4e00\u90e8\u620f\u5c31\u78b0\u5230\u4e86\u5bfc\u6f14\u9648\u51ef\u6b4c\uff0c\u800c\u4e14\u5728\u4ed6\u7684\u4e0b\u4e00\u90e8\u7535\u5f71\u300a\u591c\u534a\u6b4c\u58f0\u300b\u4e2d\u6f14\u5bf9\u624b\u620f\u7684\u5f20\u56fd\u8363\u3001\u5434\u5029\u83b2\u3001\u9ece\u660e\u7b49\u90fd\u662f\u8457\u540d\u7684\u6e2f\u53f0\u6f14\u5458\u3002\",\n        },\n        {\n            'instruction': \"\u4f60\u73b0\u5728\u662f\u4e00\u4e2a\u5f88\u5389\u5bb3\u7684\u9605\u8bfb\u7406\u89e3\u5668\uff0c\u4e25\u683c\u6309\u7167\u4eba\u7c7b\u6307\u4ee4\u8fdb\u884c\u56de\u7b54\u3002\",\n            \"input\": \"\u4e0b\u9762\u5b50\u4e2d\u7684\u4e3b\u8bed\u662f\u4ec0\u4e48\u7c7b\u522b\uff0c\u8f93\u51fa\u6210\u5217\u8868\u5f62\u5f0f\u3002\\n\\n\u7b2cN\u6b21\u5165\u4f4f\u4e86\uff0c\u5c31\u662f\u65b9\u4fbf\u53bb\u5ba2\u6237\u90a3\u91cc\u54c8\u54c8\u3002\u8fd8\u6709\u5565\u8bf4\u7684\"\n        }\n    ]\n\n    start = time.time()\n    for i, sample in enumerate(samples):\n        res = inference(\n            model,\n            tokenizer,\n            sample['instruction'],\n            sample['input']\n        )\n        print(f'res {i}: ')\n        print(res)\n    print(f'Used {round(time.time() - start, 2)}s.')\n</code></pre> <ul> <li>\u7ed3\u679c\u5c55\u793a</li> </ul>"},{"location":"04-%E5%9F%BA%E4%BA%8EChatGLM%E5%BE%AE%E8%B0%83%E5%AE%9E%E7%8E%B0%E4%BF%A1%E6%81%AF%E6%8A%BD%E5%8F%96%2B%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E7%9A%84%E5%A4%9A%E4%BB%BB%E5%8A%A1%E5%AE%9E%E6%88%98/03-LoRA%E6%96%B9%E5%BC%8F%E6%A8%A1%E5%9E%8B%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0%E4%B8%8E%E8%AE%AD%E7%BB%83.html#_6","title":"\u5c0f\u8282\u603b\u7ed3","text":"<ul> <li>\u672c\u5c0f\u8282\u5b9e\u73b0\u4e86\u57fa\u4e8eBERT+PET\u6a21\u578b\u7684\u6784\u5efa, \u5e76\u5b8c\u6210\u4e86\u8bad\u7ec3\u548c\u6d4b\u8bd5\u8bc4\u4f30.</li> </ul>"},{"location":"04-%E5%9F%BA%E4%BA%8EChatGLM%E5%BE%AE%E8%B0%83%E5%AE%9E%E7%8E%B0%E4%BF%A1%E6%81%AF%E6%8A%BD%E5%8F%96%2B%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E7%9A%84%E5%A4%9A%E4%BB%BB%E5%8A%A1%E5%AE%9E%E6%88%98/%E8%B6%8B%E5%8A%A8%E4%BA%91%E4%BD%BF%E7%94%A8%E3%80%8A%E8%A1%A5%E5%85%85%E3%80%8B.html","title":"4.4 \u8d8b\u52a8\u4e91\u4f7f\u7528\u300a\u6269\u5c55\u300b","text":""},{"location":"04-%E5%9F%BA%E4%BA%8EChatGLM%E5%BE%AE%E8%B0%83%E5%AE%9E%E7%8E%B0%E4%BF%A1%E6%81%AF%E6%8A%BD%E5%8F%96%2B%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E7%9A%84%E5%A4%9A%E4%BB%BB%E5%8A%A1%E5%AE%9E%E6%88%98/%E8%B6%8B%E5%8A%A8%E4%BA%91%E4%BD%BF%E7%94%A8%E3%80%8A%E8%A1%A5%E5%85%85%E3%80%8B.html#_1","title":"\u8d8b\u52a8\u4e91\u4f7f\u7528\u300a\u8865\u5145\u300b","text":""},{"location":"04-%E5%9F%BA%E4%BA%8EChatGLM%E5%BE%AE%E8%B0%83%E5%AE%9E%E7%8E%B0%E4%BF%A1%E6%81%AF%E6%8A%BD%E5%8F%96%2B%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E7%9A%84%E5%A4%9A%E4%BB%BB%E5%8A%A1%E5%AE%9E%E6%88%98/%E8%B6%8B%E5%8A%A8%E4%BA%91%E4%BD%BF%E7%94%A8%E3%80%8A%E8%A1%A5%E5%85%85%E3%80%8B.html#_2","title":"\u4e00\u3001\u6ce8\u518c\u767b\u9646","text":""},{"location":"04-%E5%9F%BA%E4%BA%8EChatGLM%E5%BE%AE%E8%B0%83%E5%AE%9E%E7%8E%B0%E4%BF%A1%E6%81%AF%E6%8A%BD%E5%8F%96%2B%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E7%9A%84%E5%A4%9A%E4%BB%BB%E5%8A%A1%E5%AE%9E%E6%88%98/%E8%B6%8B%E5%8A%A8%E4%BA%91%E4%BD%BF%E7%94%A8%E3%80%8A%E8%A1%A5%E5%85%85%E3%80%8B.html#11","title":"1.1 \u8fdb\u5165\u5b98\u7f51\u5730\u5740","text":"<ul> <li>\u7f51\u5740\uff1a\uff01https://platform.virtaicloud.com/gemini_web/auth/register?inviteCode=465f84d8d7d6bf365123d655a31accda</li> </ul>"},{"location":"04-%E5%9F%BA%E4%BA%8EChatGLM%E5%BE%AE%E8%B0%83%E5%AE%9E%E7%8E%B0%E4%BF%A1%E6%81%AF%E6%8A%BD%E5%8F%96%2B%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E7%9A%84%E5%A4%9A%E4%BB%BB%E5%8A%A1%E5%AE%9E%E6%88%98/%E8%B6%8B%E5%8A%A8%E4%BA%91%E4%BD%BF%E7%94%A8%E3%80%8A%E8%A1%A5%E5%85%85%E3%80%8B.html#12","title":"1.2 \u624b\u673a\u53f7\u6ce8\u518c\u3001\u767b\u9646","text":""},{"location":"04-%E5%9F%BA%E4%BA%8EChatGLM%E5%BE%AE%E8%B0%83%E5%AE%9E%E7%8E%B0%E4%BF%A1%E6%81%AF%E6%8A%BD%E5%8F%96%2B%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E7%9A%84%E5%A4%9A%E4%BB%BB%E5%8A%A1%E5%AE%9E%E6%88%98/%E8%B6%8B%E5%8A%A8%E4%BA%91%E4%BD%BF%E7%94%A8%E3%80%8A%E8%A1%A5%E5%85%85%E3%80%8B.html#13","title":"1.3 \u663e\u793a\u767b\u5f55\u6210\u529f","text":""},{"location":"04-%E5%9F%BA%E4%BA%8EChatGLM%E5%BE%AE%E8%B0%83%E5%AE%9E%E7%8E%B0%E4%BF%A1%E6%81%AF%E6%8A%BD%E5%8F%96%2B%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E7%9A%84%E5%A4%9A%E4%BB%BB%E5%8A%A1%E5%AE%9E%E6%88%98/%E8%B6%8B%E5%8A%A8%E4%BA%91%E4%BD%BF%E7%94%A8%E3%80%8A%E8%A1%A5%E5%85%85%E3%80%8B.html#14","title":"1.4 \u67e5\u770b\u8d26\u6237\u4fe1\u606f","text":""},{"location":"04-%E5%9F%BA%E4%BA%8EChatGLM%E5%BE%AE%E8%B0%83%E5%AE%9E%E7%8E%B0%E4%BF%A1%E6%81%AF%E6%8A%BD%E5%8F%96%2B%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E7%9A%84%E5%A4%9A%E4%BB%BB%E5%8A%A1%E5%AE%9E%E6%88%98/%E8%B6%8B%E5%8A%A8%E4%BA%91%E4%BD%BF%E7%94%A8%E3%80%8A%E8%A1%A5%E5%85%85%E3%80%8B.html#_3","title":"\u4e8c\u3001\u521b\u5efa\u9879\u76ee","text":"<ul> <li>\u6ce8\u610f\u6211\u4eec\u60f3\u8981\u4f7f\u7528\u8d8b\u52a8\u4e91\u8fdb\u884c\u6a21\u578b\u4ee3\u7801\u7684\u5fae\u8c03\u5b9e\u73b0\uff0c\u9700\u8981\u521b\u5efa\u9879\u76ee\u8fdb\u884c\u8fd0\u884c\u3002\u4f46\u662f\u56e0\u4e3a\u6211\u4eec\u4f7f\u7528\u7684\u6570\u636e\u96c6\u548c\u9884\u8bad\u7ec3\u6a21\u578b\u90fd\u5c5e\u4e8e\u672c\u5730\u81ea\u5df1\u62e5\u6709\u7684\uff0c\u6240\u4ee5\u9700\u8981\u5728\u8d8b\u52a8\u4e91\u4e0a\u9996\u5148\u300c\u6dfb\u52a0\u6570\u636e\u6e90\u300d\u548c\u300c\u6dfb\u52a0\u9884\u8bad\u7ec3\u6a21\u578b\u300d</li> </ul>"},{"location":"04-%E5%9F%BA%E4%BA%8EChatGLM%E5%BE%AE%E8%B0%83%E5%AE%9E%E7%8E%B0%E4%BF%A1%E6%81%AF%E6%8A%BD%E5%8F%96%2B%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E7%9A%84%E5%A4%9A%E4%BB%BB%E5%8A%A1%E5%AE%9E%E6%88%98/%E8%B6%8B%E5%8A%A8%E4%BA%91%E4%BD%BF%E7%94%A8%E3%80%8A%E8%A1%A5%E5%85%85%E3%80%8B.html#21","title":"2.1 \u6dfb\u52a0\u6570\u636e\u6e90","text":""},{"location":"04-%E5%9F%BA%E4%BA%8EChatGLM%E5%BE%AE%E8%B0%83%E5%AE%9E%E7%8E%B0%E4%BF%A1%E6%81%AF%E6%8A%BD%E5%8F%96%2B%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E7%9A%84%E5%A4%9A%E4%BB%BB%E5%8A%A1%E5%AE%9E%E6%88%98/%E8%B6%8B%E5%8A%A8%E4%BA%91%E4%BD%BF%E7%94%A8%E3%80%8A%E8%A1%A5%E5%85%85%E3%80%8B.html#211","title":"2.1.1 \u9009\u62e9\"\u6570\u636e\"\u5de5\u5177","text":""},{"location":"04-%E5%9F%BA%E4%BA%8EChatGLM%E5%BE%AE%E8%B0%83%E5%AE%9E%E7%8E%B0%E4%BF%A1%E6%81%AF%E6%8A%BD%E5%8F%96%2B%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E7%9A%84%E5%A4%9A%E4%BB%BB%E5%8A%A1%E5%AE%9E%E6%88%98/%E8%B6%8B%E5%8A%A8%E4%BA%91%E4%BD%BF%E7%94%A8%E3%80%8A%E8%A1%A5%E5%85%85%E3%80%8B.html#212","title":"2.1.2 \u521b\u5efa\u6570\u636e","text":""},{"location":"04-%E5%9F%BA%E4%BA%8EChatGLM%E5%BE%AE%E8%B0%83%E5%AE%9E%E7%8E%B0%E4%BF%A1%E6%81%AF%E6%8A%BD%E5%8F%96%2B%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E7%9A%84%E5%A4%9A%E4%BB%BB%E5%8A%A1%E5%AE%9E%E6%88%98/%E8%B6%8B%E5%8A%A8%E4%BA%91%E4%BD%BF%E7%94%A8%E3%80%8A%E8%A1%A5%E5%85%85%E3%80%8B.html#213","title":"2.1.3 \u914d\u7f6e\u6570\u636e\u4fe1\u606f","text":""},{"location":"04-%E5%9F%BA%E4%BA%8EChatGLM%E5%BE%AE%E8%B0%83%E5%AE%9E%E7%8E%B0%E4%BF%A1%E6%81%AF%E6%8A%BD%E5%8F%96%2B%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E7%9A%84%E5%A4%9A%E4%BB%BB%E5%8A%A1%E5%AE%9E%E6%88%98/%E8%B6%8B%E5%8A%A8%E4%BA%91%E4%BD%BF%E7%94%A8%E3%80%8A%E8%A1%A5%E5%85%85%E3%80%8B.html#214","title":"2.1.4 \u9009\u62e9\u7f51\u9875\u4e0a\u4f20\u65b9\u5f0f\u8fdb\u884c\u6570\u636e\u4f20\u8f93","text":""},{"location":"04-%E5%9F%BA%E4%BA%8EChatGLM%E5%BE%AE%E8%B0%83%E5%AE%9E%E7%8E%B0%E4%BF%A1%E6%81%AF%E6%8A%BD%E5%8F%96%2B%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E7%9A%84%E5%A4%9A%E4%BB%BB%E5%8A%A1%E5%AE%9E%E6%88%98/%E8%B6%8B%E5%8A%A8%E4%BA%91%E4%BD%BF%E7%94%A8%E3%80%8A%E8%A1%A5%E5%85%85%E3%80%8B.html#215","title":"2.1.5 \u62d6\u62fd\u672c\u5730\u6570\u636e\u4e0a\u4f20","text":""},{"location":"04-%E5%9F%BA%E4%BA%8EChatGLM%E5%BE%AE%E8%B0%83%E5%AE%9E%E7%8E%B0%E4%BF%A1%E6%81%AF%E6%8A%BD%E5%8F%96%2B%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E7%9A%84%E5%A4%9A%E4%BB%BB%E5%8A%A1%E5%AE%9E%E6%88%98/%E8%B6%8B%E5%8A%A8%E4%BA%91%E4%BD%BF%E7%94%A8%E3%80%8A%E8%A1%A5%E5%85%85%E3%80%8B.html#216","title":"2.1.6 \u6570\u636e\u4e0a\u4f20\u6210\u529f\u5c55\u793a","text":"<p>\u6b64\u65f6\uff0c\u91cd\u65b0\u5237\u65b0\u9875\u9762\uff0c\u4f1a\u5728\u6570\u636e\u5de5\u5177\u680f\u663e\u793a\uff0c\u5df2\u7ecf\u521b\u5efa\u597d\u7684\u6570\u636e\u6e90</p>"},{"location":"04-%E5%9F%BA%E4%BA%8EChatGLM%E5%BE%AE%E8%B0%83%E5%AE%9E%E7%8E%B0%E4%BF%A1%E6%81%AF%E6%8A%BD%E5%8F%96%2B%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E7%9A%84%E5%A4%9A%E4%BB%BB%E5%8A%A1%E5%AE%9E%E6%88%98/%E8%B6%8B%E5%8A%A8%E4%BA%91%E4%BD%BF%E7%94%A8%E3%80%8A%E8%A1%A5%E5%85%85%E3%80%8B.html#22","title":"2.2 \u6dfb\u52a0\u9884\u8bad\u7ec3\u6a21\u578b","text":""},{"location":"04-%E5%9F%BA%E4%BA%8EChatGLM%E5%BE%AE%E8%B0%83%E5%AE%9E%E7%8E%B0%E4%BF%A1%E6%81%AF%E6%8A%BD%E5%8F%96%2B%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E7%9A%84%E5%A4%9A%E4%BB%BB%E5%8A%A1%E5%AE%9E%E6%88%98/%E8%B6%8B%E5%8A%A8%E4%BA%91%E4%BD%BF%E7%94%A8%E3%80%8A%E8%A1%A5%E5%85%85%E3%80%8B.html#221","title":"2.2.1 \u9009\u62e9\"\u6a21\u578b\"\u5de5\u5177\uff0c\u521b\u5efa\u6a21\u578b","text":""},{"location":"04-%E5%9F%BA%E4%BA%8EChatGLM%E5%BE%AE%E8%B0%83%E5%AE%9E%E7%8E%B0%E4%BF%A1%E6%81%AF%E6%8A%BD%E5%8F%96%2B%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E7%9A%84%E5%A4%9A%E4%BB%BB%E5%8A%A1%E5%AE%9E%E6%88%98/%E8%B6%8B%E5%8A%A8%E4%BA%91%E4%BD%BF%E7%94%A8%E3%80%8A%E8%A1%A5%E5%85%85%E3%80%8B.html#222","title":"2.2.2 \u914d\u7f6e\u6a21\u578b\u4fe1\u606f","text":""},{"location":"04-%E5%9F%BA%E4%BA%8EChatGLM%E5%BE%AE%E8%B0%83%E5%AE%9E%E7%8E%B0%E4%BF%A1%E6%81%AF%E6%8A%BD%E5%8F%96%2B%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E7%9A%84%E5%A4%9A%E4%BB%BB%E5%8A%A1%E5%AE%9E%E6%88%98/%E8%B6%8B%E5%8A%A8%E4%BA%91%E4%BD%BF%E7%94%A8%E3%80%8A%E8%A1%A5%E5%85%85%E3%80%8B.html#223-sftp","title":"2.2.3 \u9009\u62e9SFTP\u65b9\u5f0f\u8fdb\u884c\u6570\u636e\u4f20\u8f93","text":"<ul> <li>\u56e0\u4e3a\u6a21\u578b\u6587\u4ef6\u8fc7\u5927</li> </ul>"},{"location":"04-%E5%9F%BA%E4%BA%8EChatGLM%E5%BE%AE%E8%B0%83%E5%AE%9E%E7%8E%B0%E4%BF%A1%E6%81%AF%E6%8A%BD%E5%8F%96%2B%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E7%9A%84%E5%A4%9A%E4%BB%BB%E5%8A%A1%E5%AE%9E%E6%88%98/%E8%B6%8B%E5%8A%A8%E4%BA%91%E4%BD%BF%E7%94%A8%E3%80%8A%E8%A1%A5%E5%85%85%E3%80%8B.html#224-sftp","title":"2.2.4 \u70b9\u51fbSFTP\u65b9\u5f0f\u4e4b\u540e, \u9009\u62e9\u6587\u4ef6\u4f20\u8f93","text":""},{"location":"04-%E5%9F%BA%E4%BA%8EChatGLM%E5%BE%AE%E8%B0%83%E5%AE%9E%E7%8E%B0%E4%BF%A1%E6%81%AF%E6%8A%BD%E5%8F%96%2B%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E7%9A%84%E5%A4%9A%E4%BB%BB%E5%8A%A1%E5%AE%9E%E6%88%98/%E8%B6%8B%E5%8A%A8%E4%BA%91%E4%BD%BF%E7%94%A8%E3%80%8A%E8%A1%A5%E5%85%85%E3%80%8B.html#225-sftp","title":"2.2.5 \u83b7\u53d6SFTP\u914d\u7f6e\u4fe1\u606f","text":""},{"location":"04-%E5%9F%BA%E4%BA%8EChatGLM%E5%BE%AE%E8%B0%83%E5%AE%9E%E7%8E%B0%E4%BF%A1%E6%81%AF%E6%8A%BD%E5%8F%96%2B%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E7%9A%84%E5%A4%9A%E4%BB%BB%E5%8A%A1%E5%AE%9E%E6%88%98/%E8%B6%8B%E5%8A%A8%E4%BA%91%E4%BD%BF%E7%94%A8%E3%80%8A%E8%A1%A5%E5%85%85%E3%80%8B.html#226-cmdwindows","title":"2.2.6 \u4f7f\u7528CMD\u4f20\u8f93\u6570\u636e(Windows)","text":"<p>windows10 \u6700\u65b0\u7248\u53ca\u4ee5\u4e0a\u7248\u672c\u90fd\u652f\u6301 CMD \u4f20\u8f93\u6570\u636e\u3002</p> <ol> <li>\u8f93\u5165 <code>win+r</code> \u952e\uff0c\u7cfb\u7edf\u5de6\u4e0b\u89d2\u5f39\u51fa\u8fd0\u884c\u6846\u3002</li> <li>\u8f93\u5165 <code>cmd</code> \u5e76\u56de\u8f66\uff0c\u8fdb\u5165\u547d\u4ee4\u884c\u3002</li> <li>\u53bb\u590d\u5236 \u8fde\u63a5\u5b57\u7b26\u3002</li> <li>\u5728\u547d\u4ee4\u884c\u4e2d\u7c98\u8d34 \u8fde\u63a5\u5b57\u7b26 \u5e76\u56de\u8f66\u3002</li> <li>\u5982\u8fd4\u56de\u5982\u4e0b\u4fe1\u606f\uff0c\u5219\u8f93\u5165 <code>yes</code>\u3002</li> </ol> <pre><code>Are you sure you want to continue connecting (yes/no/[fingerprint])?\n</code></pre> <ol> <li>\u8fd4\u56de\u5982\u4e0b\u4fe1\u606f\u65f6\uff0c\u8f93\u5165 \u5bc6\u7801 \u5e76\u56de\u8f66\uff0c\u5bc6\u7801 \u4ece\u4e0a\u8ff0\u56fe\u7247\u5904\u83b7\u53d6\u3002</li> </ol> <pre><code>roif48iKYp@cluster1-dev4.virtaicloud.com's password:\n</code></pre> <ol> <li>\u6700\u540e\u8fd4\u56de sftp \u547d\u4ee4\u884c\u5373\u5982\u4e0b\u4fe1\u606f\u65f6\uff0c\u8868\u793a\u8fde\u63a5\u6210\u529f\u3002</li> </ol> <pre><code>sftp&gt;\n</code></pre> <ol> <li>\u5728 sftp \u547d\u4ee4\u884c\u4e2d\u8f93\u5165\u4f20\u8f93\u547d\u4ee4\u3002</li> </ol> <pre><code>put -r D:\\Git\\tool\\ /upload\n</code></pre> <p>\u6ce8\u610f\uff1a\u60a8\u53ea\u9700\u4fee\u6539\u547d\u4ee4\u4e2d <code>D:\\Git\\tool\\</code> \u4e3a\u60a8\u5b9e\u9645\u8981\u4e0a\u4f20\u7684\u6570\u636e\u5728\u60a8\u672c\u5730\u7684\u5b58\u50a8\u8def\u5f84\u3002</p>"},{"location":"04-%E5%9F%BA%E4%BA%8EChatGLM%E5%BE%AE%E8%B0%83%E5%AE%9E%E7%8E%B0%E4%BF%A1%E6%81%AF%E6%8A%BD%E5%8F%96%2B%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E7%9A%84%E5%A4%9A%E4%BB%BB%E5%8A%A1%E5%AE%9E%E6%88%98/%E8%B6%8B%E5%8A%A8%E4%BA%91%E4%BD%BF%E7%94%A8%E3%80%8A%E8%A1%A5%E5%85%85%E3%80%8B.html#226-mac","title":"2.2.6 \u4f7f\u7528Mac\u7ec8\u7aef\u4f20\u8f93\u6570\u636e","text":"<ol> <li>\u6309 Command + \u7a7a\u683c \u6253\u5f00\u641c\u7d22\u680f\u3002</li> <li>\u641c\u7d22\u680f\u4e2d\u8f93\u5165 <code>\u7ec8\u7aef</code> \u5e76\u56de\u8f66\uff0c\u6253\u5f00\u7ec8\u7aef\u3002</li> <li>\u53bb\u590d\u5236 \u8fde\u63a5\u5b57\u7b26\u3002</li> <li>\u5728\u547d\u4ee4\u884c\u4e2d\u7c98\u8d34 \u8fde\u63a5\u5b57\u7b26 \u5e76\u56de\u8f66\u3002</li> <li>\u5982\u8fd4\u56de\u5982\u4e0b\u4fe1\u606f\uff0c\u5219\u8f93\u5165 <code>yes</code>\u3002</li> </ol> <pre><code>Are you sure you want to continue connecting (yes/no/[fingerprint])?\n</code></pre> <ol> <li>\u8fd4\u56de\u5982\u4e0b\u4fe1\u606f\u65f6\uff0c\u8f93\u5165 \u5bc6\u7801 \u5e76\u56de\u8f66\uff0c\u5bc6\u7801 \u4ece\u4e0a\u8ff0\u56fe\u7247\u5904\u83b7\u53d6\u3002</li> </ol> <pre><code>roif48iKYp@cluster1-dev4.virtaicloud.com's password:\n</code></pre> <ol> <li>\u6700\u540e\u8fd4\u56de sftp \u547d\u4ee4\u884c\u5373\u5982\u4e0b\u4fe1\u606f\u65f6\uff0c\u8868\u793a\u8fde\u63a5\u6210\u529f\u3002</li> </ol> <pre><code>sftp&gt;\n</code></pre> <ol> <li>\u5728 sftp \u547d\u4ee4\u884c\u4e2d\u8f93\u5165\u4f20\u8f93\u547d\u4ee4\u3002</li> </ol> <pre><code>put -r D:\\Git\\tool\\ /upload\n</code></pre> <p>\u6ce8\u610f\uff1a\u60a8\u53ea\u9700\u4fee\u6539\u547d\u4ee4\u4e2d <code>D:\\Git\\tool\\</code> \u4e3a\u60a8\u5b9e\u9645\u8981\u4e0a\u4f20\u7684\u6570\u636e\u5728\u60a8\u672c\u5730\u7684\u5b58\u50a8\u8def\u5f84\u3002 </p>"},{"location":"04-%E5%9F%BA%E4%BA%8EChatGLM%E5%BE%AE%E8%B0%83%E5%AE%9E%E7%8E%B0%E4%BF%A1%E6%81%AF%E6%8A%BD%E5%8F%96%2B%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E7%9A%84%E5%A4%9A%E4%BB%BB%E5%8A%A1%E5%AE%9E%E6%88%98/%E8%B6%8B%E5%8A%A8%E4%BA%91%E4%BD%BF%E7%94%A8%E3%80%8A%E8%A1%A5%E5%85%85%E3%80%8B.html#227","title":"2.2.7 \u8fde\u63a5\u6210\u529f\u4e0a\u4f20\u6570\u636e\u5c55\u793a","text":"<p>\u6ce8\u610f\uff1a\u5f53\u4f60\u7684\u6a21\u578b\u6570\u636e\u4f20\u8f93\u5b8c\u6210\u540e\uff0c\u53ef\u4ee5\u5173\u95ed\u901a\u9053\u3002</p>"},{"location":"04-%E5%9F%BA%E4%BA%8EChatGLM%E5%BE%AE%E8%B0%83%E5%AE%9E%E7%8E%B0%E4%BF%A1%E6%81%AF%E6%8A%BD%E5%8F%96%2B%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E7%9A%84%E5%A4%9A%E4%BB%BB%E5%8A%A1%E5%AE%9E%E6%88%98/%E8%B6%8B%E5%8A%A8%E4%BA%91%E4%BD%BF%E7%94%A8%E3%80%8A%E8%A1%A5%E5%85%85%E3%80%8B.html#23","title":"2.3 \u521b\u5efa\u9879\u76ee","text":""},{"location":"04-%E5%9F%BA%E4%BA%8EChatGLM%E5%BE%AE%E8%B0%83%E5%AE%9E%E7%8E%B0%E4%BF%A1%E6%81%AF%E6%8A%BD%E5%8F%96%2B%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E7%9A%84%E5%A4%9A%E4%BB%BB%E5%8A%A1%E5%AE%9E%E6%88%98/%E8%B6%8B%E5%8A%A8%E4%BA%91%E4%BD%BF%E7%94%A8%E3%80%8A%E8%A1%A5%E5%85%85%E3%80%8B.html#231","title":"2.3.1 \u70b9\u51fb\u53f3\u4e0a\u89d2\u521b\u5efa\u9879\u76ee","text":""},{"location":"04-%E5%9F%BA%E4%BA%8EChatGLM%E5%BE%AE%E8%B0%83%E5%AE%9E%E7%8E%B0%E4%BF%A1%E6%81%AF%E6%8A%BD%E5%8F%96%2B%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E7%9A%84%E5%A4%9A%E4%BB%BB%E5%8A%A1%E5%AE%9E%E6%88%98/%E8%B6%8B%E5%8A%A8%E4%BA%91%E4%BD%BF%E7%94%A8%E3%80%8A%E8%A1%A5%E5%85%85%E3%80%8B.html#232","title":"2.3.2 \u4f9d\u6b21\u914d\u7f6e\u9879\u76ee\u5185\u5bb9","text":"<p>\u6ce8\u610f\uff1a\u521b\u5efa\u9879\u76ee\u540e\uff0c\u6570\u636e\u4f1a\u9ed8\u8ba4\u4fdd\u5b58\u7684\u8def\u5f84\u4e3a\uff1a/gemini/data1\u4e0b\uff0c\u9884\u8bad\u7ec3\u6a21\u578b\u4f1a\u81ea\u52a8\u4fdd\u5b58\u5728/genmini/pretrain\u4e0b</p>"},{"location":"04-%E5%9F%BA%E4%BA%8EChatGLM%E5%BE%AE%E8%B0%83%E5%AE%9E%E7%8E%B0%E4%BF%A1%E6%81%AF%E6%8A%BD%E5%8F%96%2B%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E7%9A%84%E5%A4%9A%E4%BB%BB%E5%8A%A1%E5%AE%9E%E6%88%98/%E8%B6%8B%E5%8A%A8%E4%BA%91%E4%BD%BF%E7%94%A8%E3%80%8A%E8%A1%A5%E5%85%85%E3%80%8B.html#233","title":"2.3.3 \u955c\u50cf\u73af\u5883\u9009\u62e9","text":""},{"location":"04-%E5%9F%BA%E4%BA%8EChatGLM%E5%BE%AE%E8%B0%83%E5%AE%9E%E7%8E%B0%E4%BF%A1%E6%81%AF%E6%8A%BD%E5%8F%96%2B%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E7%9A%84%E5%A4%9A%E4%BB%BB%E5%8A%A1%E5%AE%9E%E6%88%98/%E8%B6%8B%E5%8A%A8%E4%BA%91%E4%BD%BF%E7%94%A8%E3%80%8A%E8%A1%A5%E5%85%85%E3%80%8B.html#234","title":"2.3.4 \u521b\u5efa\u5b8c\u9879\u76ee\u540e\uff0c\u8981\u6c42\u4e0a\u4f20\u672c\u5730\u4ee3\u7801\uff08\u62d6\u62fd\u5f0f\uff09","text":"<p>\u6ce8\u610f\uff1a\u4e0a\u4f20\u7684\u4ee3\u7801\u6587\u4ef6\uff0c\u9700\u8981\u8fdb\u884c\u538b\u7f29\u540e\u4e0a\u4f20</p>"},{"location":"04-%E5%9F%BA%E4%BA%8EChatGLM%E5%BE%AE%E8%B0%83%E5%AE%9E%E7%8E%B0%E4%BF%A1%E6%81%AF%E6%8A%BD%E5%8F%96%2B%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E7%9A%84%E5%A4%9A%E4%BB%BB%E5%8A%A1%E5%AE%9E%E6%88%98/%E8%B6%8B%E5%8A%A8%E4%BA%91%E4%BD%BF%E7%94%A8%E3%80%8A%E8%A1%A5%E5%85%85%E3%80%8B.html#235","title":"2.3.5 \u9879\u76ee\u521b\u5efa\u6210\u529f\u5c55\u793a","text":""},{"location":"04-%E5%9F%BA%E4%BA%8EChatGLM%E5%BE%AE%E8%B0%83%E5%AE%9E%E7%8E%B0%E4%BF%A1%E6%81%AF%E6%8A%BD%E5%8F%96%2B%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E7%9A%84%E5%A4%9A%E4%BB%BB%E5%8A%A1%E5%AE%9E%E6%88%98/%E8%B6%8B%E5%8A%A8%E4%BA%91%E4%BD%BF%E7%94%A8%E3%80%8A%E8%A1%A5%E5%85%85%E3%80%8B.html#_4","title":"\u4e09\u3001\u8fd0\u884c\u670d\u52a1\u73af\u5883\u3001\u8bad\u7ec3\u6a21\u578b","text":""},{"location":"04-%E5%9F%BA%E4%BA%8EChatGLM%E5%BE%AE%E8%B0%83%E5%AE%9E%E7%8E%B0%E4%BF%A1%E6%81%AF%E6%8A%BD%E5%8F%96%2B%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E7%9A%84%E5%A4%9A%E4%BB%BB%E5%8A%A1%E5%AE%9E%E6%88%98/%E8%B6%8B%E5%8A%A8%E4%BA%91%E4%BD%BF%E7%94%A8%E3%80%8A%E8%A1%A5%E5%85%85%E3%80%8B.html#31","title":"3.1 \u521d\u59cb\u5316\u5f00\u53d1\u73af\u5883","text":""},{"location":"04-%E5%9F%BA%E4%BA%8EChatGLM%E5%BE%AE%E8%B0%83%E5%AE%9E%E7%8E%B0%E4%BF%A1%E6%81%AF%E6%8A%BD%E5%8F%96%2B%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E7%9A%84%E5%A4%9A%E4%BB%BB%E5%8A%A1%E5%AE%9E%E6%88%98/%E8%B6%8B%E5%8A%A8%E4%BA%91%E4%BD%BF%E7%94%A8%E3%80%8A%E8%A1%A5%E5%85%85%E3%80%8B.html#311","title":"3.1.1 \u70b9\u51fb\u5f00\u53d1\u5de5\u5177\uff0c\u8fdb\u5165\u5f00\u53d1\u73af\u5883\u5b9e\u4f8b\u914d\u7f6e\u754c\u9762","text":""},{"location":"04-%E5%9F%BA%E4%BA%8EChatGLM%E5%BE%AE%E8%B0%83%E5%AE%9E%E7%8E%B0%E4%BF%A1%E6%81%AF%E6%8A%BD%E5%8F%96%2B%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E7%9A%84%E5%A4%9A%E4%BB%BB%E5%8A%A1%E5%AE%9E%E6%88%98/%E8%B6%8B%E5%8A%A8%E4%BA%91%E4%BD%BF%E7%94%A8%E3%80%8A%E8%A1%A5%E5%85%85%E3%80%8B.html#312","title":"3.1.2 \u8fdb\u884c\u5f00\u53d1\u73af\u5883\u5b9e\u4f8b\u914d\u7f6e","text":""},{"location":"04-%E5%9F%BA%E4%BA%8EChatGLM%E5%BE%AE%E8%B0%83%E5%AE%9E%E7%8E%B0%E4%BF%A1%E6%81%AF%E6%8A%BD%E5%8F%96%2B%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E7%9A%84%E5%A4%9A%E4%BB%BB%E5%8A%A1%E5%AE%9E%E6%88%98/%E8%B6%8B%E5%8A%A8%E4%BA%91%E4%BD%BF%E7%94%A8%E3%80%8A%E8%A1%A5%E5%85%85%E3%80%8B.html#313","title":"3.1.3 \u7b49\u5f85\u8d44\u6e90\u914d\u7f6e","text":""},{"location":"04-%E5%9F%BA%E4%BA%8EChatGLM%E5%BE%AE%E8%B0%83%E5%AE%9E%E7%8E%B0%E4%BF%A1%E6%81%AF%E6%8A%BD%E5%8F%96%2B%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E7%9A%84%E5%A4%9A%E4%BB%BB%E5%8A%A1%E5%AE%9E%E6%88%98/%E8%B6%8B%E5%8A%A8%E4%BA%91%E4%BD%BF%E7%94%A8%E3%80%8A%E8%A1%A5%E5%85%85%E3%80%8B.html#314","title":"3.1.4  \u8d44\u6e90\u914d\u7f6e\u5b8c\u6210\uff0c\u7b49\u5f85\u8fdb\u5165\u5f00\u53d1\u73af\u5883","text":""},{"location":"04-%E5%9F%BA%E4%BA%8EChatGLM%E5%BE%AE%E8%B0%83%E5%AE%9E%E7%8E%B0%E4%BF%A1%E6%81%AF%E6%8A%BD%E5%8F%96%2B%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E7%9A%84%E5%A4%9A%E4%BB%BB%E5%8A%A1%E5%AE%9E%E6%88%98/%E8%B6%8B%E5%8A%A8%E4%BA%91%E4%BD%BF%E7%94%A8%E3%80%8A%E8%A1%A5%E5%85%85%E3%80%8B.html#32","title":"3.2 \u914d\u7f6e\u5f00\u53d1\u73af\u5883","text":""},{"location":"04-%E5%9F%BA%E4%BA%8EChatGLM%E5%BE%AE%E8%B0%83%E5%AE%9E%E7%8E%B0%E4%BF%A1%E6%81%AF%E6%8A%BD%E5%8F%96%2B%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E7%9A%84%E5%A4%9A%E4%BB%BB%E5%8A%A1%E5%AE%9E%E6%88%98/%E8%B6%8B%E5%8A%A8%E4%BA%91%E4%BD%BF%E7%94%A8%E3%80%8A%E8%A1%A5%E5%85%85%E3%80%8B.html#321","title":"3.2.1 \u8fdb\u5165\u5f00\u53d1\u73af\u5883","text":""},{"location":"04-%E5%9F%BA%E4%BA%8EChatGLM%E5%BE%AE%E8%B0%83%E5%AE%9E%E7%8E%B0%E4%BF%A1%E6%81%AF%E6%8A%BD%E5%8F%96%2B%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E7%9A%84%E5%A4%9A%E4%BB%BB%E5%8A%A1%E5%AE%9E%E6%88%98/%E8%B6%8B%E5%8A%A8%E4%BA%91%E4%BD%BF%E7%94%A8%E3%80%8A%E8%A1%A5%E5%85%85%E3%80%8B.html#322","title":"3.2.2 \u5f00\u53d1\u73af\u5883\u5c55\u793a","text":""},{"location":"04-%E5%9F%BA%E4%BA%8EChatGLM%E5%BE%AE%E8%B0%83%E5%AE%9E%E7%8E%B0%E4%BF%A1%E6%81%AF%E6%8A%BD%E5%8F%96%2B%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E7%9A%84%E5%A4%9A%E4%BB%BB%E5%8A%A1%E5%AE%9E%E6%88%98/%E8%B6%8B%E5%8A%A8%E4%BA%91%E4%BD%BF%E7%94%A8%E3%80%8A%E8%A1%A5%E5%85%85%E3%80%8B.html#323","title":"3.2.3 \u8fdb\u5165\u7f51\u9875\u7ec8\u7aef\u64cd\u4f5c","text":""},{"location":"04-%E5%9F%BA%E4%BA%8EChatGLM%E5%BE%AE%E8%B0%83%E5%AE%9E%E7%8E%B0%E4%BF%A1%E6%81%AF%E6%8A%BD%E5%8F%96%2B%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E7%9A%84%E5%A4%9A%E4%BB%BB%E5%8A%A1%E5%AE%9E%E6%88%98/%E8%B6%8B%E5%8A%A8%E4%BA%91%E4%BD%BF%E7%94%A8%E3%80%8A%E8%A1%A5%E5%85%85%E3%80%8B.html#324","title":"3.2.4 \u67e5\u770b\u4ee3\u7801\u3001\u6570\u636e\u3001\u9884\u8bad\u7ec3\u6a21\u578b","text":""},{"location":"04-%E5%9F%BA%E4%BA%8EChatGLM%E5%BE%AE%E8%B0%83%E5%AE%9E%E7%8E%B0%E4%BF%A1%E6%81%AF%E6%8A%BD%E5%8F%96%2B%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E7%9A%84%E5%A4%9A%E4%BB%BB%E5%8A%A1%E5%AE%9E%E6%88%98/%E8%B6%8B%E5%8A%A8%E4%BA%91%E4%BD%BF%E7%94%A8%E3%80%8A%E8%A1%A5%E5%85%85%E3%80%8B.html#325","title":"3.2.5 \u5b89\u88c5\u4f9d\u8d56\u5e93","text":"<ul> <li>\u9ed8\u8ba4\u5b89\u88c5\u7684\u6709pytorch=2.0.1\uff0c\u5e76\u4e14\u652f\u6301cuda</li> <li>\u5b89\u88c5\u5176\u4ed6\u7b2c\u4e09\u65b9\u5e93</li> </ul> <pre><code>pip install protobuf==3.20.0 transformers==4.27.1 icetk cpm_kernels  \npip install peft==0.11.1\n</code></pre>"},{"location":"04-%E5%9F%BA%E4%BA%8EChatGLM%E5%BE%AE%E8%B0%83%E5%AE%9E%E7%8E%B0%E4%BF%A1%E6%81%AF%E6%8A%BD%E5%8F%96%2B%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E7%9A%84%E5%A4%9A%E4%BB%BB%E5%8A%A1%E5%AE%9E%E6%88%98/%E8%B6%8B%E5%8A%A8%E4%BA%91%E4%BD%BF%E7%94%A8%E3%80%8A%E8%A1%A5%E5%85%85%E3%80%8B.html#33","title":"3.3 \u8bad\u7ec3\u6a21\u578b","text":""},{"location":"04-%E5%9F%BA%E4%BA%8EChatGLM%E5%BE%AE%E8%B0%83%E5%AE%9E%E7%8E%B0%E4%BF%A1%E6%81%AF%E6%8A%BD%E5%8F%96%2B%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E7%9A%84%E5%A4%9A%E4%BB%BB%E5%8A%A1%E5%AE%9E%E6%88%98/%E8%B6%8B%E5%8A%A8%E4%BA%91%E4%BD%BF%E7%94%A8%E3%80%8A%E8%A1%A5%E5%85%85%E3%80%8B.html#331","title":"3.3.1 \u8def\u5f84\u4fee\u6539","text":"<ul> <li>\u56e0\u4e3a\u5728\u8d8b\u52a8\u4e91\u670d\u52a1\u4e0b\uff0c\u6570\u636e\u3001\u9884\u8bad\u7ec3\u6a21\u578b\u3001\u4ee3\u7801\u7684\u4f4d\u7f6e\u90fd\u53d1\u751f\u4e86\u6539\u53d8\uff0c\u6240\u4ee5\u8981\u6539\u53d8\u76f8\u5e94\u7684\u5f15\u7528\u6570\u636e\u4f4d\u7f6e</li> <li>config\u6587\u4ef6\u7c7b\u7684\u8def\u5f84\u4fee\u6539</li> <li>\u8fd8\u9700\u8981\u5c06train.py\uff0cmodel=model.float()\u6539\u4e3amodel=model.half()\uff0c\u52a0\u5feb\u6a21\u578b\u8bad\u7ec3</li> </ul>"},{"location":"04-%E5%9F%BA%E4%BA%8EChatGLM%E5%BE%AE%E8%B0%83%E5%AE%9E%E7%8E%B0%E4%BF%A1%E6%81%AF%E6%8A%BD%E5%8F%96%2B%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E7%9A%84%E5%A4%9A%E4%BB%BB%E5%8A%A1%E5%AE%9E%E6%88%98/%E8%B6%8B%E5%8A%A8%E4%BA%91%E4%BD%BF%E7%94%A8%E3%80%8A%E8%A1%A5%E5%85%85%E3%80%8B.html#332","title":"3.3.2 \u8bad\u7ec3\u811a\u672c","text":"<ul> <li>\u547d\u4ee4</li> </ul> <pre><code>python train.py\n</code></pre> <ul> <li>\u6267\u884c\u7ed3\u679c</li> </ul> <ul> <li>\u8d44\u6e90\u76d1\u63a7</li> </ul> <ul> <li>\u8017\u65f6</li> <li>1h20min</li> </ul>"},{"location":"04-%E5%9F%BA%E4%BA%8EChatGLM%E5%BE%AE%E8%B0%83%E5%AE%9E%E7%8E%B0%E4%BF%A1%E6%81%AF%E6%8A%BD%E5%8F%96%2B%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E7%9A%84%E5%A4%9A%E4%BB%BB%E5%8A%A1%E5%AE%9E%E6%88%98/%E8%B6%8B%E5%8A%A8%E4%BA%91%E4%BD%BF%E7%94%A8%E3%80%8A%E8%A1%A5%E5%85%85%E3%80%8B.html#34","title":"3.4 \u6a21\u578b\u9884\u6d4b","text":"<ul> <li>\u6ce8\u610f\u4fee\u6539inference.py\u811a\u672c\u91cc\u6a21\u578b\u7684\u8def\u5f84</li> <li>\u7ed3\u679c\u5c55\u793a</li> </ul>"},{"location":"04-%E5%9F%BA%E4%BA%8EChatGLM%E5%BE%AE%E8%B0%83%E5%AE%9E%E7%8E%B0%E4%BF%A1%E6%81%AF%E6%8A%BD%E5%8F%96%2B%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E7%9A%84%E5%A4%9A%E4%BB%BB%E5%8A%A1%E5%AE%9E%E6%88%98/%E8%B6%8B%E5%8A%A8%E4%BA%91%E4%BD%BF%E7%94%A8%E3%80%8A%E8%A1%A5%E5%85%85%E3%80%8B.html#35","title":"3.5 \u6a21\u578b\u4e0b\u8f7d\u672c\u5730\u5b9e\u73b0\u9884\u6d4b","text":""},{"location":"04-%E5%9F%BA%E4%BA%8EChatGLM%E5%BE%AE%E8%B0%83%E5%AE%9E%E7%8E%B0%E4%BF%A1%E6%81%AF%E6%8A%BD%E5%8F%96%2B%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E7%9A%84%E5%A4%9A%E4%BB%BB%E5%8A%A1%E5%AE%9E%E6%88%98/%E8%B6%8B%E5%8A%A8%E4%BA%91%E4%BD%BF%E7%94%A8%E3%80%8A%E8%A1%A5%E5%85%85%E3%80%8B.html#351-ssh","title":"3.5.1 SSH\u914d\u7f6e","text":"<ul> <li>\u8fdb\u5165 \u201c\u5e73\u53f0\u8bbe\u7f6e\u201d \u9875\u9762</li> <li>\u767b\u5f55\u5e73\u53f0\u3002</li> <li> <p>\u4e0b\u62c9\u53f3\u4e0a\u89d2\u8d26\u6237\u5904\uff0c\u9009\u62e9 \u5e73\u53f0\u8bbe\u7f6e\u3002</p> </li> <li> <p>\u70b9\u51fb\u5e73\u53f0\u8bbe\u7f6e</p> </li> </ul> <ul> <li>\u5728 \u5e73\u53f0\u8bbe\u7f6e \u9875\u9762\u7684 SSH Key \u5904\uff0c\u60a8\u53ef\u7ba1\u7406 \u5bc6\u7801\u51ed\u8bc1 \u548c \u79d8\u94a5\u51ed\u8bc1\uff08SSH Key\uff09\u3002</li> </ul> <p></p> <p>\u6ce8\u610f\uff1a\u9700\u8981\u81ea\u5df1\u8bbe\u5b9a\u7528\u6237\u540d\u548c\u5bc6\u7801\u3001ssh key\u81ea\u52a8\u751f\u6210\u5373\u53ef</p> <ul> <li>\u5728\u5177\u4f53\u9879\u76ee\u4e2d\u9009\u62e9 \u5f00\u53d1\uff0c\u8fdb\u5165 \u5f00\u53d1\u73af\u5883\u5b9e\u4f8b \u9875 \u83b7\u53d6\u5f00\u53d1\u73af\u5883\u7684 ssh \u8fde\u63a5\u4e32\u7528\u4e8e\u8fde\u63a5\u5f00\u53d1\u73af\u5883\u3002</li> <li>\u6ce8\u610f\uff1a\u5728\u4fee\u6539SSH\u914d\u7f6e\u4e2d\uff0c\u9009\u62e9\u5f00\u542f\uff0c\u7136\u540e\u518d\u542f\u52a8\u73af\u5883</li> </ul> <p></p> <p>\u6ce8\u610f\u53ea\u8981\u8fdb\u5165\u5f00\u53d1\u73af\u5883\uff0c\u5c31\u4f1a\u4ea7\u751f\u8d39\u7528\uff0c\u4e3a\u4e86\u8282\u7701\u8d44\u6e90\uff0c\u53ef\u4ee5\u5c06\u5b9e\u4f8b\u89c4\u683c\u8fdb\u884c\u51cf\u914d\uff0c\u8282\u7701\u7b97\u529b</p> <ul> <li>\u542f\u52a8\u73af\u5883\u540e\uff0c\u51fa\u73b0ssh\u8fde\u63a5\u4e32</li> </ul> <p></p>"},{"location":"04-%E5%9F%BA%E4%BA%8EChatGLM%E5%BE%AE%E8%B0%83%E5%AE%9E%E7%8E%B0%E4%BF%A1%E6%81%AF%E6%8A%BD%E5%8F%96%2B%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E7%9A%84%E5%A4%9A%E4%BB%BB%E5%8A%A1%E5%AE%9E%E6%88%98/%E8%B6%8B%E5%8A%A8%E4%BA%91%E4%BD%BF%E7%94%A8%E3%80%8A%E8%A1%A5%E5%85%85%E3%80%8B.html#352","title":"3.5.2 \u7ec8\u7aef\u64cd\u4f5c\u8fde\u63a5\u4e91\u670d\u52a1","text":"<ul> <li>\u590d\u5236ssh\u8fde\u63a5\u4e32\uff0c\u8fdb\u884c\u4fee\u6539</li> </ul> <pre><code>ssh -p 30022 itheima@root@ssh-736af97802ac911f1b7f454489821925.swwutgwduthw@ssh.virtaicloud.com\n\n\u4e3b\u673a\u540d\uff1assh.virtaicloud.com\n\n\u7aef\u53e3\u53f7\uff1a30022\n\n\u7528\u6237\u540d\uff1aitheima@root@ssh-736af97802ac911f1b7f454489821925.swwutgwduthw@\n</code></pre> <ul> <li>\u83b7\u53d6\u6a21\u578b\u4fdd\u5b58\u8def\u5f84</li> </ul> <pre><code>/gemini/code/checkpoints/ptune/model_best\n</code></pre> <ul> <li>\u5c06\u6a21\u578b\u4e0b\u8f7d\u5230\u672c\u5730\u684c\u9762</li> <li>\u547d\u4ee4</li> </ul> <pre><code>scp -p 30022 -r itheima@root@ssh-736af97802ac911f1b7f454489821925.swwutgwduthw@ssh.virtaicloud.com:/gemini/code/checkpoints/ptune/model_best ~/Desktop\n</code></pre>"}]}