
<!doctype html>
<html lang="zh" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
        <link rel="prev" href="01-%E5%A4%A7%E6%A8%A1%E5%9E%8BPrompt-Tuning%E6%96%B9%E6%B3%95.html">
      
      
        <link rel="next" href="../02-%E5%9F%BA%E4%BA%8EGPT2%E7%9A%84%E5%8C%BB%E7%96%97%E9%97%AE%E8%AF%8A%E6%9C%BA%E5%99%A8%E4%BA%BA/01-%E5%8C%BB%E7%96%97%E9%97%AE%E8%AF%8A%E6%9C%BA%E5%99%A8%E4%BA%BA%E5%AE%9E%E7%8E%B0.html">
      
      
      <link rel="icon" href="../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.16">
    
    
      
        <title>1.2 大模型PEFT微调方法 - 大模型微调技术V6.1</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/main.7e37652d.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL("..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    <body dir="ltr">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#llmpeft" class="md-skip">
          跳转至
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="页眉">
    <a href=".." title="大模型微调技术V6.1" class="md-header__button md-logo" aria-label="大模型微调技术V6.1" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            大模型微调技术V6.1
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              1.2 大模型PEFT微调方法
            
          </span>
        </div>
      </div>
    </div>
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="搜索" placeholder="搜索" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="查找">
        
        <button type="reset" class="md-search__icon md-icon" title="清空当前内容" aria-label="清空当前内容" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            正在初始化搜索引擎
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="导航栏" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href=".." title="大模型微调技术V6.1" class="md-nav__button md-logo" aria-label="大模型微调技术V6.1" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    大模型微调技术V6.1
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
    
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_1" checked>
        
          
          <label class="md-nav__link" for="__nav_1" id="__nav_1_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    1、大模型微调的主要方式
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_1_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_1">
            <span class="md-nav__icon md-icon"></span>
            1、大模型微调的主要方式
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="01-%E5%A4%A7%E6%A8%A1%E5%9E%8BPrompt-Tuning%E6%96%B9%E6%B3%95.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    1.1 大模型Prompt-Tuning方法
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  
  <span class="md-ellipsis">
    1.2 大模型PEFT微调方法
    
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="02-%E5%A4%A7%E6%A8%A1%E5%9E%8BPEFT%E5%BE%AE%E8%B0%83%E6%96%B9%E6%B3%95.html" class="md-nav__link md-nav__link--active">
        
  
  
  <span class="md-ellipsis">
    1.2 大模型PEFT微调方法
    
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="目录">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      目录
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#_1" class="md-nav__link">
    <span class="md-ellipsis">
      学习目标
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#peft" class="md-nav__link">
    <span class="md-ellipsis">
      PEFT(大模型参数高效微调)
    </span>
  </a>
  
    <nav class="md-nav" aria-label="PEFT(大模型参数高效微调)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1-prefix-tuning" class="md-nav__link">
    <span class="md-ellipsis">
      1. Prefix Tuning
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2-adapter-tuning" class="md-nav__link">
    <span class="md-ellipsis">
      2. Adapter Tuning
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3-lora" class="md-nav__link">
    <span class="md-ellipsis">
      3. LoRA
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#4-qlora" class="md-nav__link">
    <span class="md-ellipsis">
      4. QLoRA
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_2" class="md-nav__link">
    <span class="md-ellipsis">
      小结总结
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" >
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    2、基于GPT2的医疗问诊机器人
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            2、基于GPT2的医疗问诊机器人
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../02-%E5%9F%BA%E4%BA%8EGPT2%E7%9A%84%E5%8C%BB%E7%96%97%E9%97%AE%E8%AF%8A%E6%9C%BA%E5%99%A8%E4%BA%BA/01-%E5%8C%BB%E7%96%97%E9%97%AE%E8%AF%8A%E6%9C%BA%E5%99%A8%E4%BA%BA%E5%AE%9E%E7%8E%B0.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    2.1 医疗问诊机器人实现
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" >
        
          
          <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    3、新零售行业评价决策系统
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            3、新零售行业评价决策系统
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../03-%E6%96%B0%E9%9B%B6%E5%94%AE%E8%A1%8C%E4%B8%9A%E8%AF%84%E4%BB%B7%E5%86%B3%E7%AD%96%E7%B3%BB%E7%BB%9F/01-%E9%A1%B9%E7%9B%AE%E8%83%8C%E6%99%AF%E4%BB%8B%E7%BB%8D.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    3.1 项目背景介绍
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../03-%E6%96%B0%E9%9B%B6%E5%94%AE%E8%A1%8C%E4%B8%9A%E8%AF%84%E4%BB%B7%E5%86%B3%E7%AD%96%E7%B3%BB%E7%BB%9F/02-BERT%2BPET%E6%96%B9%E5%BC%8F%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E4%BB%8B%E7%BB%8D.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    3.2 BERT+PET方式文本分类介绍
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../03-%E6%96%B0%E9%9B%B6%E5%94%AE%E8%A1%8C%E4%B8%9A%E8%AF%84%E4%BB%B7%E5%86%B3%E7%AD%96%E7%B3%BB%E7%BB%9F/03-BERT%2BPET%E6%96%B9%E5%BC%8F%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E4%BB%8B%E7%BB%8D.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    3.3 BERT+PET方式数据预处理
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../03-%E6%96%B0%E9%9B%B6%E5%94%AE%E8%A1%8C%E4%B8%9A%E8%AF%84%E4%BB%B7%E5%86%B3%E7%AD%96%E7%B3%BB%E7%BB%9F/04-BERT%2BPET%E6%96%B9%E5%BC%8F%E6%A8%A1%E5%9E%8B%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0%E4%B8%8E%E8%AE%AD%E7%BB%83.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    3.4 BERT+PET方式模型代码实现和训练
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../03-%E6%96%B0%E9%9B%B6%E5%94%AE%E8%A1%8C%E4%B8%9A%E8%AF%84%E4%BB%B7%E5%86%B3%E7%AD%96%E7%B3%BB%E7%BB%9F/05-BERT%2BP-Tuning%E6%96%B9%E5%BC%8F%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E4%BB%8B%E7%BB%8D.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    3.5 BERT+P-Tuning方式文本分类介绍
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../03-%E6%96%B0%E9%9B%B6%E5%94%AE%E8%A1%8C%E4%B8%9A%E8%AF%84%E4%BB%B7%E5%86%B3%E7%AD%96%E7%B3%BB%E7%BB%9F/06-BERT%2BP-Tuning%E6%96%B9%E5%BC%8F%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E4%BB%8B%E7%BB%8D.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    3.6 BERT+P-Tuning方式数据预处理
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../03-%E6%96%B0%E9%9B%B6%E5%94%AE%E8%A1%8C%E4%B8%9A%E8%AF%84%E4%BB%B7%E5%86%B3%E7%AD%96%E7%B3%BB%E7%BB%9F/07-BERT%2BP-Tuning%E6%96%B9%E5%BC%8F%E6%A8%A1%E5%9E%8B%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0%E4%B8%8E%E8%AE%AD%E7%BB%83.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    3.7 BERT+P-Tuning方式模型代码实现和训练
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4" >
        
          
          <label class="md-nav__link" for="__nav_4" id="__nav_4_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    4、基于ChatGLM微调多任务实战
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4">
            <span class="md-nav__icon md-icon"></span>
            4、基于ChatGLM微调多任务实战
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../04-%E5%9F%BA%E4%BA%8EChatGLM%E5%BE%AE%E8%B0%83%E5%AE%9E%E7%8E%B0%E4%BF%A1%E6%81%AF%E6%8A%BD%E5%8F%96%2B%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E7%9A%84%E5%A4%9A%E4%BB%BB%E5%8A%A1%E5%AE%9E%E6%88%98/01-%E9%A1%B9%E7%9B%AE%E6%95%B4%E4%BD%93%E4%BB%8B%E7%BB%8D.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    4.1 项目整体简介
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../04-%E5%9F%BA%E4%BA%8EChatGLM%E5%BE%AE%E8%B0%83%E5%AE%9E%E7%8E%B0%E4%BF%A1%E6%81%AF%E6%8A%BD%E5%8F%96%2B%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E7%9A%84%E5%A4%9A%E4%BB%BB%E5%8A%A1%E5%AE%9E%E6%88%98/02-%E5%A4%9A%E4%BB%BB%E5%8A%A1%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    4.2 多任务数据预处理方式
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../04-%E5%9F%BA%E4%BA%8EChatGLM%E5%BE%AE%E8%B0%83%E5%AE%9E%E7%8E%B0%E4%BF%A1%E6%81%AF%E6%8A%BD%E5%8F%96%2B%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E7%9A%84%E5%A4%9A%E4%BB%BB%E5%8A%A1%E5%AE%9E%E6%88%98/03-LoRA%E6%96%B9%E5%BC%8F%E6%A8%A1%E5%9E%8B%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0%E4%B8%8E%E8%AE%AD%E7%BB%83.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    4.3 LoRA方式微调ChatGLM模型代码实现和训练
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../04-%E5%9F%BA%E4%BA%8EChatGLM%E5%BE%AE%E8%B0%83%E5%AE%9E%E7%8E%B0%E4%BF%A1%E6%81%AF%E6%8A%BD%E5%8F%96%2B%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E7%9A%84%E5%A4%9A%E4%BB%BB%E5%8A%A1%E5%AE%9E%E6%88%98/%E8%B6%8B%E5%8A%A8%E4%BA%91%E4%BD%BF%E7%94%A8%E3%80%8A%E8%A1%A5%E5%85%85%E3%80%8B.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    4.4 趋动云使用《扩展》
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="目录">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      目录
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#_1" class="md-nav__link">
    <span class="md-ellipsis">
      学习目标
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#peft" class="md-nav__link">
    <span class="md-ellipsis">
      PEFT(大模型参数高效微调)
    </span>
  </a>
  
    <nav class="md-nav" aria-label="PEFT(大模型参数高效微调)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1-prefix-tuning" class="md-nav__link">
    <span class="md-ellipsis">
      1. Prefix Tuning
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2-adapter-tuning" class="md-nav__link">
    <span class="md-ellipsis">
      2. Adapter Tuning
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3-lora" class="md-nav__link">
    <span class="md-ellipsis">
      3. LoRA
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#4-qlora" class="md-nav__link">
    <span class="md-ellipsis">
      4. QLoRA
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_2" class="md-nav__link">
    <span class="md-ellipsis">
      小结总结
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  



<h1 id="llmpeft">LLM的PEFT微调方法<a class="headerlink" href="#llmpeft" title="Permanent link">&para;</a></h1>
<hr />
<h2 id="_1">学习目标<a class="headerlink" href="#_1" title="Permanent link">&para;</a></h2>
<ul>
<li>理解Prefix-Tuning、Adapter-Tuning、LoRA三种大模型参数微调方法的原理</li>
</ul>
<hr />
<h2 id="peft">PEFT(大模型参数高效微调)<a class="headerlink" href="#peft" title="Permanent link">&para;</a></h2>
<p>目前在工业界应用大模型主流方式：<strong>参数高效微调方法（Parameter-Efficient Fine-Tuning，PEFT）</strong>，<strong>PEFT 方法仅微调少量或额外的模型参数，固定大部分预训练参数，大大降低了计算和存储成本，同时最先进的 PEFT 技术也能实现了与全量微调相当的性能</strong>。</p>
<p>该方法可以使 PLM 高效适应各种下游应用任务，而无需微调预训练模型的所有参数，且让大模型在消费级硬件上进行全量微调（Full Fine-Tuning）变得可行。</p>
<hr />
<p>目前应用较多的PEFT方法主要分为三大类：</p>
<ul>
<li><strong>Prefix/Prompt-Tuning</strong>：在模型的输入或隐层添加 <span class="arithmatex"><span class="MathJax_Preview">k</span><script type="math/tex">k</script></span>个额外可训练的前缀 tokens（这些前缀是连续的伪 tokens，不对应真实的 tokens），只训练这些前缀参数；</li>
<li><strong>Adapter-Tuning</strong>：将较小的神经网络层或模块插入预训练模型的每一层，这些新插入的神经模块称为 adapter（适配器），下游任务微调时也只训练这些适配器参数；</li>
<li><strong>LoRA</strong>：通过学习小参数的低秩矩阵来近似模型权重矩阵 <span class="arithmatex"><span class="MathJax_Preview">W</span><script type="math/tex">W</script></span>的参数更新，训练时只优化低秩矩阵参数；</li>
</ul>
<p>此外Huggface 开源的一个高效微调大模型的库PEFT，该算法库支持上述三类方法，可以直接调用。</p>
<h3 id="1-prefix-tuning">1. Prefix Tuning<a class="headerlink" href="#1-prefix-tuning" title="Permanent link">&para;</a></h3>
<p>Prefix-Tuning 在模型输入前添加一个连续的且任务特定的向量序列（continuous task-specific vectors），称之为前缀（prefix）。前缀被视为一系列“虚拟 tokens”，但是它由不对应于真实 tokens 的自由参数组成。与更新所有 PLM 参数的全量微调不同，Prefix-Tuning 固定 PLM 的所有参数，只更新优化特定任务的 prefix。因此，在生产部署时，只需要存储一个大型 PLM 的副本和一个学习到的特定任务的 prefix，每个下游任务只产生非常小的额外的计算和存储开销。</p>
<div align=center><img src="./assets/1-4-5.png" style="zoom:75%" ><img/></div>

<p>Fine-tuning 更新所有 PLM 参数，并且需要为每个任务存储完整的模型副本。Prefix-tuning 冻结了 PLM 参数并且只优化了 prefix。因此，只需要为每个任务存储特定 prefix，使 Prefix-tuning 模块化且节省存储空间。</p>
<hr />
<p>具体实现流程如下：</p>
<p>（1）<strong>确定任务与基模型</strong></p>
<ul>
<li>任务：条件生成（如摘要、表格到文本、对话）或 seq2seq。</li>
<li>选模型：GPT-2/decoder-only 或 BART/T5（encoder-decoder）。</li>
</ul>
<p>（2）<strong>设计 prefix 配置</strong></p>
<ul>
<li>决定 <code>num_prefix</code>（每层的虚拟 token 数，常见 10–100），以及是否对所有层都使用 prefix（论文对每层都用了 prefix，但可做只对部分层）。</li>
</ul>
<p>（3）<strong>构造可训练参数（初始化）</strong></p>
<p>可以为每层创建一个可以训练的矩阵<span class="arithmatex"><span class="MathJax_Preview">P_θ</span><script type="math/tex">P_θ</script></span> ，作为前缀向量拼接到原向量中。但是论文中提出直接优化 <span class="arithmatex"><span class="MathJax_Preview">P_θ</span><script type="math/tex">P_θ</script></span> 会导致训练不稳定，可以通过一个更小的矩阵 <span class="arithmatex"><span class="MathJax_Preview">P_w</span><script type="math/tex">P_w</script></span>和一个更大的前馈神经网络<span class="arithmatex"><span class="MathJax_Preview">MLP_θ</span><script type="math/tex">MLP_θ</script></span> 对<span class="arithmatex"><span class="MathJax_Preview">P_θ</span><script type="math/tex">P_θ</script></span> 进行重参数化: <span class="arithmatex"><span class="MathJax_Preview">P_θ[i,:]=MLP_θ(P_w[i,:])</span><script type="math/tex">P_θ[i,:]=MLP_θ(P_w[i,:])</script></span> 。</p>
<p>所以目前实际实现时，通常会先训练一个 <code>(num_layers, num_prefix, hidden_dim)</code> 的 prefix embedding，然后通过一个小的 MLP 投影成 <code>(num_layers, num_prefix, 2 * head_dim * num_heads)</code>，再 reshape 成 <code>(num_layers, num_heads, num_prefix, head_dim)</code>，分别拆成 K 和 V。因此需要创建一个可训练的矩阵 <span class="arithmatex"><span class="MathJax_Preview">P_θ</span><script type="math/tex">P_θ</script></span>  ，以及一个MLP模型。</p>
<p>（4）<strong>修改模型前向（插入 prefix）</strong></p>
<ul>
<li>在 Transformer 的每一层注意力里，将 prefix 对应的 key/value 拼接到原始的 key/value——这样后续 token 可以像“看到真实 tokens”一样 attend 到 prefix，然后一起进行训练。</li>
</ul>
<p>（5）<strong>训练设置</strong></p>
<ul>
<li>冻结原模型参数（<code>requires_grad=False</code>），只对 prefix 参数做优化。</li>
<li>损失函数通常是标准的交叉熵，训练器只更新 prefix。 </li>
</ul>
<p>（6）<strong>推理</strong></p>
<ul>
<li>推理时把训练好的 prefix 附加到每层（同训练时），然后用常见的解码策略进行生成。</li>
</ul>
<hr />
<p>如下图所示，以 GPT2 的自回归语言模型为例，将输入 <span class="arithmatex"><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span> 和输出 <span class="arithmatex"><span class="MathJax_Preview">y</span><script type="math/tex">y</script></span> 拼接为 <span class="arithmatex"><span class="MathJax_Preview">z=[x;y]</span><script type="math/tex">z=[x;y]</script></span> ，经过 LM 的某一层计算隐层表示<span class="arithmatex"><span class="MathJax_Preview">h=[h_1,...,h_i,....,h_n]</span><script type="math/tex">h=[h_1,...,h_i,....,h_n]</script></span> ， <span class="arithmatex"><span class="MathJax_Preview">h_i=LM_Ø(z_i, h&lt;i)</span><script type="math/tex">h_i=LM_Ø(z_i, h<i)</script></span> ，其中， <span class="arithmatex"><span class="MathJax_Preview">X_{idx}</span><script type="math/tex">X_{idx}</script></span> 和<span class="arithmatex"><span class="MathJax_Preview">Y_{idx}</span><script type="math/tex">Y_{idx}</script></span>分别为输入和输出序列的索引。</p>
<p>Prefix-Tuning 在输入前添加前缀，即<span class="arithmatex"><span class="MathJax_Preview">z=[Prefix,x,y]</span><script type="math/tex">z=[Prefix,x,y]</script></span> ，<span class="arithmatex"><span class="MathJax_Preview">P_{idx}</span><script type="math/tex">P_{idx}</script></span>为前缀序列的索引，<span class="arithmatex"><span class="MathJax_Preview">|P_{idx}|</span><script type="math/tex">|P_{idx}|</script></span> 为前缀的长度。前缀索引对应着由<span class="arithmatex"><span class="MathJax_Preview">θ</span><script type="math/tex">θ</script></span>参数化的向量矩阵 <span class="arithmatex"><span class="MathJax_Preview">P_θ</span><script type="math/tex">P_θ</script></span> ，维度为<span class="arithmatex"><span class="MathJax_Preview">|P_{idx}|×dim(h_i)</span><script type="math/tex">|P_{idx}|×dim(h_i)</script></span>。</p>
<p><div align=center><img src="./assets/1-4-6.png" style="zoom:75%" ><img/></div></p>
<p>在训练时，LM 的参数 <span class="arithmatex"><span class="MathJax_Preview">Ø</span><script type="math/tex">Ø</script></span> 被固定，只有前缀参数 <span class="arithmatex"><span class="MathJax_Preview">θ</span><script type="math/tex">θ</script></span> 为可训练的参数。训练完成后，只有前缀<span class="arithmatex"><span class="MathJax_Preview">P_θ</span><script type="math/tex">P_θ</script></span>被保存。</p>
<hr />
<p><strong>Prefix Tuning的特点</strong>：</p>
<ul>
<li>优点：<ul>
<li>只训练少量 prefix 参数，相对全量微调的存储和训练成本低。</li>
<li>不同任务只需切换 prefix，无需保存多个完整模型。</li>
</ul>
</li>
<li>缺点<ul>
<li>小模型表现差：在 BERT-base 等小模型上效果不佳</li>
<li>需在每层注入 prefix，会占用输入序列的长度</li>
<li>在判别式任务上常逊于 LoRA、P-Tuning v2</li>
</ul>
</li>
</ul>
<h3 id="2-adapter-tuning">2. Adapter Tuning<a class="headerlink" href="#2-adapter-tuning" title="Permanent link">&para;</a></h3>
<p>与 Prefix Tuning 和 Prompt Tuning 这类在输入前可训练添加 prompt embedding 参数来以少量参数适配下游任务不同，<strong>Adapter Tuning 则是在预训练模型内部的网络层之间添加新的网络层或模块来适配下游任务</strong>。</p>
<hr />
<p>假设预训练模型函数表示为<span class="arithmatex"><span class="MathJax_Preview">Ø_w(x)</span><script type="math/tex">Ø_w(x)</script></span>，对于 Adapter Tuning ，添加适配器之后模型函数更新为<span class="arithmatex"><span class="MathJax_Preview">Ø_{w,w_0}(x)</span><script type="math/tex">Ø_{w,w_0}(x)</script></span>， <span class="arithmatex"><span class="MathJax_Preview">w</span><script type="math/tex">w</script></span>是预训练模型的参数， <span class="arithmatex"><span class="MathJax_Preview">w_0</span><script type="math/tex">w_0</script></span>是新添加的适配器的参数，在训练过程中， <span class="arithmatex"><span class="MathJax_Preview">w</span><script type="math/tex">w</script></span>被固定，只有 <span class="arithmatex"><span class="MathJax_Preview">w_0</span><script type="math/tex">w_0</script></span>被更新。<span class="arithmatex"><span class="MathJax_Preview">|w_0|&lt;&lt;|w|</span><script type="math/tex">|w_0|<<|w|</script></span> ，这使得不同下游任务只需要添加少量可训练的参数即可，节省计算和存储开销，同时共享大规模预训练模型。</p>
<div align=center><img src="./assets/1-4-7.png" style="zoom:70%" ><img/></div>

<p>Series Adapter的适配器结构和与 Transformer 的集成如上图所示。适配器模块被添加到每个 Transformer 层两次：多头注意力映射之后和两层前馈神经网络之后。适配器是一个 bottleneck（瓶颈）结构的模块，由一个两层的前馈神经网络（由向下投影矩阵、非线性函数和向上投影矩阵构成）和一个输出输出之间的残差连接组成。</p>
<hr />
<p><strong>Adapter Tuning的特点</strong>：</p>
<ul>
<li>优点：<ul>
<li>只训练少量 adapter 参数，相对全量微调的存储和训练成本低。</li>
<li>可以为多个任务保存不同的 adapter，而共享一个大模型。</li>
<li>多个任务的 adapters 可以在推理时进行切换或融合，提升迁移和泛化能力。</li>
</ul>
</li>
<li>缺点<ul>
<li>因为大部分参数被冻结，adapter 的容量有限，对复杂任务或需要大规模参数调整的任务可能效果不如全量微调。</li>
<li>Adapter 的维度大小（瓶颈层大小）、插入位置等超参数对性能影响较大，调参复杂度较高。</li>
<li>PLM 基础上添加适配器层会引入额外的计算，带来推理延迟问题</li>
</ul>
</li>
</ul>
<h3 id="3-lora">3. LoRA<a class="headerlink" href="#3-lora" title="Permanent link">&para;</a></h3>
<p>上述Adapter Tuning 方法在 PLM 基础上添加适配器层会引入额外的计算，带来推理延迟问题；而 Prefix Tuning 方法难以优化，其性能随可训练参数规模非单调变化，更根本的是，为前缀保留部分序列长度必然会减少用于处理下游任务的序列长度。因此微软推出了LoRA方法。</p>
<hr />
<p><strong>LORA的实现方式</strong></p>
<p>我们先思考两个问题：为何用数千的样本就能将一个数十亿参数的模型微调得比较好？为何大模型表现出很好的few-shot能力？</p>
<p>Aghajanyan的研究（《Intrinsic Dimensionality Explains the Effectiveness of Language Model Fine-Tuning》）表明：预训练模型拥有极小的内在维度(instrisic dimension)，即存在一个极低维度的参数，微调它和在全参数空间中微调能起到相同的效果【大模型在预训练阶段已经学到了更多的“通用特征”，所以在微调时，只需要在一个更小的方向空间中“对齐”或“修正”即可】。</p>
<p>同时Aghajanyan发现在预训练后，越大的模型有越小的内在维度，这也解释了为何大模型都拥有很好的few-shot能力【因为它们已经覆盖了大部分语言知识，少量参数更新（甚至几条示例 in-context）就能把输出方向调整到目标任务】。</p>
<p>受instrisic dimension工作的启发，作者认为参数更新过程中也存在一个‘内在秩’。对于预训练权重矩阵 $ W_0$ ，我们可以用一个低秩分解 $ \Delta W $ 来表示参数更新，即：</p>
<div align=center><img src="./assets/image-20250816171535950.png" style="zoom:70%" ><img/></div>

<p>训练过程中冻结参数 $ W_0$ ，仅训练A和B中的参数。如上图所示，前向传播过程变为</p>
<div align=center><img src="./assets/image-20250816171810374.png" style="zoom:70%" ><img/></div>

<p>这种方式则称为低秩适应（Low-Rank Adaptation），其核心思想是对大型模型的权重矩阵进行隐式的低秩转换，也就是：<strong>通过一个较低维度的表示来近似表示一个高维矩阵或数据集</strong>。</p>
<div align=center><img src="./assets/1-4-8.png" style="zoom:70%" ><img/></div>

<p>基本原理：LoRA技术冻结预训练模型的权重，并在每个Transformer块中注入可训练层（称为秩分解矩阵），即在模型的Linear层的旁边增加一个“旁支”A和B。其中，A将数据从d维降到r维，这个r是LoRA的秩，是一个重要的超参数；B将数据从r维升到d维，B部分的参数初始为0。模型训练结束后，需要将A+B部分的参数与原大模型的参数合并在一起使用。</p>
<hr />
<p>python伪代码</p>
<div class="highlight"><pre><span></span><code><span class="n">input_dim</span> <span class="o">=</span> <span class="mi">768</span> <span class="c1"># 例如，预训练模型的隐藏大小</span>
<span class="n">output_dim</span> <span class="o">=</span> <span class="mi">768</span> <span class="c1"># 例如，层的输出大小</span>
<span class="n">rank</span> <span class="o">=</span> <span class="mi">8</span> <span class="c1"># 低秩适应的等级&#39;r&#39;</span>
<span class="n">W</span> <span class="o">=</span> <span class="o">...</span> <span class="c1"># 来自预训练网络的权重，形状为 input_dim x output_dim</span>
<span class="n">W_A</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="n">input_dim</span><span class="p">,</span> <span class="n">rank</span><span class="p">))</span> <span class="c1"># LoRA权重A</span>
<span class="n">W_B</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="n">rank</span><span class="p">,</span> <span class="n">output_dim</span><span class="p">))</span> <span class="c1"># LoRA权重B初始化LoRA权重</span>
<span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">kaiming_uniform_</span><span class="p">(</span><span class="n">W_A</span><span class="p">,</span> <span class="n">a</span><span class="o">=</span><span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">5</span><span class="p">))</span>
<span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">zeros_</span><span class="p">(</span><span class="n">W_B</span><span class="p">)</span>

<span class="k">def</span><span class="w"> </span><span class="nf">regular_forward_matmul</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">W</span><span class="p">):</span>
  <span class="n">h</span> <span class="o">=</span> <span class="n">x</span> <span class="o">@</span> <span class="n">W</span>
  <span class="k">return</span> <span class="n">h</span>

<span class="k">def</span><span class="w"> </span><span class="nf">lora_forward_matmul</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">W</span><span class="p">,</span> <span class="n">W_A</span><span class="p">,</span> <span class="n">W_B</span><span class="p">):</span>
  <span class="n">h</span> <span class="o">=</span> <span class="n">x</span> <span class="o">@</span> <span class="n">W</span> <span class="c1"># 常规矩阵乘法</span>
  <span class="n">h</span> <span class="o">+=</span> <span class="n">x</span> <span class="o">@</span> <span class="n">W_A</span> <span class="o">@</span> <span class="n">W_B</span> <span class="o">*</span> <span class="n">alpha</span> <span class="c1"># 使用缩放的LoRA权重,alpha缩放因子</span>
  <span class="k">return</span> <span class="n">h</span>
</code></pre></div>
<p>LoRA方法是目前最通用、同时也是效果最好的微调方法之一。</p>
<hr />
<p><strong>LoRA的特点</strong>：</p>
<ul>
<li>优点：<ul>
<li>只训练极少参数，相对全量微调的存储和训练成本低。</li>
<li>效果接近全参数微调，且保留原模型能力。</li>
<li>不同任务的 LoRA 模块可插拔，便于多任务部署。</li>
</ul>
</li>
<li>缺点<ul>
<li>LoRA 本质是用低秩分解逼近权重更新矩阵，这对参数空间的表达能力有限制，可能无法拟合某些复杂任务所需的高秩变化。</li>
<li>LoRA 通常加在 attention 的投影矩阵（Wq/Wv）上，但不同任务可能对位置敏感，选择不好会影响性能。</li>
</ul>
</li>
</ul>
<h3 id="4-qlora">4. QLoRA<a class="headerlink" href="#4-qlora" title="Permanent link">&para;</a></h3>
<p><strong>QLoRA出现的背景和动机:</strong></p>
<p>尽管LoRA已经大大降低了微调的参数量和存储需求，但对于GPT-3 (175B)、LLaMA-65B (65B)、Falcon-40B (40B) 这样参数量巨大的模型，即使是加载模型本身（即使冻结）也需要巨大的GPU内存。例如，加载一个65B参数的16-bit模型就需要大约130GB 的 VRAM，这超出了大多数消费级GPU的能力。</p>
<p>于是Dettmers et al. 于 2023 年在论文 <em>QLoRA: Efficient Finetuning of Quantized LLMs</em> 中提出**QLoRA (Quantized LoRA)** 。</p>
<p>它的核心思想是：<strong>通过对预训练语言模型（PLM）进行量化（通常是 4-bit NormalFloat），并结合 LoRA 技术进行微调，从而在极低的内存消耗下，仍然能够高效地微调巨型语言模型，同时保持甚至超越全量 16-bit LoRA 的性能。</strong></p>
<p>QLoRA极大地降低了微调大型模型的硬件门槛，使得在消费级 GPU 上微调数十亿甚至千亿参数的模型成为可能。</p>
<hr />
<p>QLoRA的核心在于引入了两个关键的创新点：</p>
<ol>
<li><strong>4-bit NormalFloat (NF4) 量化：</strong><ul>
<li>传统的量化方法（如 int8）可能导致精度损失。QLoRA 提出了一种**新的4-bit 数据类型NormalFloat (NF4)**，它是一种信息理论最优的量化方案，专为正态分布的数据设计，能够更有效地保留模型的精度。</li>
<li><strong>双量化 (Double Quantization)</strong>：这是 QLoRA 的一个重要细节。它对第一次量化得到的量化常数（即块量化中每个块的比例因子）再次进行量化。这能进一步减少内存占用，因为它减少了存储量化常数所需的位数。</li>
<li><strong>分页优化器 (Paged Optimizers)</strong>：QLoRA 利用 NVIDIA 的统一内存功能，在 GPU 内存不足时，将优化器状态（optimizer states）分页到 CPU RAM，从而避免了 GPU 内存溢出（OOM）错误。</li>
</ul>
</li>
<li><strong>LoRA 微调机制保持不变：</strong><ul>
<li>在量化后的冻结PLM上，<strong>LoRA的微调机制保持不变</strong>。这意味着我们仍然只训练和更新少量的低秩 A 和 B 矩阵。</li>
<li><strong>16-bit LoRA 权重</strong> ：尽管基础模型是 4-bit 量化的，但 LoRA 权重本身（A 和 B 矩阵）以及优化器状态**仍然保持 16-bit 精度**（通常是 float16 或 bfloat16）。这是 QLoRA 能够保持高性能的关键。这是因为只训练少量的 LoRA 权重，将其保持高精度不会显著增加内存负担，但对梯度计算和优化过程的稳定性至关重要。</li>
</ul>
</li>
</ol>
<hr />
<p><strong>工作原理:</strong></p>
<ol>
<li><strong>加载和 4-bit 量化基础 PLM：</strong><ul>
<li>使用<code>bitsandbytes</code>库加载预训练模型。在加载时，模型的大部分权重会被立即量化为 4-bit NormalFloat (NF4)。这个量化过程通常包含双量化。</li>
<li>模型的参数被设置为不可训练（<code>requires_grad=False</code>）。</li>
</ul>
</li>
<li><strong>在量化模型上注入 LoRA 模块：</strong><ul>
<li>像标准 LoRA 一样，选择模型中的目标线性层（如 Attention 的 Q、K、V 投影层和 FFN），并在它们旁边并行地注入可训练的低秩 A 和 B 矩阵。</li>
<li>这些 A 和 B 矩阵以及它们的优化器状态保持 16-bit 精度。</li>
</ul>
</li>
<li><strong>前向传播：</strong><ul>
<li>当进行前向传播时，量化后的 4-bit 权重会被**即时反量化 (dequantize)** 到 16-bit 精度，然后与输入相乘。</li>
<li>LoRA 模块的 16-bit 权重 BA 也会与输入相乘。</li>
<li>两个结果（来自基础模型的和来自 LoRA 模块的）相加。</li>
</ul>
</li>
<li><strong>反向传播：</strong><ul>
<li>梯度会流向 LoRA 模块的 A 和 B 矩阵。</li>
<li>由于基础模型是冻结且量化的，<strong>梯度不会流向其 4-bit 权重</strong>。</li>
</ul>
</li>
<li><strong>分页优化器：</strong><ul>
<li>优化器（如 AdamW）的状态通常会占用大量内存。QLoRA 使用分页优化器，在 GPU 内存不足时，将优化器状态在 GPU 和 CPU RAM 之间进行分页，以避免 OOM 错误。</li>
</ul>
</li>
</ol>
<hr />
<p><strong>QLoRA的特点</strong>：</p>
<ul>
<li>优点：<ul>
<li>极低的内存消耗。这是 QLoRA 最显著的优势。可以将训练巨型模型的内存需求降低 3-4 倍，使得在单张消费级 GPU 上（如 24GB VRAM 的 RTX 3090/4090）微调 65B 甚至 70B 参数的模型成为可能。</li>
<li>性能优异：尽管进行了 4-bit 量化，但由于 16-bit 的 LoRA 权重和优化器状态，QLoRA 在许多任务上能够保持与 16-bit LoRA 甚至全量微调相媲美的性能。</li>
<li>训练速度快：由于只训练少量参数且内存效率高，训练速度非常快。</li>
</ul>
</li>
<li>缺点<ul>
<li>虽然 NF4 优化了精度，但极端任务或敏感任务可能仍受 4-bit 量化影响。</li>
<li>由于量化和分页机制的存在，训练和问题调试会比标准 LoRA 更复杂。</li>
</ul>
</li>
</ul>
<h3 id="_2">小结总结<a class="headerlink" href="#_2" title="Permanent link">&para;</a></h3>
<ul>
<li>本小节主要介绍了目前主流的PEFT方式的原理。</li>
</ul>
<hr />












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      <script id="__config" type="application/json">{"base": "..", "features": [], "search": "../assets/javascripts/workers/search.d50fe291.min.js", "tags": null, "translations": {"clipboard.copied": "\u5df2\u590d\u5236", "clipboard.copy": "\u590d\u5236", "search.result.more.one": "\u5728\u8be5\u9875\u4e0a\u8fd8\u6709 1 \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.more.other": "\u5728\u8be5\u9875\u4e0a\u8fd8\u6709 # \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.none": "\u6ca1\u6709\u627e\u5230\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.one": "\u627e\u5230 1 \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.other": "# \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.placeholder": "\u952e\u5165\u4ee5\u5f00\u59cb\u641c\u7d22", "search.result.term.missing": "\u7f3a\u5c11", "select.version": "\u9009\u62e9\u5f53\u524d\u7248\u672c"}, "version": null}</script>
    
    
      <script src="../assets/javascripts/bundle.50899def.min.js"></script>
      
        <script src="../js/extra.js"></script>
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML"></script>
      
    
  </body>
</html>